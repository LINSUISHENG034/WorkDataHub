name: "P-031: Annuity Performance Pipeline Refactor & Legacy Baseline"
description: |

## Purpose
Refactor the `annuity_performance` domain to use the shared pipeline framework while establishing an auditable golden baseline from the legacy `AnnuityPerformanceCleaner`, ensuring regression-safe migration with preserved enrichment behavior and observability.

## Core Principles
1. **Context is King**: Include ALL necessary documentation, examples, and caveats
2. **Validation Loops**: Provide executable tests/lints the AI can run and fix
3. **Information Dense**: Use keywords and patterns from the codebase
4. **Progressive Success**: Start simple, validate, then enhance
5. **Global rules**: Be sure to follow all rules in CLAUDE.md

---

## Goal
Create a **legacy golden baseline** by extracting and isolating the `AnnuityPerformanceCleaner` logic, then rebuild identical transformation behavior using the shared pipeline framework to enable safe rollback while maintaining company enrichment integration and CSV exports.

## Why
- **Business value**: Enables unified data processing architecture across domains while preserving critical legacy behavior
- **Integration**: Seamlessly integrates with existing enrichment service and observability infrastructure
- **Problems solved**: Eliminates legacy MySQL dependencies while maintaining data quality and regulatory compliance for "规模明细" processing

## What
A complete refactor where:
- Legacy cleansing logic is extracted and preserved as deterministic golden fixtures
- Shared pipeline framework reproduces identical transformations through composable steps
- Enrichment service integration and CSV exports remain unchanged
- Configuration toggle enables safe rollback during deployment

### Success Criteria
- [ ] Legacy baseline script (`scripts/tools/run_legacy_annuity_cleaner.py`) executes the real legacy cleaner and generates deterministic golden fixtures matching manual verification
- [ ] Shared pipeline reproduces legacy baseline exactly (100% parity on curated test subsets)
- [ ] Enrichment integration preserves all metrics counters and queue behavior
- [ ] Configuration toggle `WDH_ANNUITY_PIPELINE_ENABLED` works with CLI override
- [ ] All validation gates pass including ruff, mypy, and pytest
- [ ] Documentation includes rollback procedures and validation commands

## All Needed Context

### Documentation & References
```yaml
# MUST READ - Include these in your context window
- file: legacy/annuity_hub/data_handler/data_cleaner.py:158-232
  why: Authoritative AnnuityPerformanceCleaner._clean_method() implementation

- file: src/work_data_hub/domain/annuity_performance/service.py:142-303
  why: Current process_with_enrichment function and enrichment integration patterns

- file: src/work_data_hub/domain/pipelines/core.py
  why: TransformStep and Pipeline execution patterns for new implementation

- file: tests/fixtures/sample_legacy_mappings.json
  why: Example mapping structure and company ID resolution test data

- file: tests/domain/pipelines/test_core.py
  why: Pipeline testing patterns and step composition examples

- file: src/work_data_hub/config/settings.py
  why: Configuration pattern for WDH_ANNUITY_PIPELINE_ENABLED toggle

- docfile: docs/research/Data Pipeline Migration Testing Best Practices.md
  why: Research validation of golden baseline testing, deterministic comparison protocols, and feature toggle patterns

- docfile: docs/research/legacy_baseline_workflow.md
  why: Concrete playbook for replacing legacy mapping modules and running cleaners without MySQL
```

### Implementation-Facing Research Notes
```yaml
sources:
  - file: legacy/annuity_hub/data_handler/data_cleaner.py:10-18
    section: mapping imports
    why: Required mapping dictionaries for legacy script dependency injection
    type: codebase

tldr:
  - Legacy cleaner uses 5-priority company_id resolution: COMPANY_ID1_MAPPING (plan codes), COMPANY_ID2_MAPPING (account numbers), COMPANY_ID3_MAPPING (hardcoded), COMPANY_ID4_MAPPING (customer names), COMPANY_ID5_MAPPING (account names)
  - Default fallback company_id is '600866980' when customer_name is null/empty
  - Portfolio code defaults use business type logic: 'QTAN003' for 职年受托/职年投资, otherwise DEFAULT_PORTFOLIO_CODE_MAPPING
  - Column renaming handles legacy field names: 机构→机构名称, 计划号→计划代码, 流失（含待遇支付）→流失(含待遇支付)
  - Plan code normalization: '1P0290'→'P0290', '1P0807'→'P0807', empty集合计划→'AN001', empty单一计划→'AN002'

setup_commands:
  - "mkdir -p scripts/tools tests/fixtures/annuity_performance"
  - "touch tests/fixtures/annuity_performance/golden_legacy.parquet"

api_decisions:
  - name: golden_baseline_output_format
    choice: Parquet with deterministic sorting by (月度, 计划代码, company_id)
    rationale: Better data type preservation than CSV, matches composite PK, enables reliable comparisons. Research shows Parquet prevents int/float ambiguity issues.
  - name: mapping_injection_strategy
    choice: Register an in-memory replacement module for legacy mappings before importing AnnuityPerformanceCleaner
    rationale: Prevents legacy MySQL calls at import time while preserving exact legacy logic
  - name: pipeline_step_granularity
    choice: Single TransformStep per logical operation (date parsing, code mapping, company resolution)
    rationale: Enables fine-grained testing and reusability across domains
  - name: dataframe_comparison_strategy
    choice: Enhanced canonicalization with pandas.testing.assert_frame_equal
    rationale: Research-validated approach handles floating-point precision, column/row ordering, and NaN comparisons robustly

versions:
  - library: pandas
    constraint: ">=1.5.0,<3.0"
    compatibility: Excel reading and DataFrame operations
  - library: pydantic
    constraint: ">=2.0.0,<3.0"
    compatibility: Model validation and field aliasing

pitfalls_and_mitigations:
  - issue: Chinese characters in column names may cause encoding issues
    mitigation: Use UTF-8 encoding explicitly and validate field mapping in tests
  - issue: Decimal precision differences between legacy and pipeline
    mitigation: Use Decimal fields with explicit decimal_places=4 in models
  - issue: Legacy mapping dictionaries are module-level globals
    mitigation: Cache original values and restore after monkeypatching
  - issue: Pipeline step order dependencies
    mitigation: Document step dependencies and validate in pipeline builder
  - issue: Non-deterministic DataFrame comparisons due to floating-point precision and row ordering
    mitigation: Use enhanced canonicalization protocol with configurable tolerance (rtol=1e-5, atol=1e-8)
  - issue: CSV format loses data type information during golden baseline storage
    mitigation: Use Parquet format which preserves schema and prevents int/float ambiguity

open_questions:
  - Should the legacy baseline script handle multiple Excel files or focus on single file processing?
  - What level of logging detail is needed for the golden baseline generation?
```

### Current Codebase tree
```bash
src/work_data_hub/
  domain/
    annuity_performance/
      __init__.py
      models.py              # AnnuityPerformanceOut with field validation
      service.py             # process_with_enrichment with enrichment integration
      csv_export.py          # CSV export functionality
    pipelines/
      __init__.py
      core.py                # TransformStep and Pipeline base classes
      builder.py             # Pipeline construction utilities
      config.py              # Pipeline configuration models
      types.py               # Shared type definitions
      adapters.py            # Integration adapters
      exceptions.py          # Pipeline-specific exceptions
  orchestration/
    jobs.py                  # CLI entry points and job definitions
  config/
    settings.py              # WDH_* environment variable handling
legacy/
  annuity_hub/
    data_handler/
      data_cleaner.py        # AnnuityPerformanceCleaner source of truth
      mappings.py            # COMPANY_ID*_MAPPING and other dictionaries
tests/
  fixtures/
    sample_legacy_mappings.json  # Test mapping data structure
  domain/
    pipelines/
      test_core.py           # Pipeline testing patterns
```

### Desired Codebase tree with files to be added and responsibility of file
```bash
scripts/
  tools/
    run_legacy_annuity_cleaner.py    # Legacy baseline generator with mapping injection
src/work_data_hub/
  domain/
    annuity_performance/
      pipeline_steps.py              # TransformStep implementations for annuity cleansing
tests/
  fixtures/
    annuity_performance/
      golden_legacy.parquet              # Deterministic legacy output for regression testing
  domain/
    annuity_performance/
      test_pipeline_vs_legacy.py     # Automated parity validation tests
```

### Known Gotchas of our codebase & Library Quirks
```python
# CRITICAL: Legacy cleaner uses pandas with dtype=str to preserve leading zeros
# CRITICAL: AnnuityPerformanceOut uses field aliases for database column mapping
# CRITICAL: Company enrichment service expects specific field names from raw_row
# CRITICAL: Pipeline steps must handle None/NaN values gracefully
# CRITICAL: CSV export maintains enrichment statistics for observability
# CRITICAL: Configuration toggle must work in both orchestration and CLI contexts
# CRITICAL: Decimal fields need explicit precision to match legacy behavior
# CRITICAL: Chinese column names require proper UTF-8 handling throughout
```

## Implementation Blueprint

### Data models and structure

The core transformation logic will be decomposed into discrete pipeline steps:

```python
# pipeline_steps.py - Modular transformation steps
class ColumnNormalizationStep(TransformStep):
    """Normalize legacy column names to standardized format."""

class DateParsingStep(TransformStep):
    """Parse and standardize date fields using parse_to_standard_date."""

class PlanCodeCleansingStep(TransformStep):
    """Apply plan code corrections and defaults (AN001, AN002)."""

class InstitutionCodeMappingStep(TransformStep):
    """Map institution names to codes via COMPANY_BRANCH_MAPPING."""

class PortfolioCodeDefaultStep(TransformStep):
    """Set portfolio code defaults based on business type logic."""

class CompanyIdResolutionStep(TransformStep):
    """5-priority company_id resolution matching legacy algorithm."""

class DataValidationStep(TransformStep):
    """Schema and business rule validation using Pandera contracts."""

class FieldCleanupStep(TransformStep):
    """Remove invalid columns and finalize record structure."""
```

### List of tasks to be completed to fulfill the PRP in the order they should be completed

```yaml
Task 1: Create Legacy Baseline Generation Script
CREATE scripts/tools/run_legacy_annuity_cleaner.py:
  - PATTERN: Follow src/work_data_hub/orchestration/jobs.py CLI argument handling
  - Load mapping dictionaries from tests/fixtures/sample_legacy_mappings.json
  - Register an in-memory replacement for legacy.annuity_hub.data_handler.mappings *before* importing the cleaner so no MySQL calls fire at import time
  - Execute the real AnnuityPerformanceCleaner on Excel subsets with deterministic output
  - Sort results by (月度, 计划代码, company_id) for reproducible diffs

Task 2: Generate and Validate Golden Baseline
RUN baseline generation:
  - Execute script against tests/fixtures/sample_data/annuity_subsets
  - Validate output structure matches expected Parquet schema
  - Perform manual spot-checks against known mapping results
  - Commit golden_legacy.parquet to version control
  - Capture any manual checks or mapping updates in docs/research/legacy_baseline_workflow.md

Task 3: Implement Pipeline Steps
CREATE src/work_data_hub/domain/annuity_performance/pipeline_steps.py:
  - PATTERN: Inherit from src/work_data_hub/domain/pipelines/core.TransformStep
  - Implement each step to match specific legacy cleaner operations
  - Use existing utility functions (parse_to_standard_date, clean_company_name)
  - Handle Chinese column names and character encoding properly

Task 4: Build Pipeline Configuration
MODIFY src/work_data_hub/domain/annuity_performance/pipeline_steps.py:
  - Add build_annuity_pipeline() factory function
  - Create PipelineConfig with appropriate error handling settings
  - Define step sequence matching legacy cleaner execution order
  - Configure field validation and type coercion

Task 5: Refactor Service Integration
MODIFY src/work_data_hub/domain/annuity_performance/service.py:
  - Add use_pipeline parameter to process_with_enrichment
  - Implement pipeline execution path alongside existing logic
  - Preserve enrichment service integration patterns exactly
  - Maintain CSV export and metrics collection behavior

Task 6: Add Configuration Toggle
MODIFY src/work_data_hub/config/settings.py:
  - Add WDH_ANNUITY_PIPELINE_ENABLED: bool = True
  - PATTERN: Follow existing WDH_* environment variable patterns

MODIFY src/work_data_hub/orchestration/jobs.py:
  - Add --use-pipeline/--no-use-pipeline CLI flag
  - Override setting when flag provided
  - Pass flag to process_with_enrichment calls

Task 7: Implement Parity Testing
CREATE tests/domain/annuity_performance/test_pipeline_vs_legacy.py:
  - Load golden_legacy.parquet as reference data
  - Execute pipeline on same Excel inputs
  - Use enhanced DataFrame comparison with canonicalization protocol
  - Compare outputs field-by-field with floating-point tolerance
  - Test enrichment statistics match expected patterns
  - Validate CSV export functionality

Task 8: Add Documentation and Validation
CREATE docs/VALIDATION.md (if not exists):
  - Document baseline generation commands
  - Include manual verification procedures
  - List rollback steps and troubleshooting
  - Provide metric comparison guidelines

UPDATE README.md:
  - Reference new pipeline toggle and baseline script
  - Document validation procedures and testing approach
```

### Per task pseudocode as needed added to each task

```python
# Task 1: Legacy Baseline Script
def run_legacy_cleaner(input_paths, output_path, mappings_source):
    # PATTERN: Command-line argument parsing like jobs.py
    mappings = load_mappings_from_json(mappings_source)["mappings"]

    # CRITICAL: Build an in-memory legacy mappings module BEFORE importing cleaner
    mapping_module = build_mapping_module_from_fixture(mappings)
    sys.modules["legacy.annuity_hub.data_handler.mappings"] = mapping_module

    from legacy.annuity_hub.data_handler.data_cleaner import AnnuityPerformanceCleaner

    results = []
    for excel_path in input_paths:
        cleaner = AnnuityPerformanceCleaner(excel_path, sheet_name="规模明细")
        df = cleaner.clean()
        results.append(df)

    # CRITICAL: Deterministic sorting for reproducible output
    combined_df = pd.concat(results)
    combined_df.sort_values(["月度", "计划代码", "company_id"], inplace=True)
    combined_df.to_parquet(output_path, index=False)

# Task 3: Pipeline Step Implementation
class CompanyIdResolutionStep(TransformStep):
    def apply(self, row: Row, context: Dict) -> StepResult:
        # PATTERN: Exact replication of legacy 5-step resolution
        company_id = None

        # Priority 1: Plan code mapping
        if row.get("计划代码"):
            company_id = COMPANY_ID1_MAPPING.get(row["计划代码"])

        # Priority 2: Account number mapping
        if not company_id and row.get("集团企业客户号"):
            cleaned_account = row["集团企业客户号"].lstrip('C') if row["集团企业客户号"] else None
            company_id = COMPANY_ID2_MAPPING.get(cleaned_account)

        # Priority 3-5: Continue exact legacy logic...

        # CRITICAL: Default fallback logic
        if not company_id and not row.get("客户名称"):
            company_id = "600866980"

        return StepResult(
            row={**row, "company_id": company_id},
            warnings=[],
            errors=[]
        )

# Task 5: Service Integration
def process_with_enrichment(
    rows, data_source="unknown", enrichment_service=None,
    sync_lookup_budget=0, export_unknown_names=True,
    use_pipeline=None  # New parameter
):
    # PATTERN: Respect configuration hierarchy
    if use_pipeline is None:
        use_pipeline = get_settings().annuity_pipeline_enabled

    if use_pipeline:
        # New pipeline path
        pipeline = build_annuity_pipeline()
        processed_records = []
        for raw_row in rows:
            result = pipeline.execute(raw_row)
            if not result.errors:
                processed_records.append(transform_to_output_model(result.row))
    else:
        # Existing legacy path
        processed_records = [_transform_single_row(row, data_source, i) for i, row in enumerate(rows)]

    # CRITICAL: Enrichment integration identical in both paths
    # ... rest of enrichment logic unchanged
```

### Integration Points
```yaml
CONFIGURATION:
  - add to: src/work_data_hub/config/settings.py
  - pattern: "annuity_pipeline_enabled: bool = Field(True, description='Use shared pipeline for annuity processing')"

CLI:
  - add to: src/work_data_hub/orchestration/jobs.py
  - pattern: "add_argument('--use-pipeline/--no-use-pipeline', dest='use_pipeline', action=BooleanOptionalAction)"

ENRICHMENT:
  - preserve: CompanyEnrichmentService integration contract
  - maintain: EnrichmentStats structure and field names
  - keep: CSV export behavior and file naming patterns

LOGGING:
  - pattern: Use context.log in ops, logger in services
  - maintain: Metric field names and structured logging format
  - preserve: Processing time measurement and error boundaries
```

## Validation Loop

### Level 1: Syntax & Style
```bash
# Run these FIRST - fix any errors before proceeding
uv run ruff check src/ --fix              # Auto-fix style issues
uv run mypy src/                           # Type checking

# Expected: No errors. If errors, READ the error and fix.
```

### Level 2: Unit Tests
```python
# test_pipeline_vs_legacy.py
import pandas as pd
from pandas.testing import assert_frame_equal

def assert_dataframes_logically_equal(df1: pd.DataFrame, df2: pd.DataFrame, key_cols: list, rtol=1e-5, atol=1e-8):
    """Enhanced DataFrame comparison with canonicalization protocol."""
    # Canonical representation
    df1_canonical = df1.reindex(sorted(df1.columns), axis=1)
    df2_canonical = df2.reindex(sorted(df2.columns), axis=1)

    # Sort rows by key columns
    if key_cols:
        df1_canonical = df1_canonical.sort_values(by=key_cols).reset_index(drop=True)
        df2_canonical = df2_canonical.sort_values(by=key_cols).reset_index(drop=True)

    # Robust comparison with tolerance
    assert_frame_equal(
        df1_canonical, df2_canonical,
        check_exact=False, rtol=rtol, atol=atol, check_dtype=False
    )

class TestPipelineVsLegacy:
    def test_golden_baseline_reproduction(self):
        """Pipeline reproduces exact legacy output on curated subsets."""
        # Load golden reference (Parquet preserves data types)
        golden_df = pd.read_parquet("tests/fixtures/annuity_performance/golden_legacy.parquet")

        # Execute pipeline on same inputs
        pipeline = build_annuity_pipeline()
        pipeline_results = []
        for excel_file in CURATED_EXCEL_SUBSETS:
            rows = read_excel_rows(excel_file, sheet="规模明细")
            for row in rows:
                result = pipeline.execute(row)
                pipeline_results.append(result.row)

        # Compare outputs using enhanced comparison protocol
        pipeline_df = pd.DataFrame(pipeline_results)

        assert_dataframes_logically_equal(
            golden_df, pipeline_df,
            key_cols=["月度", "计划代码", "company_id"],
            rtol=1e-5, atol=1e-8
        )

    def test_enrichment_statistics_preservation(self):
        """Enrichment metrics match between legacy and pipeline paths."""
        # Mock enrichment service
        mock_enrichment = create_mock_enrichment_service()

        # Process with both paths
        legacy_result = process_with_enrichment(SAMPLE_ROWS, use_pipeline=False, enrichment_service=mock_enrichment)
        pipeline_result = process_with_enrichment(SAMPLE_ROWS, use_pipeline=True, enrichment_service=mock_enrichment)

        # Compare enrichment statistics
        assert legacy_result.enrichment_stats.total_records == pipeline_result.enrichment_stats.total_records
        assert legacy_result.enrichment_stats.success_internal == pipeline_result.enrichment_stats.success_internal
```

```bash
# Run tests iteratively until passing:
uv run pytest tests/domain/annuity_performance/test_pipeline_vs_legacy.py -v
uv run pytest tests/domain/annuity_performance/ -v

# If failing: Read error, understand root cause, fix code, re-run
```

### Level 3: Integration Test
```bash
# Generate golden baseline
uv run python scripts/tools/run_legacy_annuity_cleaner.py \
  --inputs tests/fixtures/sample_data/annuity_subsets \
  --output tests/fixtures/annuity_performance/golden_legacy.parquet

# Test CLI toggle functionality
WDH_DATA_BASE_DIR=tests/fixtures/sample_data/annuity_subsets \
WDH_ANNUITY_PIPELINE_ENABLED=true \
uv run python -m src.work_data_hub.orchestration.jobs \
  --domain annuity_performance --plan-only --max-files 1

# Test rollback capability
WDH_DATA_BASE_DIR=tests/fixtures/sample_data/annuity_subsets \
WDH_ANNUITY_PIPELINE_ENABLED=false \
uv run python -m src.work_data_hub.orchestration.jobs \
  --domain annuity_performance --plan-only --max-files 1

# Verify both modes produce equivalent results
```

## Final validation Checklist
- [ ] All tests pass: `uv run pytest tests/domain/annuity_performance/ -v`
- [ ] No linting errors: `uv run ruff check src/`
- [ ] No type errors: `uv run mypy src/`
- [ ] Golden baseline generation script works: `uv run python scripts/tools/run_legacy_annuity_cleaner.py --help`
- [ ] Pipeline toggle works: Both `--use-pipeline` and `--no-use-pipeline` execute successfully
- [ ] Parity tests pass: Pipeline output matches golden baseline with enhanced comparison protocol
- [ ] Data type preservation: Parquet format maintains schema integrity in golden baseline
- [ ] Floating-point precision handled: Tolerance-based comparisons prevent spurious failures
- [ ] Enrichment integration preserved: Statistics and CSV exports work identically
- [ ] Configuration documented: README includes validation commands and rollback procedures
- [ ] Manual verification completed: Spot-check sample records against known mappings

---

## Anti-Patterns to Avoid
- ❌ Don't modify legacy cleaner logic - extract and preserve as-is
- ❌ Don't skip golden baseline generation - critical for regression detection
- ❌ Don't change enrichment service integration - preserve exact behavior
- ❌ Don't ignore Chinese character encoding - handle UTF-8 properly
- ❌ Don't hardcode paths - use configuration and CLI arguments
- ❌ Don't batch validation steps - run incrementally and fix errors immediately

## Confidence Score: 9/10

High confidence due to:
- Clear legacy implementation to replicate
- Existing pipeline framework provides solid foundation
- Comprehensive mapping fixtures available for testing
- Well-defined integration patterns for enrichment
- Extensive test coverage exists for validation
- **Research-validated testing strategy**: Golden baseline + deterministic comparison protocols confirmed as industry best practice
- **Enhanced comparison toolkit**: Robust DataFrame comparison handling floating-point precision and ordering issues
- **Parquet format adoption**: Better data type preservation prevents comparison ambiguities

Minor uncertainty on:
- Exact mapping dictionary structure in production vs. test fixtures
- Performance impact of pipeline framework vs. direct pandas operations
