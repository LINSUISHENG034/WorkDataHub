# PRP P-026: Reference Backfill for Annuity Plans & Portfolios

---

## Goal
Implement reference backfill functionality that derives candidate reference rows (Annuity Plans & Portfolios) from processed fact batch data and safely inserts missing references before loading facts. Support both insert_missing (default) and fill_null_only modes with plan-only preview capability and CLI integration.

## Why
- **Data Integrity**: Ensures referential integrity by automatically creating missing reference records before fact table inserts
- **Operational Efficiency**: Eliminates manual reference data maintenance for new plans and portfolios discovered in fact data
- **Chinese Naming Support**: Properly handles Chinese identifiers (年金计划号, 组合代码) with PostgreSQL best practices
- **Fail-Safe Design**: Provides plan-only mode to preview operations before execution, preventing data corruption

## What
End-to-end reference backfill system that:
1. **Derives Candidates**: Extracts unique plan and portfolio references from processed fact data
2. **Safe Insertion**: Uses PostgreSQL `INSERT ... ON CONFLICT DO NOTHING` for concurrent-safe operations
3. **Flexible Modes**: Supports insert_missing (default) and fill_null_only for different use cases
4. **CLI Integration**: Adds `--backfill-refs` and `--backfill-mode` flags to existing job runner
5. **Plan-Only Support**: Generates execution plans for validation before database operations

### Success Criteria
- [ ] CLI flags parsed correctly and passed to orchestration layer
- [ ] Candidate derivation extracts correct plan/portfolio data from fact rows
- [ ] INSERT operations handle Chinese identifiers without SQL errors
- [ ] Plan-only mode generates executable SQL for preview
- [ ] Backfill operations complete before fact loading in job pipeline
- [ ] All validation gates pass with new functionality

## All Needed Context

### Documentation & References
```yaml
- file: src/work_data_hub/io/loader/warehouse_loader.py
  why: Existing PostgreSQL integration patterns, quote_ident function, batch operations

- file: src/work_data_hub/orchestration/ops.py  
  why: Op structure, Config classes, context logging patterns

- file: src/work_data_hub/orchestration/jobs.py
  why: CLI argument parsing, job composition, run_config building

- file: reference/db_migration/db_structure.json
  why: Reference table schemas - 年金计划 and 组合计划 tables

- file: src/work_data_hub/domain/annuity_performance/service.py
  why: Field mapping patterns, processed data structure

- file: tests/io/test_warehouse_loader.py
  why: Testing patterns for database operations and SQL building
```

### Implementation-Facing Research Notes
```yaml
sources:
  - research: PostgreSQL bulk insert best practices with conflict handling
    type: external_research
    why: Performance optimization for reference insertion

tldr:
  - INSERT ... ON CONFLICT DO NOTHING is superior to SELECT-then-INSERT for performance and concurrency safety
  - Batch operations with execute_values provide optimal throughput 
  - Chinese identifiers must be double-quoted with proper escaping
  - C collation offers best performance for technical identifier columns

api_decisions:
  - name: conflict_handling_strategy
    choice: "INSERT ... ON CONFLICT DO NOTHING"
    rationale: "Atomic operation, race condition safe, better performance than SELECT filtering"

  - name: identifier_quoting
    choice: "quote_ident() function reuse"
    rationale: "Existing function handles Chinese characters correctly with double quotes"

  - name: batch_size
    choice: "chunk_size parameter (default 1000)"
    rationale: "Consistent with existing loader patterns, tunable performance"

pitfalls_and_mitigations:
  - issue: "Race conditions with concurrent backfill operations"
    mitigation: "ON CONFLICT DO NOTHING handles duplicate insertions safely"
  
  - issue: "Chinese identifier SQL injection risks"
    mitigation: "Use quote_ident() for all table/column names, parameterized queries for values"

  - issue: "Memory usage with large candidate sets"
    mitigation: "Chunk processing similar to existing warehouse_loader pattern"
```

### Current Codebase Patterns
```python
# EXISTING: PostgreSQL identifier quoting for Chinese names
def quote_ident(name: str) -> str:
    escaped = name.replace('"', '""')
    return f'"{escaped}"'

# EXISTING: Config-driven ops with validation
class LoadConfig(Config):
    table: str = "sample_trustee_performance"
    mode: str = "delete_insert"
    pk: List[str] = ["report_date", "plan_code", "company_code"]

# EXISTING: Batch INSERT with execute_values  
execute_values(
    cursor,
    f"INSERT INTO {quoted_table} ({col_list}) VALUES %s",
    row_data,
    page_size=min(chunk_size, 1000),
)

# EXISTING: Op execution pattern with connection management
@op
def operation_op(context: OpExecutionContext, config: Config, data: List[Dict]) -> Dict:
    try:
        # Database connection handling
        # Business logic
        context.log.info("Operation completed", extra={"key": "value"})
        return result
    except Exception as e:
        context.log.error(f"Operation failed: {e}")
        raise
```

### Reference Table Schemas
```sql
-- 年金计划 (Annuity Plan) Table
CREATE TABLE "年金计划" (
    "年金计划号" TEXT PRIMARY KEY,  -- Plan code
    "计划全称" TEXT,               -- Plan full name
    "计划类型" TEXT,               -- Plan type
    "客户名称" TEXT,               -- Client name
    "company_id" TEXT              -- Company identifier
);

-- 组合计划 (Portfolio Plan) Table  
CREATE TABLE "组合计划" (
    "组合代码" TEXT PRIMARY KEY,   -- Portfolio code
    "年金计划号" TEXT,             -- Plan code (FK)
    "组合名称" TEXT,               -- Portfolio name
    "组合类型" TEXT,               -- Portfolio type
    "运作开始日" DATE               -- Operation start date
);
```

### Known Gotchas & Library Quirks
```python
# CRITICAL: Chinese identifiers require double quotes in PostgreSQL
# Correct: INSERT INTO "年金计划" ("年金计划号") VALUES (%s)
# Wrong:   INSERT INTO 年金计划 (年金计划号) VALUES (%s)  # Syntax error

# CRITICAL: psycopg2 connection cleanup in ops
# Always close connections in finally block to prevent leaks

# GOTCHA: Pydantic model_dump mode affects serialization
# Use mode="json" for database-friendly types (Decimal -> str, datetime -> ISO)

# GOTCHA: execute_values expects list of lists, not list of tuples
# Correct: [[val1, val2], [val3, val4]]
# Wrong:   [(val1, val2), (val3, val4)]
```

## Implementation Blueprint

### Data Models and Structure
```python
# New Config classes for backfill operations
class BackfillRefsConfig(Config):
    """Configuration for reference backfill operation."""
    targets: List[str] = Field(default_factory=list)  # ["plans", "portfolios", "all"]  
    mode: str = "insert_missing"  # or "fill_null_only"
    plan_only: bool = True
    chunk_size: int = 1000
    
    @field_validator("targets")
    @classmethod
    def validate_targets(cls, v: List[str]) -> List[str]:
        valid = ["plans", "portfolios", "all"]
        for target in v:
            if target not in valid:
                raise ValueError(f"Invalid target: {target}. Valid: {valid}")
        return v

# Reference candidate data structures
class AnnuityPlanCandidate(BaseModel):
    年金计划号: str
    计划全称: Optional[str] = None
    计划类型: Optional[str] = None  
    客户名称: Optional[str] = None
    company_id: Optional[str] = None

class PortfolioCandidate(BaseModel):
    组合代码: str
    年金计划号: str
    组合名称: Optional[str] = None
    组合类型: Optional[str] = None
    运作开始日: Optional[date] = None
```

### Task Sequence
```yaml
Task 1: Add Reference Backfill Helper Functions
MODIFY src/work_data_hub/io/loader/warehouse_loader.py:
  - ADD insert_missing() function with ON CONFLICT DO NOTHING
  - ADD fill_null_only() function with conditional UPDATE
  - REUSE existing quote_ident() and batch patterns
  - PRESERVE transaction safety and error handling

Task 2: Add Candidate Derivation Functions  
CREATE src/work_data_hub/domain/reference_backfill/:
  - CREATE service.py with derive_plan_candidates() and derive_portfolio_candidates()
  - EXTRACT from processed annuity fact rows
  - MAP fields: 计划代码 -> 年金计划号, 计划名称 -> 计划全称, etc.
  - DEDUPLICATE candidates by primary key

Task 3: Add Backfill Orchestration Ops
MODIFY src/work_data_hub/orchestration/ops.py:
  - ADD derive_plan_refs_op() and derive_portfolio_refs_op()
  - ADD backfill_refs_op() with BackfillRefsConfig
  - MIRROR existing op patterns for logging and error handling
  - INTEGRATE with connection management patterns

Task 4: Extend CLI and Job Composition
MODIFY src/work_data_hub/orchestration/jobs.py:
  - ADD --backfill-refs flag (choices: plans|portfolios|all)
  - ADD --backfill-mode flag (choices: insert_missing|fill_null_only)  
  - MODIFY annuity_performance_job() to include backfill ops
  - UPDATE build_run_config() to pass backfill settings

Task 5: Add Comprehensive Tests
CREATE tests/domain/reference_backfill/:
  - TEST candidate derivation with various input scenarios
  - TEST deduplication logic and field mapping
CREATE tests/io/ additions:
  - TEST insert_missing with ON CONFLICT behavior
  - TEST fill_null_only with NULL vs non-NULL updates
  - TEST Chinese identifier handling in SQL generation
MODIFY tests/orchestration/:
  - TEST new ops integration and CLI flag parsing
```

### Per Task Pseudocode

#### Task 1: Helper Functions
```python
# ADD to src/work_data_hub/io/loader/warehouse_loader.py

def insert_missing(
    table: str, 
    key_cols: List[str], 
    rows: List[Dict[str, Any]], 
    conn: Any,
    chunk_size: int = 1000
) -> Dict[str, Any]:
    """Insert rows that don't conflict with existing keys."""
    
    if not rows:
        return {"inserted": 0, "batches": 0}
    
    # Build ON CONFLICT SQL - key insight from research
    quoted_table = quote_ident(table)
    all_cols = _get_column_order(rows)
    quoted_cols = [quote_ident(col) for col in all_cols] 
    col_list = ",".join(quoted_cols)
    
    # ON CONFLICT clause for composite keys
    pk_cols = ",".join(quote_ident(col) for col in key_cols)
    
    inserted_total = 0
    batches = 0
    
    for i in range(0, len(rows), chunk_size):
        chunk = rows[i:i + chunk_size]
        
        # Build VALUES clause
        row_data = [[_adapt_param(row.get(col)) for col in all_cols] for row in chunk]
        
        # CRITICAL: Use ON CONFLICT DO NOTHING for safety
        sql = f"INSERT INTO {quoted_table} ({col_list}) VALUES %s ON CONFLICT ({pk_cols}) DO NOTHING"
        
        execute_values(cursor, sql, row_data, page_size=min(chunk_size, 1000))
        inserted_total += len(chunk)
        batches += 1
        
    return {"inserted": inserted_total, "batches": batches}

def fill_null_only(
    table: str,
    key_cols: List[str], 
    rows: List[Dict[str, Any]],
    updatable_cols: List[str],
    conn: Any
) -> Dict[str, Any]:
    """Update only NULL columns for existing rows."""
    
    # For each updatable column, build UPDATE with WHERE key match AND col IS NULL
    # Use batch parameters for efficiency
    updated_total = 0
    
    for col in updatable_cols:
        # Filter rows that have non-null values for this column
        update_rows = [row for row in rows if row.get(col) is not None]
        if not update_rows:
            continue
            
        # Build parameterized UPDATE
        quoted_table = quote_ident(table) 
        quoted_col = quote_ident(col)
        key_conditions = " AND ".join(f"{quote_ident(k)} = %s" for k in key_cols)
        
        sql = f"UPDATE {quoted_table} SET {quoted_col} = %s WHERE {key_conditions} AND {quoted_col} IS NULL"
        
        # Execute for each row (could be optimized with CASE WHEN for batching)
        for row in update_rows:
            params = [row[col]] + [row[k] for k in key_cols]
            cursor.execute(sql, params)
            updated_total += cursor.rowcount
            
    return {"updated": updated_total}
```

#### Task 2: Candidate Derivation
```python
# CREATE src/work_data_hub/domain/reference_backfill/service.py

def derive_plan_candidates(processed_rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Derive unique plan candidates from processed fact data."""
    
    plan_map = {}  # Key: 年金计划号, Value: candidate dict
    
    for row in processed_rows:
        plan_code = row.get("计划代码")
        if not plan_code:
            continue
            
        # Build candidate if not seen or merge additional fields
        if plan_code not in plan_map:
            plan_map[plan_code] = {
                "年金计划号": plan_code,
                "计划全称": row.get("计划名称"),
                "计划类型": row.get("计划类型"),
                "客户名称": row.get("客户名称"), 
                "company_id": row.get("company_id")
            }
        else:
            # Merge non-null values (last non-null wins)
            existing = plan_map[plan_code]
            for field in ["计划全称", "计划类型", "客户名称", "company_id"]:
                if row.get(field) and not existing.get(field):
                    existing[field] = row[field]
    
    return list(plan_map.values())

def derive_portfolio_candidates(processed_rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Derive unique portfolio candidates from processed fact data."""
    
    portfolio_map = {}  # Key: 组合代码, Value: candidate dict
    
    for row in processed_rows:
        portfolio_code = row.get("组合代码") 
        plan_code = row.get("计划代码")
        
        if not portfolio_code or not plan_code:
            continue
            
        if portfolio_code not in portfolio_map:
            portfolio_map[portfolio_code] = {
                "组合代码": portfolio_code,
                "年金计划号": plan_code,
                "组合名称": row.get("组合名称"),
                "组合类型": row.get("组合类型"),
                "运作开始日": row.get("运作开始日")  # Already processed by domain service
            }
        # Note: Could merge fields like plans, but portfolio code is usually unique per plan
    
    return list(portfolio_map.values())
```

#### Task 3: Orchestration Ops
```python
# ADD to src/work_data_hub/orchestration/ops.py

@op
def derive_plan_refs_op(
    context: OpExecutionContext,
    processed_rows: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """Derive plan reference candidates from processed fact data."""
    
    try:
        candidates = derive_plan_candidates(processed_rows)
        
        context.log.info(
            "Plan candidate derivation completed",
            extra={
                "input_rows": len(processed_rows),
                "unique_plans": len(candidates),
                "domain": "annuity_performance"
            }
        )
        
        return candidates
        
    except Exception as e:
        context.log.error(f"Plan candidate derivation failed: {e}")
        raise

@op 
def backfill_refs_op(
    context: OpExecutionContext,
    config: BackfillRefsConfig,
    plan_candidates: List[Dict[str, Any]],
    portfolio_candidates: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """Execute reference backfill operations."""
    
    # PATTERN: Mirror load_op connection handling
    conn = None
    try:
        # Connection setup (mirror load_op pattern)
        if not config.plan_only:
            global psycopg2
            # ... (connection setup logic)
            conn = psycopg2.connect(dsn)
            
        result = {"operations": [], "plan_only": config.plan_only}
        
        # Execute backfill for each target
        if "plans" in config.targets or "all" in config.targets:
            if config.mode == "insert_missing":
                plan_result = insert_missing(
                    table="年金计划", 
                    key_cols=["年金计划号"],
                    rows=plan_candidates,
                    conn=conn,
                    chunk_size=config.chunk_size
                )
            elif config.mode == "fill_null_only":
                plan_result = fill_null_only(
                    table="年金计划",
                    key_cols=["年金计划号"], 
                    rows=plan_candidates,
                    updatable_cols=["计划全称", "计划类型", "客户名称", "company_id"],
                    conn=conn
                )
            result["operations"].append({"table": "年金计划", **plan_result})
        
        # Similar for portfolios...
        
        context.log.info("Reference backfill completed", extra=result)
        return result
        
    except Exception as e:
        context.log.error(f"Reference backfill failed: {e}")
        raise
    finally:
        if conn:
            conn.close()
```

### Integration Points
```yaml
DATABASE:
  - tables: "年金计划", "组合计划" (existing)
  - operation: INSERT with ON CONFLICT DO NOTHING
  - constraints: Primary key constraints on 年金计划号, 组合代码
  
CONFIG:
  - extend: jobs.py argparse with --backfill-refs, --backfill-mode flags
  - pattern: build_run_config() includes backfill settings in ops config
  
ORCHESTRATION:
  - modify: annuity_performance_job() composition
  - pattern: discover → read → process → derive_refs → backfill_refs → load_facts
```

## Validation Loop

### Level 1: Syntax & Style
```bash
# Run these FIRST - fix any errors before proceeding
uv run ruff check src/ --fix
uv run mypy src/

# Expected: No errors. If errors, READ the error and fix.
```

### Level 2: Unit Tests
```python
# CREATE tests/domain/reference_backfill/test_service.py
def test_derive_plan_candidates_basic():
    """Test basic plan candidate derivation."""
    input_rows = [
        {"计划代码": "PLAN001", "计划名称": "Test Plan", "计划类型": "DC"},
        {"计划代码": "PLAN001", "客户名称": "Client A"},  # Merge test
        {"计划代码": "PLAN002", "计划名称": "Another Plan"}
    ]
    
    candidates = derive_plan_candidates(input_rows)
    
    assert len(candidates) == 2
    plan001 = next(c for c in candidates if c["年金计划号"] == "PLAN001")
    assert plan001["计划全称"] == "Test Plan" 
    assert plan001["客户名称"] == "Client A"  # Merged

def test_derive_plan_candidates_empty_input():
    """Test handling of empty input.""" 
    assert derive_plan_candidates([]) == []

# CREATE tests/io/test_warehouse_loader_backfill.py  
def test_insert_missing_on_conflict():
    """Test ON CONFLICT DO NOTHING behavior."""
    # Test with mock connection and cursor
    # Verify SQL generation includes ON CONFLICT clause
    # Verify proper parameter binding with Chinese identifiers

def test_fill_null_only_preserves_existing():
    """Test that non-NULL values are preserved.""" 
    # Mock cursor with controlled rowcount
    # Verify WHERE clause includes IS NULL condition
```

```bash
# Run and iterate until passing:
uv run pytest tests/domain/reference_backfill/ -v
uv run pytest tests/io/ -k backfill -v
# If failing: Read error, understand root cause, fix code, re-run
```

### Level 3: Integration Test
```bash
# Test plan-only mode first (safe)
WDH_DATA_BASE_DIR="tests/fixtures/sample_data/annuity_subsets" \
  uv run python -m src.work_data_hub.orchestration.jobs \
  --domain annuity_performance --plan-only --max-files 1 \
  --backfill-refs all --backfill-mode insert_missing

# Expected: SQL plans generated for both 年金计划 and 组合计划 tables
# Check logs for candidate derivation counts and SQL preview

# Test execution mode (requires test database)
WDH_DATA_BASE_DIR="tests/fixtures/sample_data/annuity_subsets" \
  uv run python -m src.work_data_hub.orchestration.jobs \
  --domain annuity_performance --execute --max-files 1 \
  --backfill-refs plans --backfill-mode insert_missing

# Expected: References inserted before fact loading
# Verify idempotent behavior (run twice, second run inserts 0)
```

## Final Validation Checklist
- [ ] All tests pass: `uv run pytest tests/ -v`
- [ ] No linting errors: `uv run ruff check src/`
- [ ] No type errors: `uv run mypy src/`
- [ ] Plan-only mode generates correct SQL with Chinese identifiers
- [ ] CLI flags parsed and passed correctly to ops
- [ ] Candidate derivation extracts expected reference data
- [ ] ON CONFLICT DO NOTHING prevents duplicate insert errors
- [ ] Backfill operations complete before fact loading in job pipeline
- [ ] Error cases logged appropriately with context

## Quality Assessment
**Confidence Score: 8/10**

**Strengths:**
- Comprehensive context from existing codebase patterns
- External research validates PostgreSQL approach  
- Plan-only mode enables safe validation
- Follows established architectural patterns

**Potential Challenges:**
- Chinese identifier edge cases in SQL generation
- Performance with large candidate sets
- Connection management complexity in ops

**Risk Mitigation:**
- Extensive unit tests for SQL building functions
- Chunked processing for memory efficiency  
- Existing quote_ident() function handles Chinese characters
- Plan-only validation before execution

---

## Anti-Patterns to Avoid
- ❌ Don't use SELECT-then-INSERT (race condition prone, poor performance)
- ❌ Don't skip identifier quoting for Chinese table/column names
- ❌ Don't create new connection management patterns (reuse existing)
- ❌ Don't ignore ON CONFLICT clause (essential for concurrent safety)
- ❌ Don't batch operations without chunking (memory risk)
- ❌ Don't skip plan-only validation (prevents data corruption)