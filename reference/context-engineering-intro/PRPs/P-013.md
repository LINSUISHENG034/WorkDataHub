name: "Trustee Performance E2E Execution Validation & Testing Enhancement"
description: |

## Purpose
Comprehensive validation and testing enhancement to ensure the trustee_performance Dagster job executes successfully end-to-end with proper DB context handling, JSONB parameter adaptation, and decimal precision validation. Focus on test coverage and validation gates rather than new implementation.

## Core Principles
1. **Validation First**: Ensure existing fixes work correctly through comprehensive testing
2. **Comprehensive Coverage**: Test all edge cases for DB context, JSONB, and decimal handling
3. **E2E Confidence**: Validate both plan-only and execute modes work reliably
4. **Documentation**: Document the solutions and patterns for future reference

---

## Goal
Validate and enhance testing for the trustee_performance E2E execution pipeline to ensure reliable operation with PostgreSQL database connections, JSONB parameter handling, and decimal precision validation.

## Why
- **Business value**: Ensures trustee performance data processing is reliable and error-free
- **Integration**: Validates the complete ETL pipeline from Excel files to PostgreSQL
- **Problems solved**: Prevents regressions in DB connection handling, JSONB processing, and decimal validation

## What
Comprehensive validation suite that verifies:
- DB connection lifecycle management without recursion errors
- JSONB parameter adaptation for PostgreSQL execute_values operations
- Decimal field quantization and precision handling
- End-to-end job execution in both plan-only and execute modes

### Success Criteria
- [ ] All existing tests pass with enhanced coverage
- [ ] E2E execute mode completes successfully without connection errors
- [ ] JSONB parameters are correctly adapted and stored
- [ ] Decimal precision validation handles float precision issues correctly
- [ ] Plan-only mode continues to work for testing scenarios
- [ ] All validation gates (ruff, mypy, pytest) pass

## All Needed Context

### Documentation & References
```yaml
# MUST READ - Include these in your context window
- url: https://docs.pydantic.dev/2.11/usage/validators/#field-validators
  why: Pydantic v2 field validator patterns with FieldValidationInfo
  
- url: https://docs.python.org/3/library/decimal.html#decimal.Decimal.quantize
  why: Decimal quantization best practices for precision handling
  
- url: https://www.psycopg.org/docs/extras.html#json-adaptation
  why: psycopg2 JSON/JSONB adaptation with extras.Json
  
- url: https://www.psycopg.org/docs/connection.html#connection
  why: psycopg2 connection context manager best practices
  
- file: src/work_data_hub/orchestration/ops.py
  why: Current DB connection lifecycle implementation
  
- file: src/work_data_hub/io/loader/warehouse_loader.py  
  why: JSONB parameter adaptation and transaction management
  
- file: src/work_data_hub/domain/trustee_performance/models.py
  why: Decimal quantization with Pydantic v2 validators
  
- file: tests/io/test_warehouse_loader.py
  why: Existing test patterns for DB and JSONB operations
  
- file: tests/orchestration/test_ops.py
  why: Existing test patterns for orchestration operations
```

### Current Codebase tree
```bash
src/work_data_hub/
  orchestration/
    ops.py               # DB connection lifecycle (load_op)
    jobs.py              # CLI and run_config
  io/
    loader/warehouse_loader.py  # JSONB adaptation and transaction management
    readers/excel_reader.py      # Excel data reading
  domain/trustee_performance/
    models.py            # Pydantic v2 decimal quantization
    service.py           # Data transformation service
  config/
    settings.py          # Database connection configuration
    data_sources.yml     # Table and schema configuration
tests/
  orchestration/test_ops.py           # Ops testing patterns
  io/test_warehouse_loader.py         # Loader testing patterns
  domain/trustee_performance/         # Domain model testing
```

### Desired Codebase tree with files to be enhanced
```bash
tests/
  orchestration/
    test_ops.py                      # Enhanced DB connection lifecycle tests
  io/
    test_warehouse_loader.py         # Enhanced JSONB and DB integration tests
  domain/trustee_performance/
    test_models.py                   # Enhanced decimal validation tests
  e2e/
    test_trustee_performance_e2e.py  # NEW: End-to-end validation tests
```

### Known Gotchas & Library Quirks
```python
# CRITICAL: psycopg2 connection context manager nesting
# Only one layer should manage transactions - warehouse_loader uses `with conn:`
# ops.py should use bare connection and let warehouse_loader handle transactions

# CRITICAL: Pydantic v2 decimal_max_places validation
# Float precision issues require conversion to string before Decimal construction
# Use str(float_value) before Decimal() to avoid binary float precision errors

# CRITICAL: JSONB parameter adaptation for execute_values
# Dict/list values must be wrapped with psycopg2.extras.Json for JSONB columns
# Adaptation happens per-parameter in execute_values data preparation

# CRITICAL: Windows paths in test fixtures
# Use forward slashes or Path objects to avoid path separator issues

# CRITICAL: Test database isolation
# Use @pytest.mark.postgres for live DB tests and temp tables for isolation
```

## Implementation Blueprint

### Data models and validation patterns

Current implementation should handle these scenarios correctly:
```python
# Decimal quantization patterns from models.py
FIELD_PRECISION_MAP = {
    "return_rate": 6,        # NUMERIC(8,6) - handles percentage precision
    "net_asset_value": 4,    # NUMERIC(18,4) - handles currency precision  
    "fund_scale": 2          # NUMERIC(18,2) - handles large number precision
}

# JSONB data structures that need adaptation
JSONB_TEST_DATA = {
    "validation_warnings": ["warning1", "warning2"],
    "metadata": {"source": "test", "processed": True}
}
```

### List of validation tasks to be completed

```yaml
Task 1: Enhance DB Connection Lifecycle Testing
ENHANCE tests/orchestration/test_ops.py:
  - ADD comprehensive connection context manager tests
  - VERIFY bare connection usage in execute mode
  - TEST connection cleanup in success and failure scenarios
  - VALIDATE no context manager nesting occurs

Task 2: Enhance JSONB Parameter Adaptation Testing  
ENHANCE tests/io/test_warehouse_loader.py:
  - EXPAND TestJSONBParameterAdaptation test coverage
  - TEST complex nested dict/list structures
  - VERIFY execute_values handles Json-wrapped parameters correctly
  - ADD integration tests with actual JSONB columns

Task 3: Enhance Decimal Precision Validation Testing
ENHANCE tests/domain/trustee_performance/test_models.py:
  - TEST float precision edge cases that trigger decimal_max_places
  - VERIFY quantization works for all field types
  - TEST percentage conversion with precision handling
  - VALIDATE string-to-decimal conversion patterns

Task 4: Create Comprehensive E2E Validation Tests
CREATE tests/e2e/test_trustee_performance_e2e.py:
  - TEST complete ETL pipeline with sample data
  - VERIFY plan-only mode execution
  - VALIDATE execute mode with test database
  - TEST error handling and recovery scenarios

Task 5: Enhance Test Infrastructure
MODIFY existing test files:
  - ADD pytest fixtures for consistent test data
  - IMPROVE database test isolation patterns
  - ENHANCE mocking for unit tests vs integration tests
  - STANDARDIZE test naming and organization
```

### Per task validation pseudocode

```python
# Task 1: DB Connection Testing
def test_load_op_connection_lifecycle():
    """Verify proper DB connection handling without nesting."""
    # PATTERN: Mock psycopg2.connect to return bare connection
    with patch('psycopg2.connect') as mock_connect:
        mock_conn = Mock()
        mock_connect.return_value = mock_conn
        
        # VERIFY: Connection created without context manager
        load_op(context, config, rows)
        mock_connect.assert_called_once()
        
        # VERIFY: Connection closed in finally block
        mock_conn.close.assert_called_once()

# Task 3: Decimal Precision Testing  
def test_decimal_quantization_handles_float_precision():
    """Test quantization of problematic float values."""
    # CRITICAL: Test exact float values that cause precision errors
    test_cases = [
        (0.048799999999999996, "return_rate", Decimal("0.048800")),
        (1.0512000000000001, "net_asset_value", Decimal("1.0512")),
        (12000000.003, "fund_scale", Decimal("12000000.00"))
    ]
    
    for input_val, field_name, expected in test_cases:
        # PATTERN: Use FieldValidationInfo mock
        info = Mock(spec=FieldValidationInfo)
        info.field_name = field_name
        
        result = TrusteePerformanceOut.clean_decimal_fields(input_val, info)
        assert result == expected
```

### Integration Points
```yaml
ENVIRONMENT:
  - verify: .env configuration for database connection
  - test: Both development and test database scenarios
  
DATABASE:  
  - schema: Existing trustee_performance table with JSONB columns
  - isolation: Use temp tables for integration tests
  - connection: Verify connection string building from settings
  
CLI:
  - commands: Test both --plan-only and --execute modes
  - validation: Ensure --max-files parameter works correctly
  - output: Verify success/failure reporting
```

## Validation Loop

### Level 1: Syntax & Style
```bash
# Run these FIRST - must pass before proceeding
uv run ruff check src/ tests/ --fix
uv run mypy src/ 

# Expected: No errors. Critical requirement for validation.
```

### Level 2: Unit Tests with Enhanced Coverage
```bash
# Run enhanced unit tests with coverage reporting
uv run pytest tests/orchestration/test_ops.py -v --cov=src.work_data_hub.orchestration
uv run pytest tests/io/test_warehouse_loader.py -v --cov=src.work_data_hub.io.loader  
uv run pytest tests/domain/trustee_performance/ -v --cov=src.work_data_hub.domain.trustee_performance

# Expected: All tests pass with >90% coverage on enhanced modules
```

### Level 3: Integration Tests
```bash
# Test with actual database (requires PostgreSQL)
uv run pytest tests/ -v -m "postgres" --maxfail=1

# Expected: All database integration tests pass
```

### Level 4: End-to-End Validation
```bash
# Plan-only mode (should always work)
uv run python -m src.work_data_hub.orchestration.jobs --plan-only --max-files 2

# Execute mode (requires database connection)  
uv run python -m src.work_data_hub.orchestration.jobs --execute --max-files 2

# Expected outcomes:
# - Plan-only: Job succeeds, returns SQL plans with DELETE + INSERT operations
# - Execute: Job succeeds, no connection recursion errors, rows inserted correctly
```

## Final Validation Checklist
- [ ] All enhanced unit tests pass: `uv run pytest tests/ -v`
- [ ] No linting errors: `uv run ruff check src/ tests/`
- [ ] No type errors: `uv run mypy src/`
- [ ] Database integration tests pass: `uv run pytest -m postgres -v`
- [ ] Plan-only E2E execution succeeds
- [ ] Execute mode E2E execution succeeds (with database)
- [ ] JSONB columns accept complex data structures
- [ ] Decimal precision validation handles float edge cases
- [ ] Connection lifecycle management prevents recursion errors
- [ ] Test coverage >90% on critical modules
- [ ] Error handling provides clear, actionable messages

---

## Anti-Patterns to Avoid
- ❌ Don't create new implementations - focus on validation and testing
- ❌ Don't skip database integration tests - they catch real-world issues
- ❌ Don't ignore float precision edge cases in decimal testing
- ❌ Don't nest psycopg2 context managers across layers
- ❌ Don't use mocks where integration tests are more valuable
- ❌ Don't skip E2E validation - it's the ultimate test

## Quality Assurance Notes

### Test Data Preparation
```python
# Use realistic test data that mirrors production scenarios
TRUSTEE_PERFORMANCE_SAMPLE = [
    {
        "report_date": "2024-01-01",
        "plan_code": "PLAN001", 
        "company_code": "COMP001",
        "return_rate": 0.048799999999999996,  # Problematic float precision
        "net_asset_value": 1.0512000000000001,  # Another float edge case
        "fund_scale": 12000000.003,  # Large number precision issue
        "validation_warnings": ["Sample warning"],  # JSONB list
        "metadata": {"source": "test", "processed": True}  # JSONB dict
    }
]
```

### Database Test Isolation
```python
@pytest.mark.postgres
def test_with_temp_table(db_connection):
    """Pattern for isolated database testing."""
    with db_connection.cursor() as cursor:
        # Create temp table that mirrors production schema
        cursor.execute("""
            CREATE TEMP TABLE test_trustee_performance (
                report_date DATE,
                plan_code VARCHAR(50),
                company_code VARCHAR(20),
                return_rate NUMERIC(8,6),
                net_asset_value NUMERIC(18,4), 
                fund_scale NUMERIC(18,2),
                validation_warnings JSONB,
                PRIMARY KEY (report_date, plan_code, company_code)
            )
        """)
        
        # Test operations use temp table - automatic cleanup
```

## Confidence Score: 8/10

High confidence due to:
- Clear understanding of the existing fixes that need validation
- Comprehensive test patterns already established in codebase  
- Well-documented external APIs and library behaviors
- Focus on validation rather than complex new implementation

Minor uncertainty on edge cases in decimal precision handling and potential database connection nuances in Windows environment, but comprehensive testing will surface these issues.