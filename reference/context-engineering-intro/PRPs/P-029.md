name: "P-029: Backfill Hardening & Configurability (Skip Facts, Config-Driven Refs/Schema, Qualified SQL)"
description: |

## Purpose
Implement enhanced Reference Backfill functionality with sophisticated Annuity Plan derivations, configurable skip-facts mode, and qualified SQL generation. This PRP provides comprehensive context for one-pass implementation of the remaining functionality missing from the already-implemented infrastructure.

## Core Principles
1. **Context is King**: Include ALL necessary documentation, examples, and caveats
2. **Validation Loops**: Provide executable tests/lints the AI can run and fix
3. **Information Dense**: Use keywords and patterns from the codebase
4. **Progressive Success**: Start simple, validate, then enhance
5. **Global rules**: Be sure to follow all rules in CLAUDE.md

---

## Goal
Complete the implementation of Reference Backfill hardening with sophisticated Annuity Plan derivations that handle tie-breaking, date formatting, and business type filtering according to precise specifications. Enable safe backfill-only execution through skip-facts mode.

## Why
- **Business value**: Enables sophisticated data derivation for annuity plan references with deterministic tie-breaking rules
- **Integration**: Builds on existing backfill infrastructure to add enhanced derivation logic
- **Problems solved**: Provides configurable backfill execution, qualified SQL generation, and robust plan reference creation from fact data

## What
Enhanced derivation logic for Annuity Plan candidates with sophisticated aggregation rules:
- 客户名称: Most frequent value with tie-breaking by maximum 期末资产规模
- 主拓代码, 主拓机构: From the row with maximum 期末资产规模
- 备注: Date formatted as `YYMM_新建` from 月度 field
- 资格: Filtered business types in specific order joined with '+'

### Success Criteria
- [ ] Enhanced plan derivation logic correctly handles all tie-breaking scenarios
- [ ] Skip-facts mode works end-to-end (backfill only, no fact loading)
- [ ] Qualified SQL generation uses configured schema properly
- [ ] All existing and new tests pass
- [ ] Documentation updated with configuration examples and validation results

## All Needed Context

### Documentation & References
```yaml
# MUST READ - Include these in your context window
- file: src/work_data_hub/orchestration/jobs.py
  why: Skip-facts functionality already implemented (lines 278-283, 162, 434-443)

- file: src/work_data_hub/orchestration/ops.py
  why: Backfill infrastructure and config reading (lines 665-694, 711, 720, 767, 776)

- file: src/work_data_hub/io/loader/warehouse_loader.py
  why: Qualified SQL generation already implemented (quote_qualified function, lines 46-78)

- file: src/work_data_hub/domain/reference_backfill/service.py
  why: Basic derivation logic exists but needs enhancement (lines 16-82)

- file: src/work_data_hub/config/data_sources.yml
  why: Configuration structure already supports refs schema (lines 67-78)

- file: tests/domain/reference_backfill/test_service.py
  why: Existing test patterns to follow for new derivation logic

- file: tests/orchestration/test_jobs_run_config.py
  why: Skip-facts testing patterns (lines 37-72)
```

### Implementation-Facing Research Notes

```yaml
sources:
  - research: "Python data aggregation best practices with pandas/collections"
    type: gemini_research
    why: "Sophisticated tie-breaking logic for 客户名称 derivation"

  - research: "Chinese date parsing and YYMM formatting"
    type: gemini_research
    why: "备注 field generation from 月度 dates"

  - research: "Set operations and ordered string filtering"
    type: gemini_research
    why: "资格 field generation from 业务类型 values"

tldr:
  - Use pandas-style aggregation with custom functions for tie-breaking
  - datetime.strptime() with %Y%m format for date parsing, handle edge cases
  - List comprehension with membership check preserves order for business type filtering
  - Collections.Counter for frequency analysis with max() for tie-breaking
  - F-string formatting for YYMM_新建 pattern: `f"{year % 100:02d}{month:02d}_新建"`

setup_commands:
  - "Infrastructure already exists - no new dependencies needed"
  - "Tests can be run with: uv run pytest -v -k 'backfill or derivation or skip_facts'"

api_decisions:
  - name: "Tie-breaking strategy"
    choice: "Most frequent value, then max numeric column for ties"
    rationale: "Deterministic and reproducible results"

  - name: "Date formatting approach"
    choice: "Parse to datetime then format with f-strings"
    rationale: "Robust handling of various input formats"

  - name: "Business type filtering"
    choice: "Preserve predefined order through list comprehension"
    rationale: "Maintains business requirement ordering"

versions:
  - library: "No new libraries needed"
    constraint: "Uses existing pandas, datetime, collections"
    compatibility: "python 3.11, pydantic v2, existing project deps"

pitfalls_and_mitigations:
  - issue: "Tie-breaking must be deterministic"
    mitigation: "Use stable sorting and first-occurrence selection"

  - issue: "Date parsing can fail on various formats"
    mitigation: "Try multiple parsing strategies with error handling"

  - issue: "Empty or null aggregation inputs"
    mitigation: "Handle edge cases explicitly with default returns"

open_questions:
  - "Should we validate the enhanced derivations against existing data before deploying?"
```

### Current Codebase Analysis

**✅ Already Implemented:**
- Skip-facts CLI flag and execution logic (`jobs.py` lines 278-283, `ops.py` lines 434-443)
- Qualified SQL generation (`warehouse_loader.py` lines 46-78: `quote_qualified` function)
- Config-driven refs reading (`ops.py` lines 665-694)
- Basic plan/portfolio derivation (`domain/reference_backfill/service.py`)
- Test infrastructure for skip-facts (`tests/orchestration/test_jobs_run_config.py`)

**❌ Missing Implementation:**
- Enhanced Annuity Plan derivation logic with sophisticated tie-breaking
- Tests for new derivation scenarios
- README/VALIDATION.md documentation updates

### Known Gotchas & Library Quirks
```python
# CRITICAL: Existing quote_qualified function handles schema properly
# Example: quote_qualified("public", "年金计划") returns "public"."年金计划"

# CRITICAL: Skip-facts already implemented via LoadConfig.skip field
# No need to modify job graph - load_op checks config.skip and returns early

# CRITICAL: Refs config already read from data_sources.yml
# Structure: domains.annuity_performance.refs.plans.{schema,table,key,updatable}

# CRITICAL: Use deterministic tie-breaking for reproducible results
# pandas idxmax() returns first occurrence for tied maximums

# CRITICAL: Handle edge cases in derivation logic
# Empty inputs, null values, invalid dates must not break the pipeline
```

## Implementation Blueprint

### Data models and structure

The core data models already exist and don't need changes. Focus on service-layer enhancements:

```python
# Enhanced derivation function signatures (to be implemented)
def derive_enhanced_plan_candidates(processed_rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Enhanced plan derivation with sophisticated business logic:
    - 客户名称: most frequent + tie-break by max 期末资产规模
    - 主拓代码, 主拓机构: from row with max 期末资产规模
    - 备注: YYMM_新建 format from 月度
    - 资格: filtered business types in specific order
    """
```

### List of tasks to be completed to fulfill the PRP

```yaml
Task 1: Implement Enhanced Plan Derivation Logic
MODIFY src/work_data_hub/domain/reference_backfill/service.py:
  - FIND function: derive_plan_candidates
  - REPLACE with enhanced implementation containing sophisticated aggregation logic
  - ADD helper functions for tie-breaking, date formatting, business type filtering
  - PRESERVE existing function signature and error handling patterns

Task 2: Add Derivation Helper Functions
ADD to src/work_data_hub/domain/reference_backfill/service.py:
  - _get_most_frequent_with_tiebreak(): pandas-style aggregation with tie-breaking
  - _format_remark_from_date(): YYMM_新建 formatting from various date inputs
  - _format_qualification_from_business_types(): ordered filtering and joining
  - FOLLOW existing code style and error handling patterns

Task 3: Update Tests for Enhanced Derivation
MODIFY tests/domain/reference_backfill/test_service.py:
  - ADD test cases for tie-breaking scenarios (most frequent value conflicts)
  - ADD test cases for date formatting edge cases (various input formats)
  - ADD test cases for business type filtering and ordering
  - ADD test cases for edge cases (empty inputs, null values)
  - MIRROR existing test patterns and naming conventions

Task 4: Add Integration Tests
ADD to tests/orchestration/:
  - Test skip-facts end-to-end execution (backfill only)
  - Test qualified SQL generation with schema configuration
  - Test enhanced derivations in full pipeline context
  - FOLLOW existing e2e test patterns

Task 5: Update Documentation
MODIFY README.md:
  - ADD section on refs configuration (schema/table/key/updatable)
  - ADD section on skip-facts usage and examples
  - ADD section on enhanced derivation logic explanation

CREATE/UPDATE VALIDATION.md:
  - ADD validation results showing skip-facts execution
  - ADD examples of qualified SQL generation
  - ADD examples of enhanced plan derivations
```

### Per task pseudocode

```python
# Task 1: Enhanced Plan Derivation Logic
def derive_plan_candidates(processed_rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Enhanced implementation with sophisticated business logic."""
    if not processed_rows:
        return []

    # Group by plan code for aggregation
    grouped_data = defaultdict(list)
    for row in processed_rows:
        plan_code = row.get("计划代码")
        if plan_code:
            grouped_data[plan_code].append(row)

    candidates = []
    for plan_code, rows in grouped_data.items():
        candidate = {
            "年金计划号": plan_code,

            # BUSINESS RULE: Most frequent 客户名称, tie-break by max 期末资产规模
            "客户名称": _get_most_frequent_with_tiebreak(
                rows, "客户名称", "期末资产规模"
            ),

            # BUSINESS RULE: From row with max 期末资产规模
            "主拓代码": _get_value_from_max_row(rows, "期末资产规模", "计划代码"),
            "主拓机构": _get_value_from_max_row(rows, "期末资产规模", "机构名称"),

            # BUSINESS RULE: Format as YYMM_新建 from 月度
            "备注": _format_remark_from_date(rows[0].get("月度")),

            # BUSINESS RULE: Filter and order business types
            "资格": _format_qualification_from_business_types(
                {row.get("业务类型") for row in rows}
            ),

            # Standard fields
            "计划全称": rows[0].get("计划名称"),
            "计划类型": rows[0].get("计划类型"),
            "company_id": rows[0].get("company_id"),
        }
        candidates.append(candidate)

    return candidates

# Task 2: Helper Functions
def _get_most_frequent_with_tiebreak(rows, value_col, tiebreak_col):
    """Find most frequent value, break ties with max tiebreak_col."""
    # PATTERN: Use Counter for frequency, max() for tie-breaking
    from collections import Counter

    values = [row.get(value_col) for row in rows if row.get(value_col)]
    if not values:
        return None

    counts = Counter(values)
    max_freq = max(counts.values())

    # Get all values with max frequency
    tied_values = [val for val, freq in counts.items() if freq == max_freq]

    if len(tied_values) == 1:
        return tied_values[0]

    # Tie-breaking: find rows with tied values, get max tiebreak_col
    tied_rows = [row for row in rows if row.get(value_col) in tied_values]

    # CRITICAL: Handle numeric conversion and comparison safely
    max_row = max(tied_rows, key=lambda r: _safe_numeric(r.get(tiebreak_col)))
    return max_row.get(value_col)

def _format_remark_from_date(date_input):
    """Convert date to YYMM_新建 format."""
    # PATTERN: Multiple parsing strategies with error handling
    if not date_input:
        return None

    try:
        # Handle datetime objects directly
        if isinstance(date_input, (datetime.datetime, datetime.date)):
            dt_obj = date_input
        # Handle string/int YYYYMM format
        elif str(date_input).isdigit() and len(str(date_input)) == 6:
            dt_obj = datetime.strptime(str(date_input), "%Y%m")
        # Handle date strings
        else:
            dt_obj = dateutil.parser.parse(str(date_input))

        # Format as YYMM_新建
        return f"{dt_obj.year % 100:02d}{dt_obj.month:02d}_新建"

    except Exception:
        return None  # Graceful degradation

def _format_qualification_from_business_types(business_types_set):
    """Filter business types and join in specific order."""
    # BUSINESS RULE: Specific order must be preserved
    ALLOWED_ORDER = ["企年受托", "企年投资", "职年受托", "职年投资"]

    # PATTERN: Iterate through order list, not input set
    filtered = [bt for bt in ALLOWED_ORDER if bt in business_types_set]

    return "+".join(filtered) if filtered else None
```

### Integration Points
```yaml
CLI:
  - Skip-facts flag: --skip-facts (already implemented in jobs.py)
  - Example usage: --domain annuity_performance --execute --backfill-refs all --skip-facts

CONFIG:
  - Refs configuration: data_sources.yml domains.annuity_performance.refs
  - Schema support: refs.plans.schema, refs.portfolios.schema

DATABASE:
  - Qualified SQL: "public"."年金计划" via quote_qualified function
  - ON CONFLICT handling: existing fallback logic preserved
```

## Validation Loop

### Level 1: Syntax & Style
```bash
# Run these FIRST - fix any errors before proceeding
uv run ruff check src/work_data_hub/domain/reference_backfill/ --fix
uv run mypy src/work_data_hub/domain/reference_backfill/

# Expected: No errors. If errors, READ the error and fix.
```

### Level 2: Unit Tests
```python
# Key test scenarios to implement
def test_enhanced_plan_derivation_tie_breaking():
    """Test tie-breaking logic for most frequent values."""
    # Setup rows with tied frequencies but different 期末资产规模
    rows = [
        {"计划代码": "PLAN001", "客户名称": "Client A", "期末资产规模": 1000000},
        {"计划代码": "PLAN001", "客户名称": "Client A", "期末资产规模": 2000000},
        {"计划代码": "PLAN001", "客户名称": "Client B", "期末资产规模": 1500000},
        {"计划代码": "PLAN001", "客户名称": "Client B", "期末资产规模": 500000},
    ]
    # Both clients appear twice (tied), but Client A has max 期末资产规模 (2M)

    candidates = derive_plan_candidates(rows)
    assert len(candidates) == 1
    assert candidates[0]["客户名称"] == "Client A"  # Won tie-break

def test_remark_formatting_various_dates():
    """Test date formatting handles various inputs."""
    # Test different input formats
    assert _format_remark_from_date(202411) == "2411_新建"
    assert _format_remark_from_date("202411") == "2411_新建"
    assert _format_remark_from_date(datetime.date(2024, 11, 1)) == "2411_新建"
    assert _format_remark_from_date("2024-11-01") == "2411_新建"
    assert _format_remark_from_date("invalid") is None  # Graceful failure

def test_qualification_ordering():
    """Test business type filtering and ordering."""
    business_types = {"职年投资", "其他类型", "企年受托"}
    result = _format_qualification_from_business_types(business_types)
    # Should preserve order: 企年受托 + 职年投资 (skip 其他类型)
    assert result == "企年受托+年+职年投资"
```

```bash
# Run and iterate until passing:
uv run pytest tests/domain/reference_backfill/test_service.py -v -k "derivation"
uv run pytest tests/orchestration/ -v -k "skip_facts"

# If failing: Read error, understand root cause, fix code, re-run
```

### Level 3: Integration Test
```bash
# Test skip-facts mode end-to-end
uv run python -m src.work_data_hub.orchestration.jobs \
  --domain annuity_performance \
  --execute \
  --backfill-refs all \
  --skip-facts \
  --sheet "规模明细" \
  --debug

# Expected:
# - "Skip facts: True" in output
# - Reference Backfill Summary shows operations
# - No fact loading summary (skipped)
# - Qualified SQL with schema in logs

# Test qualified SQL generation
# Expected SQL patterns: INSERT INTO "public"."年金计划" (...)
```

## Final Validation Checklist
- [ ] All tests pass: `uv run pytest tests/ -v -k "backfill or derivation or skip_facts"`
- [ ] No linting errors: `uv run ruff check src/`
- [ ] No type errors: `uv run mypy src/`
- [ ] Manual skip-facts test successful with backfill-only execution
- [ ] Enhanced derivations produce expected results on sample data
- [ ] Qualified SQL appears in logs with proper schema prefixes
- [ ] Documentation updated with configuration examples
- [ ] VALIDATION.md contains evidence of all acceptance criteria

---

## Anti-Patterns to Avoid
- ❌ Don't create new infrastructure - most functionality already exists
- ❌ Don't modify job graph structure - use existing skip flag in load_op
- ❌ Don't skip tie-breaking determinism - use stable sorting approaches
- ❌ Don't ignore edge cases in derivation logic - handle nulls/empties gracefully
- ❌ Don't hardcode business logic - make it testable and maintainable
- ❌ Don't break existing functionality - preserve backward compatibility

## Confidence Score: 9/10

High confidence due to:
- Comprehensive analysis shows most infrastructure already implemented
- Clear identification of missing components (enhanced derivation logic)
- Well-researched Python best practices for complex aggregation scenarios
- Existing test patterns provide clear implementation guidance
- Focused scope on completing specific derivation enhancements

Minor uncertainty only on edge cases in real data that may need refinement during implementation.