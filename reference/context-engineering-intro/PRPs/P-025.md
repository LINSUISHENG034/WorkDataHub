name: "End-to-End Testing for Annuity Performance Delete_Insert vs Append Modes"
description: |

## Purpose
Implement comprehensive end-to-end and unit testing for annuity performance (规模明细) data processing using real business samples derived into small test datasets. This validates the semantic correctness of `delete_insert` vs `append` modes with runtime PK overrides, ensuring data pipeline robustness and observability.

## Core Principles
1. **Context is King**: Include ALL necessary documentation, examples, and caveats
2. **Validation Loops**: Provide executable tests/lints the AI can run and fix
3. **Information Dense**: Use keywords and patterns from the codebase
4. **Progressive Success**: Start simple, validate, then enhance
5. **Global rules**: Be sure to follow all rules in CLAUDE.md

---

## Goal
Create a robust testing framework for annuity performance ETL operations that validates both delete_insert (覆盖写入) and append (非覆盖写入) modes using strategically generated small datasets derived from real business data. Enable runtime primary key overrides via `--pk` CLI parameter and ensure correct SQL plan generation and row counting.

## Why
- **Business value**: Prevents data corruption in production annuity performance reporting by validating ETL semantic correctness
- **Integration**: Ensures warehouse loader, orchestration ops, and CLI work together correctly
- **Problems solved**: Validates complex composite key handling, runtime PK overrides, and mode-specific behaviors that could cause data loss or duplication

## What
A comprehensive test suite that:
- Generates 3 strategically designed test datasets (5/6/3 rows) from real Excel samples
- Validates delete_insert mode with default and runtime-overridden primary keys
- Tests append mode independence from primary key requirements
- Provides plan-only testing for CI/CD environments without database dependencies
- Ensures loader validation catches configuration errors early

### Success Criteria
- [ ] Subset generation script creates 3 files with expected row distributions
- [ ] E2E tests validate correct deleted/inserted counts for each scenario
- [ ] Runtime PK override (`--pk`) correctly changes delete planning behavior
- [ ] Loader unit tests catch delete_insert mode without PK configuration
- [ ] All tests run in plan-only mode without database connection
- [ ] Tests are deterministic and suitable for CI/CD automation

## All Needed Context

### Documentation & References
```yaml
# MUST READ - Include these in your context window
- file: src/work_data_hub/domain/annuity_performance/models.py
  why: AnnuityPerformanceIn/Out models with Chinese field mappings and validation
  
- file: src/work_data_hub/domain/annuity_performance/service.py
  why: process() function and transformation logic patterns
  
- file: src/work_data_hub/io/loader/warehouse_loader.py
  why: delete_insert vs append mode implementation and plan-only testing
  
- file: src/work_data_hub/orchestration/ops.py
  why: LoadConfig validation and plan_only parameter handling
  
- file: src/work_data_hub/orchestration/jobs.py
  why: CLI argument parsing, --pk override, and build_run_config patterns
  
- file: src/work_data_hub/config/data_sources.yml
  why: annuity_performance domain configuration with default PK settings
  
- file: tests/domain/annuity_performance/test_service.py
  why: Existing test patterns for domain service validation
  
- file: tests/io/test_warehouse_loader.py  
  why: Loader testing patterns and validation error handling
  
- file: tests/e2e/test_trustee_performance_e2e.py
  why: E2E test structure and environment variable mocking patterns
```

### Implementation-Facing Research Notes
Purpose: Consolidate external research into actionable, implementation-facing notes to avoid duplicate searching and reduce information drift.

```yaml
sources:
  - url: https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html
    section: Excel reading parameters
    why: Subset generation script Excel manipulation
    type: docs
  
  - url: https://docs.pytest.org/en/stable/reference/reference.html#pytest.MonkeyPatch
    section: monkeypatch.setenv usage
    why: Environment variable isolation in tests
    type: docs

tldr:
  - Use pandas.read_excel() with sheet_name parameter for consistent sheet reading
  - Generate subsets using strategic sampling: distinct records, overlapping PKs, append scenarios
  - Use monkeypatch.setenv('WDH_DATA_BASE_DIR', subset_dir) for test isolation
  - Plan-only mode returns SQL plans without database connection for CI compatibility
  - LoadConfig validation happens at configuration time, catches missing PK early

setup_commands:
  - "# Generate subsets: uv run python -m scripts.testdata.make_annuity_subsets"
  - "# Run E2E tests: uv run pytest -v -k annuity_overwrite_append"
  - "# Set test data dir: export WDH_DATA_BASE_DIR=tests/fixtures/sample_data/annuity_subsets"

api_decisions:
  - name: Subset generation strategy
    choice: Driver table approach with strategic PK sampling
    rationale: Maintains data relationships while testing specific scenarios
  
  - name: Test isolation method
    choice: monkeypatch.setenv for WDH_DATA_BASE_DIR
    rationale: Follows existing test patterns in codebase
    
  - name: Testing mode
    choice: Plan-only mode for E2E tests
    rationale: No database dependency, suitable for CI/CD

versions:
  - library: pandas
    constraint: "already in dependencies"
    compatibility: Python 3.10+, existing uv environment
  
  - library: pytest
    constraint: "already in dependencies" 
    compatibility: Existing monkeypatch and fixture patterns

pitfalls_and_mitigations:
  - issue: File naming must match data_sources.yml pattern
    mitigation: Use exact pattern "2024年11月年金终稿数据_subset_*.xlsx" format
  
  - issue: Windows path handling in environment variables
    mitigation: Use Path objects and proper path separators in test setup
    
  - issue: Non-deterministic subset generation
    mitigation: Use fixed random seed or deterministic selection criteria
    
  - issue: LoadConfig validation timing
    mitigation: Validation happens at config parse time, not execution time

open_questions:
  - Should subsets be committed to git or generated on-demand in tests?
  - Need to verify sheet name "规模明细" exists in source file before processing
```

### Current Codebase Tree
```bash
src/work_data_hub/
  config/
    data_sources.yml         # annuity_performance domain config
    settings.py              # WDH_DATA_BASE_DIR environment variable
  domain/
    annuity_performance/
      models.py              # AnnuityPerformanceIn/Out with Chinese fields
      service.py             # process() transformation function
  io/
    loader/warehouse_loader.py # delete_insert/append mode implementation
  orchestration/
    jobs.py                  # CLI with --pk override capability  
    ops.py                   # LoadConfig validation and load_op

tests/
  domain/annuity_performance/
    test_service.py          # Domain service test patterns
  e2e/
    test_trustee_performance_e2e.py # E2E test structure reference
  io/
    test_warehouse_loader.py # Loader testing patterns
  fixtures/
    sample_data/             # Location for test data files
```

### Desired Codebase Tree with files to be added
```bash
scripts/
  testdata/
    __init__.py              # Package initialization
    make_annuity_subsets.py  # Subset generation script

tests/
  fixtures/
    sample_data/
      annuity_subsets/       # Generated test datasets directory
        2024年11月年金终稿数据_subset_distinct_5.xlsx
        2024年11月年金终稿数据_subset_overlap_pk_6.xlsx  
        2024年11月年金终稿数据_subset_append_3.xlsx
  e2e/
    test_annuity_overwrite_append_small_subsets.py  # Main E2E test file
```

### Known Gotchas & Library Quirks
```python
# CRITICAL: File naming must match annuity_performance pattern in data_sources.yml
# Pattern: "(?P<year>\\d{2}|20\\d{2})年(?P<month>0?[1-9]|1[0-2])月.*年金.*终稿数据.*\\.(xlsx|xlsm)$"
# Use: "2024年11月年金终稿数据_subset_*.xlsx" format

# CRITICAL: Default PK from data_sources.yml is ["月度", "计划代码", "company_id"] 
# Runtime override via --pk affects only delete_insert planning, not append mode

# CRITICAL: LoadConfig validation happens at Dagster config parse time
# Missing PK for delete_insert raises ValidationError before op execution

# CRITICAL: Plan-only mode (conn=None) returns sql_plans without database execution
# Essential for CI/CD environments without database access

# CRITICAL: Environment variable isolation using monkeypatch.setenv
# Prevents test cross-contamination and follows existing patterns
```

## Implementation Blueprint

### Data Models and Structure
The existing AnnuityPerformanceIn/Out models handle Chinese field names and validation:
```python
# From src/work_data_hub/domain/annuity_performance/models.py
class AnnuityPerformanceOut(BaseModel):
    计划代码: str = Field(..., min_length=1)  # Plan code (PK component)
    company_id: str = Field(..., min_length=1)  # Company ID (PK component)
    月度: Optional[date] = Field(None)  # Report date (PK component)
    # ... other financial fields
```

### List of tasks to be completed

```yaml
Task 1: CREATE scripts/testdata/make_annuity_subsets.py
PURPOSE: Generate strategic test datasets from real Excel samples
PATTERN: Use pandas for Excel manipulation, follow driver table strategy
CREATE scripts/testdata/make_annuity_subsets.py:
  - READ source Excel file with sheet_name="规模明细"
  - GENERATE 3 subsets using strategic selection:
    * distinct_5.xlsx: 5 rows with unique PK combinations (月度, 计划代码, company_id)
    * overlap_pk_6.xlsx: 6 rows with 3 unique PK combinations (tests delete count)
    * append_3.xlsx: 3 rows with completely different PKs
  - PRESERVE Chinese column names and data relationships
  - WRITE to tests/fixtures/sample_data/annuity_subsets/ with proper naming

Task 2: CREATE tests/e2e/test_annuity_overwrite_append_small_subsets.py  
PURPOSE: End-to-end validation of delete_insert vs append modes
PATTERN: Follow tests/e2e/test_trustee_performance_e2e.py structure
CREATE tests/e2e/test_annuity_overwrite_append_small_subsets.py:
  - USE monkeypatch.setenv for WDH_DATA_BASE_DIR isolation
  - TEST CASE A: delete_insert with default PK (distinct_5 dataset)
    * Validate deleted count matches unique PK combinations
    * Validate inserted count equals 5
  - TEST CASE B: delete_insert with overlapping PK (overlap_pk_6 dataset)  
    * Validate deleted count equals 3 (unique PK combinations)
    * Validate inserted count equals 6
  - TEST CASE C: delete_insert with --pk override
    * Use "月度,计划代码" override on overlap_pk_6
    * Validate deleted count reflects two-column grouping
  - TEST CASE D: append mode validation
    * Use append_3 dataset with mode="append"
    * Validate deleted count equals 0
    * Validate inserted count equals 3

Task 3: EXTEND tests/io/test_warehouse_loader.py
PURPOSE: Unit test validation error handling
PATTERN: Use existing pytest.raises patterns for validation errors
ADD to tests/io/test_warehouse_loader.py:
  - TEST delete_insert mode without PK raises DataWarehouseLoaderError
  - TEST append mode works correctly without PK requirement  
  - VERIFY error messages are actionable and specific
```

### Per Task Pseudocode

```python
# Task 1: Subset Generation Script
def main():
    """Generate strategic subsets from source Excel file."""
    # PATTERN: Use argparse like existing scripts
    parser = argparse.ArgumentParser()
    parser.add_argument("--src", required=True, help="Source Excel file path")
    parser.add_argument("--sheet", default="规模明细", help="Sheet name")
    args = parser.parse_args()
    
    # CRITICAL: Use pandas for reliable Excel reading
    df = pd.read_excel(args.src, sheet_name=args.sheet)
    logger.info(f"Loaded {len(df)} rows from {args.src}")
    
    # STRATEGY: Create distinct subset (5 rows, unique PKs)
    distinct_subset = df.drop_duplicates(subset=["月度", "计划代码", "company_id"]).head(5)
    
    # STRATEGY: Create overlapping PK subset (6 rows, 3 unique PK combinations)
    unique_pks = df.drop_duplicates(subset=["月度", "计划代码", "company_id"]).head(3)
    overlap_subset = pd.concat([
        unique_pks,
        unique_pks.sample(3, replace=True)  # Duplicate some rows
    ]).reset_index(drop=True)
    
    # STRATEGY: Create append subset (3 rows, different PKs)
    append_subset = df.drop_duplicates(subset=["月度", "计划代码", "company_id"]).tail(3)
    
    # CRITICAL: Follow naming pattern for data_sources.yml matching
    output_dir = Path("tests/fixtures/sample_data/annuity_subsets")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    distinct_subset.to_excel(output_dir / "2024年11月年金终稿数据_subset_distinct_5.xlsx", index=False)
    overlap_subset.to_excel(output_dir / "2024年11月年金终稿数据_subset_overlap_pk_6.xlsx", index=False)
    append_subset.to_excel(output_dir / "2024年11月年金终稿数据_subset_append_3.xlsx", index=False)

# Task 2: E2E Test Implementation
@pytest.fixture
def subset_dir():
    """Return path to generated subset directory."""
    return "tests/fixtures/sample_data/annuity_subsets"

def test_delete_insert_distinct_pk_default(monkeypatch, subset_dir):
    """Test delete_insert with distinct PKs using default configuration."""
    # PATTERN: Environment isolation like existing E2E tests
    monkeypatch.setenv("WDH_DATA_BASE_DIR", subset_dir)
    
    # PATTERN: Use orchestration jobs.py CLI interface
    args = build_args(domain="annuity_performance", mode="delete_insert", plan_only=True, max_files=1)
    run_config = build_run_config(args)
    
    # CRITICAL: Execute in plan-only mode (no database required)
    result = annuity_performance_job.execute_in_process(run_config=run_config, instance=None)
    
    load_result = result.output_for_node("load_op")
    
    # VALIDATION: Expected counts based on subset characteristics  
    assert load_result["deleted"] == 5  # Unique PK combinations in distinct_5
    assert load_result["inserted"] == 5
    assert load_result["mode"] == "delete_insert"
    assert "sql_plans" in load_result  # Plan-only mode
    
def test_delete_insert_with_pk_override(monkeypatch, subset_dir):
    """Test delete_insert with runtime PK override."""
    monkeypatch.setenv("WDH_DATA_BASE_DIR", subset_dir)
    
    # CRITICAL: Use --pk override functionality
    args = build_args(domain="annuity_performance", mode="delete_insert", 
                     plan_only=True, pk="月度,计划代码")  # Override default PK
    
    run_config = build_run_config(args)
    result = annuity_performance_job.execute_in_process(run_config=run_config, instance=None)
    
    load_result = result.output_for_node("load_op")
    
    # VALIDATION: Different delete count due to two-column grouping
    # overlap_pk_6 has 6 rows but fewer unique (月度,计划代码) combinations
    expected_deleted = calculate_unique_combinations(overlap_data, ["月度", "计划代码"])
    assert load_result["deleted"] == expected_deleted
    assert load_result["inserted"] == 6

# Task 3: Loader Unit Test Extension  
def test_delete_insert_requires_pk():
    """Test that delete_insert mode requires primary key configuration."""
    rows = [{"id": 1, "data": "test"}]
    
    # CRITICAL: Missing PK should raise DataWarehouseLoaderError
    with pytest.raises(DataWarehouseLoaderError, match="Primary key required"):
        load(table="test_table", rows=rows, mode="delete_insert", pk=[], conn=None)

def test_append_mode_ignores_pk_requirement():
    """Test that append mode works without primary key."""
    rows = [{"id": 1, "data": "test"}]
    
    # PATTERN: append mode should not require PK
    result = load(table="test_table", rows=rows, mode="append", pk=[], conn=None)
    
    assert result["mode"] == "append"
    assert result["deleted"] == 0  # append never deletes
    assert result["inserted"] == 1
```

### Integration Points
```yaml
ENVIRONMENT:
  - variable: WDH_DATA_BASE_DIR
    purpose: Points to subset directory for file discovery
    test_value: tests/fixtures/sample_data/annuity_subsets
  
CONFIG:
  - file: src/work_data_hub/config/data_sources.yml
    section: annuity_performance domain
    usage: Default table and PK configuration, pattern matching
    
CLI_INTEGRATION:
  - parameter: --pk
    purpose: Runtime primary key override for delete_insert planning
    example: "--pk '月度,计划代码,company_id'"
    
ORCHESTRATION:
  - job: annuity_performance_job
    ops_flow: discover_files_op -> read_excel_op -> process_annuity_performance_op -> load_op
    test_mode: plan_only=True for database-free testing
```

## Validation Loop

### Level 1: Syntax & Style
```bash
# Run these FIRST - fix any errors before proceeding
uv run ruff check src/ --fix
uv run ruff check scripts/ --fix  
uv run mypy src/

# Expected: No errors. If errors, READ the error and fix.
```

### Level 2: Unit Tests
```python
# Test subset generation
def test_subset_generation_creates_expected_files():
    """Verify subset script creates files with correct row counts."""
    # Run subset generation script
    result = subprocess.run([
        "uv", "run", "python", "-m", "scripts.testdata.make_annuity_subsets",
        "--src", "tests/fixtures/sample_data/【for年金分战区经营分析】24年11月年金终稿数据1209采集.xlsx",
        "--sheet", "规模明细"
    ], capture_output=True, text=True)
    
    assert result.returncode == 0
    
    # Verify files exist with expected row counts
    distinct_df = pd.read_excel("tests/fixtures/sample_data/annuity_subsets/2024年11月年金终稿数据_subset_distinct_5.xlsx")
    assert len(distinct_df) == 5
    
    overlap_df = pd.read_excel("tests/fixtures/sample_data/annuity_subsets/2024年11月年金终稿数据_subset_overlap_pk_6.xlsx")  
    assert len(overlap_df) == 6

def test_loader_validation_errors():
    """Test loader catches configuration errors early."""
    rows = [{"计划代码": "TEST001", "company_id": "COMP001"}]
    
    # delete_insert without PK should fail
    with pytest.raises(DataWarehouseLoaderError, match="Primary key required"):
        load(table="test", rows=rows, mode="delete_insert", pk=[], conn=None)
        
    # append should work without PK  
    result = load(table="test", rows=rows, mode="append", pk=[], conn=None)
    assert result["deleted"] == 0
    assert result["inserted"] == 1
```

```bash
# Run tests iteratively until passing:
uv run pytest tests/e2e/test_annuity_overwrite_append_small_subsets.py -v
uv run pytest tests/io/test_warehouse_loader.py::test_delete_insert_requires_pk -v

# If failing: Debug specific test, understand root cause, fix code, re-run
```

### Level 3: Integration Test  
```bash
# Test complete CLI workflow with subset generation
cd E:\Projects\WorkDataHub

# Generate subsets
uv run python -m scripts.testdata.make_annuity_subsets \
  --src "tests/fixtures/sample_data/【for年金分战区经营分析】24年11月年金终稿数据1209采集.xlsx" \
  --sheet "规模明细"

# Test delete_insert with default PK (plan-only)
export WDH_DATA_BASE_DIR=tests/fixtures/sample_data/annuity_subsets
uv run python -m src.work_data_hub.orchestration.jobs \
  --domain annuity_performance --plan-only --max-files 1

# Expected: SQL plans with DELETE and INSERT operations, correct counts

# Test with PK override
uv run python -m src.work_data_hub.orchestration.jobs \
  --domain annuity_performance --plan-only --max-files 1 \
  --mode delete_insert --pk "月度,计划代码"

# Expected: Different delete count due to two-column grouping

# Test append mode  
uv run python -m src.work_data_hub.orchestration.jobs \
  --domain annuity_performance --plan-only --max-files 1 \
  --mode append

# Expected: DELETE count = 0, only INSERT operations
```

## Final Validation Checklist
- [ ] Subset generation script creates 3 files with expected row counts
- [ ] All E2E tests pass: `uv run pytest -v -k annuity_overwrite_append`
- [ ] No linting errors: `uv run ruff check src/ scripts/`
- [ ] No type errors: `uv run mypy src/`
- [ ] CLI workflows work with generated subsets
- [ ] delete_insert mode correctly calculates deleted counts based on PK
- [ ] Runtime PK override changes delete planning behavior
- [ ] append mode ignores PK requirements
- [ ] Plan-only mode generates SQL without database connection
- [ ] Error cases handled gracefully with actionable messages

---

## Anti-Patterns to Avoid
- ❌ Don't hardcode file paths - use Path objects and environment variables
- ❌ Don't commit large or sensitive test data - use small, anonymized subsets  
- ❌ Don't skip plan-only testing - essential for CI/CD compatibility
- ❌ Don't ignore file naming patterns - must match data_sources.yml regex
- ❌ Don't forget environment variable isolation in tests
- ❌ Don't assume database availability in CI - use plan-only mode

## Confidence Score: 9/10

High confidence due to:
- Comprehensive codebase analysis and existing pattern identification
- Clear implementation path with specific file modifications  
- Actionable external research on test data generation best practices
- Executable validation gates with specific commands
- Well-established testing patterns in the codebase to follow

Minor uncertainty on exact row count calculations for overlapping PK scenarios, but the implementation approach provides clear validation steps to verify correctness.