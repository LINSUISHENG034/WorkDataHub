name: "Legacy Mapping Extraction - Formalize AnnuityPerformanceCleaner Mapping Dependencies"
description: |

## Purpose
Extract and formalize all mapping dictionaries consumed by the legacy `AnnuityPerformanceCleaner` into version-controlled JSON fixtures so the new pipeline can reuse the exact lookup data without hitting MySQL. This eliminates database dependencies for critical business logic mappings.

## Core Principles
1. **Context is King**: Include ALL necessary documentation, examples, and caveats
2. **Validation Loops**: Provide executable tests/lints the AI can run and fix
3. **Information Dense**: Use keywords and patterns from the codebase
4. **Progressive Success**: Start simple, validate, then enhance
5. **Global rules**: Be sure to follow all rules in CLAUDE.md

---

## Goal
Create a robust mapping extraction utility that reads migrated PostgreSQL tables and generates standardized JSON fixture files for all 10 mapping dictionaries referenced by the legacy `AnnuityPerformanceCleaner`, ensuring complete data fidelity and UTF-8 compatibility for Chinese characters.

## Why
- **Business value**: Eliminates MySQL dependency for mapping lookups in the new pipeline
- **Integration**: Critical for annuity performance domain migration from legacy system
- **Problems solved**: Provides version-controlled, testable mapping data for downstream services

## What
A Python extraction utility that:
- Connects to migrated PostgreSQL database using existing connection patterns
- Extracts 10 mapping dictionaries from various tables with proper SQL queries
- Generates standardized JSON files with metadata and UTF-8 encoding
- Documents missing entries and validation gaps for manual backfill
- Follows existing codebase patterns for database operations and JSON export

### Success Criteria
- [ ] JSON files exist for all 10 mappings listed in INITIAL.md inventory table
- [ ] Each JSON includes metadata (description, source, export timestamp)
- [ ] UTF-8 encoding properly preserves Chinese characters
- [ ] Missing entries documented in `_missing_entries.md` report
- [ ] Validation commands execute successfully to verify export integrity
- [ ] Row count verification between source tables and exported JSON

## All Needed Context

### Documentation & References
```yaml
# MUST READ - Include these in your context window
- file: legacy/annuity_hub/data_handler/mappings.py
  why: Source of truth for all 10 mapping functions and SQL queries

- file: src/work_data_hub/config/settings.py
  why: DatabaseSettings class and connection string pattern

- file: tests/fixtures/sample_legacy_mappings.json
  why: Existing JSON structure and metadata pattern to follow

- file: scripts/demos/manual_test_verification.py
  why: PostgreSQL connection pattern with proper imports

- file: legacy/annuity_hub/scripts/export_db_structure.py
  why: JSON export pattern with UTF-8 encoding and error handling

- url: https://www.postgresql.org/docs/current/functions-aggregate.html#FUNCTIONS-AGGREGATE-TABLE
  section: jsonb_object_agg function for efficient key-value aggregation
  critical: Essential for memory-efficient mapping extraction
```

### Implementation-Facing Research Notes
Purpose: Consolidate external research into actionable, implementation-facing notes to avoid duplicate searching and reduce information drift.

```yaml
sources:
  - url: https://www.postgresql.org/docs/current/functions-json.html#FUNCTIONS-JSON-CREATION-TABLE
    section: JSON creation functions (jsonb_build_object, jsonb_object_agg)
    why: Efficient database-side JSON aggregation patterns
    type: docs

tldr:
  - Use jsonb_object_agg(key, value) for efficient key-value pair creation in SQL
  - jsonb_build_object() for structured metadata inclusion in single query
  - COALESCE() function handles NULL values with sensible defaults
  - Named cursors (server-side) prevent memory issues with large datasets
  - ensure_ascii=False in json.dump() preserves Chinese characters

setup_commands:
  - "# Database connection using existing patterns"
  - "from src.work_data_hub.config.settings import get_settings"
  - "import psycopg2"
  - "settings = get_settings(); dsn = settings.get_database_connection_string()"

api_decisions:
  - name: JSON export function
    choice: json.dump(data, f, indent=2, ensure_ascii=False)
    rationale: Preserves UTF-8 Chinese characters and provides readable formatting

versions:
  - library: psycopg2
    constraint: "already in project"
    compatibility: PostgreSQL connection, utf-8 support

pitfalls_and_mitigations:
  - issue: Chinese characters become \uXXXX escape codes
    mitigation: Use ensure_ascii=False and encoding='utf-8' in file operations
  - issue: Large result sets cause memory issues
    mitigation: Use server-side cursor for streaming (though single JSON object pattern avoids this)
  - issue: NULL values break JSON structure
    mitigation: Use COALESCE(value, 'missing') in SQL queries

open_questions:
  - Verify all referenced table names exist in migrated PostgreSQL schema
  - Confirm column names match between MySQL legacy and PostgreSQL migrated tables
```

### Current Codebase tree (relevant portions)
```bash
src/work_data_hub/
├── config/
│   └── settings.py                 # DatabaseSettings class, get_settings()
├── io/loader/
│   └── warehouse_loader.py         # psycopg2 import patterns
└── orchestration/
    └── ops.py                      # Database connection patterns

legacy/annuity_hub/data_handler/
└── mappings.py                     # Source mapping functions (10 total)

tests/fixtures/
├── mappings/                       # Target directory for JSON exports
└── sample_legacy_mappings.json     # Structure template to follow

scripts/
├── demos/
│   └── manual_test_verification.py # PostgreSQL connection example
└── other/                          # Target directory for export utility
```

### Desired Codebase tree with files to be added
```bash
scripts/tools/
└── export_mapping.py              # Main extraction utility

tests/fixtures/mappings/
├── company_id1_mapping.json       # Plan → company_id (年金计划)
├── company_id2_mapping.json       # Account → company_id (annuity_account_mapping)
├── company_id3_mapping.json       # Hardcoded dict from mappings.py
├── company_id4_mapping.json       # Name → company_id (company_id_mapping)
├── company_id5_mapping.json       # Account name → company_id (规模明细)
├── company_branch_mapping.json    # Institution → code (组织架构)
├── business_type_code_mapping.json # Business type → code (产品线)
├── default_portfolio_code_mapping.json # Hardcoded dict from mappings.py
├── product_id_mapping.json        # Product → ID (产品明细, if used)
├── profit_metrics_mapping.json    # Metrics → code (利润指标, if used)
└── _missing_entries.md             # Missing data report
```

### Known Gotchas & Library Quirks
```python
# CRITICAL: PostgreSQL table names may differ from MySQL legacy names
# Example: MySQL `年金计划` vs PostgreSQL equivalent table name

# CRITICAL: psycopg2 requires proper encoding handling for Chinese characters
# Must use encoding='utf-8' in file operations and ensure_ascii=False in json.dump

# CRITICAL: Legacy mappings.py uses MySQL connections - do NOT copy connection logic
# Use existing DatabaseSettings from src/work_data_hub/config/settings.py instead

# CRITICAL: Some mappings in legacy code are hardcoded dictionaries (not DB queries)
# These must be extracted from the Python code itself, not database tables

# CRITICAL: Verify table existence before querying - migration may be incomplete
# Use information_schema.tables to check table existence first
```

## Implementation Blueprint

### Data models and structure

Create core data models to ensure type safety and validation consistency.

```python
# Models for structured mapping extraction
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional
from datetime import datetime

class MappingMetadata(BaseModel):
    description: str
    source: str  # table name or "hardcoded_dictionary"
    export_timestamp: datetime
    extractor_version: str = "1.0"
    row_count: Optional[int] = None

class MappingExport(BaseModel):
    metadata: MappingMetadata
    data: Dict[str, str]  # key -> value mappings

    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }

class ExtractionPlan(BaseModel):
    mapping_name: str
    source_type: str  # "database" or "hardcoded"
    table_name: Optional[str] = None
    sql_query: Optional[str] = None
    hardcoded_data: Optional[Dict[str, str]] = None
    output_file: str
```

### List of tasks to be completed to fulfill the PRP in the order they should be completed

```yaml
Task 1: Create extraction utility foundation
CREATE scripts/tools/export_mapping.py:
  - PATTERN: Mirror database connection from scripts/demos/manual_test_verification.py
  - PRESERVE: Use existing DatabaseSettings from config/settings.py
  - INJECT: Pydantic models for type safety and validation

Task 2: Implement database-backed mapping extraction
MODIFY scripts/tools/export_mapping.py:
  - FIND pattern: Legacy MySQL queries in mappings.py
  - CONVERT: Translate to PostgreSQL equivalents using jsonb_object_agg
  - PRESERVE: Original SQL logic and filtering (e.g., WHERE 计划类型='单一计划')

Task 3: Implement hardcoded mapping extraction
MODIFY scripts/tools/export_mapping.py:
  - FIND pattern: Hardcoded dictionaries in mappings.py (COMPANY_ID3_MAPPING, DEFAULT_PORTFOLIO_CODE_MAPPING)
  - EXTRACT: Dictionary data directly from Python code
  - PRESERVE: Exact key-value pairs without modification

Task 4: Add metadata and UTF-8 support
MODIFY scripts/tools/export_mapping.py:
  - PATTERN: Follow tests/fixtures/sample_legacy_mappings.json structure
  - INJECT: Export timestamp, source information, row counts
  - ENSURE: UTF-8 encoding with ensure_ascii=False for Chinese characters

Task 5: Implement missing data detection and reporting
MODIFY scripts/tools/export_mapping.py:
  - DETECT: Empty query results or obvious key gaps
  - CREATE: tests/fixtures/mappings/_missing_entries.md with detailed context
  - CONTINUE: Export with available data, don't fail entire run

Task 6: Add CLI interface and validation commands
MODIFY scripts/tools/export_mapping.py:
  - PATTERN: argparse CLI like existing scripts
  - SUPPORT: Single mapping export and bulk export modes
  - PROVIDE: Row count verification and sample key display

Task 7: Create comprehensive validation script
CREATE scripts/tools/validate_mappings.py:
  - VERIFY: JSON structure matches expected schema
  - CHECK: UTF-8 character preservation
  - COMPARE: Row counts between source and exported data
  - REPORT: Missing entries and data quality metrics

Task 8: Generate all 10 mapping files
EXECUTE scripts/tools/export_mapping.py:
  - RUN: Export for each mapping in INITIAL.md inventory table
  - VERIFY: All Chinese characters properly preserved
  - DOCUMENT: Any missing tables or incomplete data in _missing_entries.md
```

### Per task pseudocode

```python
# Task 2: Database-backed extraction pseudocode
async def extract_mapping_from_database(mapping_config: ExtractionPlan) -> MappingExport:
    # PATTERN: Follow existing psycopg2 connection pattern
    settings = get_settings()
    dsn = settings.get_database_connection_string()

    with psycopg2.connect(dsn) as conn:
        with conn.cursor() as cursor:
            # CRITICAL: Check table existence first
            cursor.execute("""
                SELECT EXISTS (
                    SELECT 1 FROM information_schema.tables
                    WHERE table_name = %s
                );
            """, (mapping_config.table_name,))

            if not cursor.fetchone()[0]:
                raise TableNotFoundError(f"Table {mapping_config.table_name} not found")

            # PATTERN: Use jsonb_object_agg for efficient key-value extraction
            cursor.execute(f"""
                SELECT jsonb_object_agg(key_column, value_column)
                FROM {mapping_config.table_name}
                WHERE {mapping_config.where_clause}
            """)

            result = cursor.fetchone()[0] or {}

            # PATTERN: Include metadata with export
            metadata = MappingMetadata(
                description=mapping_config.description,
                source=mapping_config.table_name,
                export_timestamp=datetime.utcnow(),
                row_count=len(result)
            )

            return MappingExport(metadata=metadata, data=result)

# Task 4: UTF-8 JSON export pseudocode
def save_mapping_to_json(mapping: MappingExport, output_path: str):
    # CRITICAL: Use UTF-8 encoding and ensure_ascii=False for Chinese characters
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(
            mapping.dict(),
            f,
            indent=2,
            ensure_ascii=False,  # Preserves Chinese characters
            sort_keys=True       # Stable diffs
        )
```

### Integration Points
```yaml
DATABASE:
  - connection: Use existing DatabaseSettings from config/settings.py
  - query: PostgreSQL tables from migrated database schema

CONFIG:
  - environment: WDH_DATABASE__* variables from .env
  - pattern: settings = get_settings(); dsn = settings.get_database_connection_string()

FILES:
  - output: tests/fixtures/mappings/*.json (one per mapping)
  - report: tests/fixtures/mappings/_missing_entries.md

VALIDATION:
  - schema: Verify JSON structure matches MappingExport model
  - encoding: Confirm Chinese characters preserved correctly
```

## Validation Loop

### Level 1: Syntax & Style
```bash
# Run these FIRST - fix any errors before proceeding
uv run ruff check scripts/tools/export_mapping.py --fix
uv run mypy scripts/tools/export_mapping.py

# Expected: No errors. If errors, READ the error and fix.
```

### Level 2: Unit Tests
```python
# CREATE tests/tools/test_export_mapping.py with these test cases:
def test_database_connection():
    """Test DatabaseSettings connection works"""
    settings = get_settings()
    dsn = settings.get_database_connection_string()
    with psycopg2.connect(dsn) as conn:
        assert conn.status == psycopg2.extensions.STATUS_READY

def test_json_utf8_preservation():
    """Test Chinese characters preserved in JSON export"""
    test_data = {"测试": "中文"}
    mapping = MappingExport(
        metadata=MappingMetadata(description="test", source="test", export_timestamp=datetime.utcnow()),
        data=test_data
    )

    with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', delete=False) as f:
        json.dump(mapping.dict(), f, ensure_ascii=False)
        temp_path = f.name

    with open(temp_path, 'r', encoding='utf-8') as f:
        loaded = json.load(f)
        assert loaded['data']['测试'] == '中文'

def test_missing_table_handling():
    """Test graceful handling of missing tables"""
    with pytest.raises(TableNotFoundError):
        extract_mapping_from_database(ExtractionPlan(
            mapping_name="test",
            source_type="database",
            table_name="nonexistent_table",
            output_file="test.json"
        ))
```

```bash
# Run and iterate until passing:
uv run pytest tests/tools/test_export_mapping.py -v
# If failing: Read error, understand root cause, fix code, re-run
```

### Level 3: Integration Test
```bash
# Test single mapping export
uv run python scripts/tools/export_mapping.py \
  --mapping company_id1_mapping \
  --output tests/fixtures/mappings/company_id1_mapping.json

# Verify JSON structure and UTF-8 content
python -c "
import json
with open('tests/fixtures/mappings/company_id1_mapping.json', 'r', encoding='utf-8') as f:
    data = json.load(f)
    print(f'Keys: {len(data[\"data\"])}')
    print(f'Has metadata: {\"metadata\" in data}')
    print(f'Sample keys: {list(data[\"data\"].keys())[:3]}')
"

# Test bulk export
uv run python scripts/tools/export_mapping.py --all

# Verify all 10 files created
ls -la tests/fixtures/mappings/*.json | wc -l
# Expected: 10 JSON files

# Check for missing entries report
cat tests/fixtures/mappings/_missing_entries.md
```

## Final Validation Checklist
- [ ] All 10 mapping JSON files exist: `ls tests/fixtures/mappings/*.json`
- [ ] No linting errors: `uv run ruff check scripts/tools/`
- [ ] No type errors: `uv run mypy scripts/tools/`
- [ ] Chinese characters preserved: `grep -r "中国" tests/fixtures/mappings/`
- [ ] Metadata includes source and timestamp in all files
- [ ] Row count verification passes for database-sourced mappings
- [ ] Missing entries documented in _missing_entries.md
- [ ] JSON structure validates against MappingExport schema

---

## Anti-Patterns to Avoid
- ❌ Don't copy MySQL connection logic from legacy mappings.py
- ❌ Don't use ensure_ascii=True (breaks Chinese characters)
- ❌ Don't skip table existence checks before querying
- ❌ Don't hardcode database credentials - use settings.py
- ❌ Don't fail entire export if one mapping has missing data
- ❌ Don't skip UTF-8 encoding specification in file operations

## Confidence Score: 8/10

High confidence due to:
- Clear examples of database connection patterns in codebase
- Existing JSON export patterns with UTF-8 handling
- Well-defined mapping inventory in INITIAL.md
- External research provides proven PostgreSQL JSON extraction patterns
- Existing test fixture structure to follow

Minor uncertainty on:
- Exact table names in migrated PostgreSQL schema (may differ from MySQL)
- Completeness of migration data (some mappings may have gaps)