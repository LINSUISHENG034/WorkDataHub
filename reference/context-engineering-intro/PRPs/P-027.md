name: "Reference Backfill E2E Validation (Annuity Plans & Portfolios)"
description: |

## Purpose
Validate that Reference Backfill correctly writes to the database for Annuity Scale Details, including plan-only preview, execute mode with accurate counts, fallback path without unique indexes, and sheet name support. This ensures the reference backfill functionality works end-to-end with real PostgreSQL databases.

## Core Principles
1. **Context is King**: Include ALL necessary documentation, examples, and caveats
2. **Validation Loops**: Provide executable tests/lints the AI can run and fix
3. **Information Dense**: Use keywords and patterns from the codebase
4. **Progressive Success**: Start simple, validate, then enhance
5. **Global rules**: Be sure to follow all rules in CLAUDE.md

---

## Goal
Reproduce, diagnose, and validate the Reference Backfill (insert_missing + fallback; optional fill_null_only) with a real PostgreSQL database. The outcome is an auditable report in VALIDATION.md showing candidate counts, SQL plans, execution summaries, and actual DB row counts for both tables (年金计划, 组合计划).

## Why
- **Business value**: Ensures reference backfill functionality works correctly in production environments
- **Integration**: Validates complete ETL pipeline with real database operations
- **Problems solved**: Confirms both ON CONFLICT and fallback paths work correctly for data engineers

## What
End-to-end validation of reference backfill functionality using small test datasets, documenting all execution paths and results in VALIDATION.md.

### Success Criteria
- [ ] Plan-only preview shows non-zero candidate counts and SQL plans
- [ ] Execute mode with indexes shows non-zero inserts into both reference tables
- [ ] Execute mode without indexes demonstrates fallback path with correct results
- [ ] Sheet name support works: `--sheet "规模明细"` processes correctly
- [ ] All validation gates pass (ruff, mypy, pytest)
- [ ] VALIDATION.md contains comprehensive test results

## All Needed Context

### Documentation & References
```yaml
# MUST READ - Include these in your context window
- file: src/work_data_hub/orchestration/jobs.py
  lines: 208-407
  why: CLI interface and main() function patterns for argument handling

- file: src/work_data_hub/orchestration/ops.py
  lines: 583-717
  why: backfill_refs_op implementation with connection handling and logging patterns

- file: src/work_data_hub/io/loader/warehouse_loader.py
  lines: 248-404
  why: insert_missing implementation with ON CONFLICT and fallback logic

- file: tests/orchestration/test_backfill_ops.py
  why: Existing test patterns for backfill operations and mocking strategies

- file: tests/e2e/test_annuity_overwrite_append_small_subsets.py
  why: E2E test structure and CLI command patterns to follow

- file: VALIDATION.md
  why: Current format and expected output structure for documentation

- file: INITIAL.md
  why: Original requirements, examples, and acceptance criteria
```

### Implementation-Facing Research Notes
```yaml
sources:
  - type: external_research
    topic: PostgreSQL ON CONFLICT vs SELECT+INSERT patterns
    why: Understanding concurrency and performance implications for testing

tldr:
  - INSERT ... ON CONFLICT DO NOTHING is atomic, safe, and performant
  - SELECT + INSERT pattern has race conditions and requires two round trips
  - ON CONFLICT requires unique indexes; fallback uses SELECT-filter + plain INSERT
  - Sequence "burning" occurs even on conflicts (affects ID testing expectations)
  - Chinese identifiers must be properly quoted in SQL

setup_commands:
  - "# Set environment variables"
  - "set WDH_DATA_BASE_DIR=tests\\fixtures\\sample_data\\annuity_subsets"
  - "set WDH_DATABASE__URI=postgresql://user:pwd@host:5432/dbname"

api_decisions:
  - name: backfill_mode
    choice: insert_missing
    rationale: Primary mode with ON CONFLICT DO NOTHING for concurrent safety
  - name: targets
    choice: all
    rationale: Test both plans and portfolios in single execution
  - name: sheet_specification
    choice: "规模明细"
    rationale: Must use exact Chinese sheet name, not index

versions:
  - library: psycopg2
    constraint: ">=2.8"
    compatibility: PostgreSQL connection handling, bulk operations
  - library: dagster
    constraint: "current"
    compatibility: Op execution context and logging patterns

pitfalls_and_mitigations:
  - issue: Wrong schema/search_path appears as "no rows inserted"
    mitigation: Include schema diagnostics and \dt output verification
  - issue: Sheet name mismatch yields zero candidates
    mitigation: Use exact sheet name "规模明细" and test negative case
  - issue: Chinese identifiers require proper quoting
    mitigation: Warehouse loader handles quoting automatically
  - issue: ON CONFLICT requires unique indexes for target constraint
    mitigation: Test both with and without indexes to verify fallback
  - issue: Connection leaks in test environments
    mitigation: Use proper try/finally blocks and connection.close()

open_questions:
  - None - functionality exists and requirements are clear
```

### Current Codebase tree
```bash
src/work_data_hub/
  orchestration/
    jobs.py                    # CLI + Reference Backfill Summary printing
    ops.py                     # derive_*_refs_op, backfill_refs_op (reads sheet name or index)
  io/loader/warehouse_loader.py # insert_missing (with ON CONFLICT fallback), fill_null_only
  domain/reference_backfill/
    service.py, models.py      # candidate derivation (plans/portfolios)

tests/fixtures/sample_data/annuity_subsets/
  2024年11月年金终稿数据_subset_append_3.xlsx  # use as default input (sheet "规模明细")
```

### Desired Codebase tree with files to be added
```bash
# No new files needed - this is validation work
VALIDATION.md                  # Enhanced with comprehensive test results
# All functionality already exists in the codebase
```

### Known Gotchas & Library Quirks
```python
# CRITICAL: PostgreSQL sequence "burning" - IDs increment even on conflicts
# This affects test expectations - don't assume sequential IDs

# CRITICAL: Chinese table names require proper quoting
# Tables: "年金计划", "组合计划" - warehouse loader handles this

# CRITICAL: ON CONFLICT requires unique indexes
# Without them, fallback to SELECT-filter + plain INSERT (slower but correct)

# CRITICAL: Sheet name must be exact string match
# Use --sheet "规模明细" not --sheet 0 for proper validation

# CRITICAL: Environment variables must be set correctly
# WDH_DATA_BASE_DIR points to subset directory
# WDH_DATABASE__URI for database connection

# CRITICAL: Connection management in psycopg2
# Always close connections in finally blocks to prevent leaks
```

## Implementation Blueprint

### Data models and structure
```python
# Reference table structures (already exist)
年金计划_schema = {
    "年金计划号": "TEXT PRIMARY KEY",     # plan_code
    "计划全称": "TEXT",                   # plan_name
    "计划类型": "TEXT",                   # plan_type
    "客户名称": "TEXT",                   # client_name
    "company_id": "VARCHAR(50)"           # company_id
}

组合计划_schema = {
    "组合代码": "TEXT PRIMARY KEY",       # portfolio_code
    "年金计划号": "TEXT",                 # plan_code (FK reference)
    "组合名称": "TEXT",                   # portfolio_name
    "组合类型": "TEXT",                   # portfolio_type
    "运作开始日": "DATE"                  # operation_start_date (None for new)
}

# Candidate derivation outputs
plan_candidates = [
    {
        "年金计划号": "计划代码",           # from processed annuity data
        "计划全称": "计划名称",
        "计划类型": "计划类型",
        "客户名称": "客户名称",
        "company_id": "derived_company_id"
    }
]

portfolio_candidates = [
    {
        "组合代码": "组合代码",
        "年金计划号": "计划代码",
        "组合名称": "组合名称",
        "组合类型": "组合类型",
        "运作开始日": None                  # Always None for new portfolios
    }
]
```

### List of tasks to be completed in order

```yaml
Task 1: Database Environment Setup
SETUP PostgreSQL test environment:
  - CREATE reference tables with Chinese identifiers
  - VERIFY connection using WDH_DATABASE__URI
  - DOCUMENT schema setup commands for reproducibility

Task 2: Plan-Only Validation
EXECUTE plan-only command:
  - RUN CLI command with --plan-only flag
  - CAPTURE candidate counts from derive_plan_refs_op and derive_portfolio_refs_op
  - EXTRACT Reference Backfill Summary showing operations > 0
  - RECORD SQL plan excerpt (INSERT_MISSING ... ON CONFLICT ...)

Task 3: Execute with Unique Indexes
SETUP unique indexes on both reference tables:
  - CREATE unique indexes on primary key columns
  - EXECUTE reference backfill with --execute flag
  - CAPTURE Reference Backfill Summary with inserted counts
  - QUERY database for actual row counts in both tables
  - VERIFY non-zero inserts for both 年金计划 and 组合计划

Task 4: Execute without Indexes (Fallback Path)
REMOVE unique indexes and test fallback:
  - TRUNCATE both reference tables
  - DROP unique indexes to force fallback path
  - EXECUTE same command and look for "Insert missing (fallback)" logs
  - CAPTURE Reference Backfill Summary and verify results
  - DOCUMENT performance difference from indexed approach

Task 5: Sheet Name Validation
TEST sheet name handling:
  - EXECUTE with correct sheet name --sheet "规模明细"
  - EXECUTE with wrong sheet name and expect zero candidates
  - VERIFY that sheet specification works correctly

Task 6: Schema Diagnostics
VERIFY database schema consistency:
  - CAPTURE current_database(), current_schema(), search_path
  - RUN \dt commands on both reference tables
  - ENSURE query schema matches write schema

Task 7: Documentation and Validation
COMPILE comprehensive VALIDATION.md:
  - ORGANIZE all command outputs and results
  - SANITIZE database connection strings
  - RUN validation gates (ruff, mypy, pytest)
  - VERIFY all acceptance criteria met
```

### Per task pseudocode

```sql
-- Task 1: Database Setup
CREATE TABLE IF NOT EXISTS "年金计划" (
  "年金计划号" TEXT PRIMARY KEY,
  "计划全称"   TEXT,
  "计划类型"   TEXT,
  "客户名称"   TEXT,
  "company_id" VARCHAR(50)
);

CREATE TABLE IF NOT EXISTS "组合计划" (
  "组合代码"   TEXT PRIMARY KEY,
  "年金计划号" TEXT,
  "组合名称"   TEXT,
  "组合类型"   TEXT,
  "运作开始日" DATE
);

-- Optional unique indexes (Task 3)
CREATE UNIQUE INDEX IF NOT EXISTS "uq_年金计划_年金计划号" ON "年金计划" ("年金计划号");
CREATE UNIQUE INDEX IF NOT EXISTS "uq_组合计划_组合代码" ON "组合计划" ("组合代码");
```

```bash
# Task 2: Plan-Only Command (PowerShell)
$env:WDH_DATA_BASE_DIR = "tests\fixtures\sample_data\annuity_subsets"
uv run python -m src.work_data_hub.orchestration.jobs `
  --domain annuity_performance `
  --plan-only `
  --max-files 1 `
  --backfill-refs all `
  --backfill-mode insert_missing `
  --sheet "规模明细" `
  --debug

# Expected output patterns to capture:
# - derive_plan_refs_op: unique_plans = X
# - derive_portfolio_refs_op: unique_portfolios = Y
# - Reference Backfill Summary: plan_only=True, operations>0
# - SQL plan excerpt with INSERT_MISSING ... ON CONFLICT ...
```

```bash
# Task 3: Execute with Indexes
$env:WDH_DATABASE__URI = "postgresql://user:pwd@host:5432/dbname"
uv run python -m src.work_data_hub.orchestration.jobs `
  --domain annuity_performance `
  --execute `
  --max-files 1 `
  --backfill-refs all `
  --backfill-mode insert_missing `
  --sheet "规模明细" `
  --mode append `
  --debug --raise-on-error

# Expected output patterns:
# - Reference Backfill Summary: inserted counts per table (both non-zero)
# - Follow with: SELECT COUNT(*) FROM "年金计划"; and SELECT COUNT(*) FROM "组合计划";
```

### Integration Points
```yaml
ENVIRONMENT:
  - WDH_DATA_BASE_DIR: points to tests/fixtures/sample_data/annuity_subsets
  - WDH_DATABASE__URI: PostgreSQL connection string
  - DAGSTER_HOME: not required (ephemeral instance for debug)

DATABASE:
  - Tables: "年金计划", "组合计划" with Chinese identifiers
  - Indexes: Optional unique indexes for ON CONFLICT testing
  - Schema: Ensure consistent search_path for reads and writes

CLI:
  - Domain: annuity_performance
  - Sheet: "规模明细" (exact Chinese name)
  - Backfill: --backfill-refs all --backfill-mode insert_missing
  - Mode: append (for facts), insert_missing (for references)

VALIDATION:
  - Logs: Reference Backfill Summary with execution metadata
  - Database: Row counts in reference tables after execution
  - SQL Plans: ON CONFLICT statements and fallback patterns
```

## Validation Loop

### Level 1: Syntax & Style
```bash
# Run these FIRST - fix any errors before proceeding
uv run ruff check src/ --fix       # Auto-fix style issues
uv run mypy src/                   # Type checking

# Expected: No errors. If errors, READ and fix.
```

### Level 2: Unit Tests
```bash
# Run existing backfill tests to ensure nothing is broken
uv run pytest -v -k backfill

# Specific test files to verify:
uv run pytest tests/orchestration/test_backfill_ops.py -v
uv run pytest tests/io/test_warehouse_loader_backfill.py -v

# Expected: All tests pass. If failing, investigate and fix.
```

### Level 3: Manual E2E Validation
```bash
# Follow the task sequence above:
# 1. Set up database with reference tables
# 2. Run plan-only command and capture output
# 3. Run execute with indexes and verify results
# 4. Run execute without indexes and verify fallback
# 5. Test sheet name validation
# 6. Document all results in VALIDATION.md

# Expected: All commands succeed with expected output patterns
```

## Final validation Checklist
- [ ] Database setup completed with both reference tables
- [ ] Plan-only shows non-zero candidates and operations > 0
- [ ] Execute with indexes: both tables show inserted > 0
- [ ] Execute without indexes: fallback path demonstrated
- [ ] Sheet name validation: correct name works, wrong name fails
- [ ] VALIDATION.md updated with comprehensive results
- [ ] All validation gates pass: `uv run ruff check src/`, `uv run mypy src/`, `uv run pytest -v -k backfill`
- [ ] Database schema diagnostics captured
- [ ] Connection strings properly sanitized in documentation

---

## Anti-Patterns to Avoid
- ❌ Don't skip the fallback path testing - it's critical for environments without unique indexes
- ❌ Don't hardcode database credentials in documentation - sanitize connection strings
- ❌ Don't assume sequential IDs due to sequence burning behavior
- ❌ Don't use sheet index when sheet name is required for validation
- ❌ Don't skip connection cleanup - always close database connections
- ❌ Don't ignore Chinese identifier quoting - let warehouse loader handle it

## Confidence Score: 8/10

High confidence due to:
- Existing, tested functionality that just needs validation
- Clear requirements and acceptance criteria from INITIAL.md
- Comprehensive test patterns already established
- Well-documented database setup procedures
- Detailed command examples and expected outputs

Minor uncertainty on:
- Database environment setup variations across different PostgreSQL versions
- Potential timing issues in concurrent testing scenarios
- Sheet name encoding issues in different terminal environments

The validation approach is methodical and all major gotchas have been identified and documented.