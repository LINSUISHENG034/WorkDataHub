# "Orchestration Follow-Up: Multi-File Processing, Flag Normalization, DB Connection Context"

## Purpose
Implement suggested next steps for the Dagster orchestration layer: multi-file processing, flag normalization, DB connection cleanup via context managers, and consistent CLI output. Keep plan-only as the safe default without introducing advanced Dagster features.

## Core Principles
1. **Context is King**: Include ALL necessary documentation, examples, and caveats
2. **Validation Loops**: Provide executable tests/lints the AI can run and fix
3. **Information Dense**: Use keywords and patterns from the codebase
4. **Progressive Success**: Start simple, validate, then enhance
5. **Global rules**: Be sure to follow all rules in CLAUDE.md

---

## Goal
Enhance the existing Dagster orchestration system to support multi-file processing (up to N files), normalize execution flags for consistency, implement proper DB connection context management, and ensure CLI output matches the effective execution mode.

## Why
- **Operational efficiency**: Process multiple discovered files in a single job run instead of one-at-a-time
- **User experience**: Eliminate confusion between `--plan-only` and `--execute` flags with clear precedence
- **Reliability**: Ensure DB connections are properly closed even on errors through context managers
- **Consistency**: CLI output and execution behavior should match the determined execution mode

## What
Enhanced orchestration system where:
- Job processes up to N discovered files and accumulates results before loading once
- Single source of truth for execution mode derived from CLI flags
- DB connections use context managers for automatic cleanup
- Display output matches effective execution mode (plan vs execute)
- Maintains backward compatibility with existing job structure

### Success Criteria
- [ ] Multi-file: job processes up to N files and loads once; records are accumulated correctly
- [ ] Flags: one source of truth for execution mode; help text documents behavior; display reflects effective mode  
- [ ] DB connection uses a context manager; no persistent open connection on errors
- [ ] Tests: multi-file accumulation (mocked), CLI display consistency, execute path (psycopg2 mocked)
- [ ] Ruff, mypy, pytest all pass

## All Needed Context

### Documentation & References
```yaml
# MUST READ - Include these in your context window
- url: https://docs.dagster.io/concepts/ops-jobs-graphs/ops
  why: Op creation patterns, configuration validation, JSON-serializable contracts
  
- url: https://docs.dagster.io/guides/dagster/run-a-job  
  why: In-process execution patterns for multi-file scenarios without dynamic mapping
  
- url: https://www.psycopg.org/docs/module.html#psycopg2.connect
  why: Connection context manager usage for transaction management and cleanup
  
- url: https://docs.pydantic.dev/latest/concepts/validators/
  why: Field validation patterns for Config classes (max_files validation)
  
- file: src/work_data_hub/orchestration/ops.py
  why: Existing op patterns, Config classes, JSON-serializable contracts, load_op structure
  
- file: src/work_data_hub/orchestration/jobs.py  
  why: CLI argument parsing, run_config building, flag handling, display logic
  
- file: src/work_data_hub/io/loader/warehouse_loader.py
  why: load() function signature with conn parameter, plan-only vs execute modes
  
- file: tests/orchestration/test_ops.py
  why: Testing patterns, psycopg2 mocking approach, Config validation tests
```

### Current Codebase Tree
```bash
src/work_data_hub/
├── orchestration/
│   ├── jobs.py                      # CLI + job wiring (normalize flags; multi-file path)
│   └── ops.py                       # existing ops (add new op or enhance)
├── io/
│   ├── readers/excel_reader.py      # read_excel_rows function
│   ├── loader/warehouse_loader.py   # load function with conn parameter
│   └── connectors/file_connector.py # DataSourceConnector for file discovery
├── domain/trustee_performance/
│   └── service.py                   # process function for domain logic
└── config/
    └── settings.py                  # get_settings, DatabaseSettings
tests/orchestration/
├── test_ops.py                      # Op testing patterns
└── test_jobs.py                     # Job testing patterns
```

### Desired Codebase Tree (minimal changes)
```bash
src/work_data_hub/orchestration/
├── jobs.py                          # MODIFIED: flag normalization logic
└── ops.py                           # MODIFIED: add combined multi-file op
tests/orchestration/  
├── test_ops.py                      # MODIFIED: add multi-file and DB context tests
└── test_jobs.py                     # MODIFIED: add flag normalization tests
```

### Known Gotchas & Library Quirks
```python
# CRITICAL: Dagster ops require JSON-serializable data contracts between ops
# CRITICAL: load() function already accepts conn=None (plan-only) or conn=connection (execute)
# CRITICAL: psycopg2 context manager `with conn:` manages transactions, not connection lifecycle
# CRITICAL: Must use try/finally or separate context manager for connection closure
# CRITICAL: Multi-file iteration must occur INSIDE an op to avoid Dagster dynamic mapping
# CRITICAL: File paths must be JSON-serializable strings, not Path objects
# CRITICAL: Tests must mock psycopg2.connect to avoid requiring live database
# CRITICAL: Flag precedence: --execute takes priority over --plan-only if both present
# CRITICAL: Preserve existing op signatures for backward compatibility
# CRITICAL: effective_plan_only should be used for BOTH run_config AND display logic
```

## Implementation Blueprint

### Data Models and Structure
```python
# New Config class for combined multi-file operation
from dagster import Config
from pydantic import field_validator

class ReadProcessConfig(Config):
    """Configuration for reading and processing multiple trustee files."""
    sheet: int = 0
    max_files: int = 1
    
    @field_validator("max_files")
    @classmethod  
    def validate_max_files(cls, v: int) -> int:
        """Validate max_files is positive and reasonable."""
        if v < 1:
            raise ValueError("max_files must be at least 1")
        if v > 20:  # Reasonable upper bound
            raise ValueError("max_files cannot exceed 20")
        return v
```

### List of Tasks to Complete (In Order)

```yaml
Task 1: Create Combined Multi-File Op
MODIFY src/work_data_hub/orchestration/ops.py:
  - CREATE ReadProcessConfig class with sheet and max_files parameters
  - CREATE read_and_process_trustee_files_op function
  - PATTERN: Follow existing op structure with OpExecutionContext, Config, return List[Dict]
  - LOGIC: Iterate file_paths[:max_files], call read_excel_rows + process per file
  - ACCUMULATE: Extend list with model_dump() results from each file
  - PRESERVE: JSON-serializable contracts, structured logging

Task 2: Normalize CLI Flags in Jobs  
MODIFY src/work_data_hub/orchestration/jobs.py:
  - ADD effective_plan_only calculation in build_run_config()
  - FORMULA: effective_plan_only = not args.execute if hasattr(args, "execute") else getattr(args, "plan_only", True)
  - REPLACE: All usage of args.plan_only with effective_plan_only
  - UPDATE: help text to document flag precedence clearly
  - MODIFY: Display logic in main() to use effective_plan_only

Task 3: Implement DB Connection Context Manager
MODIFY src/work_data_hub/orchestration/ops.py in load_op:
  - WRAP: psycopg2.connect(dsn) in `with` statement when not plan_only
  - PATTERN: `with psycopg2.connect(dsn) as conn:` for automatic transaction handling
  - PRESERVE: Existing error handling and DataWarehouseLoaderError patterns
  - MAINTAIN: plan_only path with conn=None unchanged

Task 4: Wire New Op Into Job Graph
MODIFY src/work_data_hub/orchestration/jobs.py:
  - UPDATE: trustee_performance_job to optionally use new combined op
  - PRESERVE: Backward compatibility with existing 4-op chain
  - ADD: max_files parameter to run_config building
  - STRATEGY: Use new op when max_files > 1, otherwise keep existing chain

Task 5: Add Comprehensive Tests
MODIFY tests/orchestration/test_ops.py:
  - ADD: test_read_and_process_trustee_files_op_multi_file()
  - ADD: test_load_op_db_context_manager_mocked()
  - PATTERN: Mock psycopg2.connect like existing tests
  - VALIDATE: Multi-file accumulation, JSON serialization, error handling

MODIFY tests/orchestration/test_jobs.py:
  - ADD: test_effective_plan_only_flag_precedence() 
  - ADD: test_build_run_config_max_files()
  - VALIDATE: Flag normalization logic, run_config structure
```

### Per Task Pseudocode

```python
# Task 1: Combined Multi-File Op
@op
def read_and_process_trustee_files_op(
    context: OpExecutionContext, 
    config: ReadProcessConfig, 
    file_paths: List[str]
) -> List[Dict]:
    """Process multiple trustee files and return accumulated results."""
    # PATTERN: Limit files like existing MVP approach
    paths_to_process = (file_paths or [])[: min(len(file_paths), config.max_files)]
    all_processed: List[Dict] = []
    
    for file_path in paths_to_process:
        # PATTERN: Follow existing read_excel_op approach
        rows = read_excel_rows(file_path, sheet=config.sheet)
        
        # PATTERN: Follow existing process_trustee_performance_op approach  
        models = process(rows, data_source=file_path)
        
        # CRITICAL: JSON-serializable accumulation
        processed_dicts = [model.model_dump() for model in models]
        all_processed.extend(processed_dicts)
        
        # PATTERN: Structured logging like existing ops
        context.log.info(f"Processed {file_path}: {len(rows)} rows -> {len(processed_dicts)} records")
    
    context.log.info(f"Multi-file processing completed: {len(paths_to_process)} files, {len(all_processed)} total records")
    return all_processed

# Task 2: Flag Normalization  
def build_run_config(args: argparse.Namespace) -> Dict[str, Any]:
    # CRITICAL: Single source of truth calculation
    effective_plan_only = not args.execute if hasattr(args, "execute") else getattr(args, "plan_only", True)
    
    run_config = {
        "ops": {
            # ... existing config ...
            "load_op": {
                "config": {
                    # ... existing fields ...
                    "plan_only": effective_plan_only,  # Use effective flag
                }
            },
        }
    }
    return run_config

# Task 3: DB Context Manager in load_op
@op  
def load_op(context: OpExecutionContext, config: LoadConfig, processed_rows: List[Dict[str, Any]]) -> Dict[str, Any]:
    try:
        conn = None
        if not config.plan_only:
            import psycopg2
            settings = get_settings()
            dsn = settings.database.get_connection_string()
            
            # PATTERN: Context manager for transaction handling
            with psycopg2.connect(dsn) as conn:
                # CRITICAL: Connection auto-commits on success, rolls back on exception
                result = load(table=config.table, rows=processed_rows, mode=config.mode, pk=config.pk, conn=conn)
        else:
            # PATTERN: Unchanged plan-only path
            result = load(table=config.table, rows=processed_rows, mode=config.mode, pk=config.pk, conn=None)
        
        return result
    except Exception as e:
        context.log.error(f"Load operation failed: {e}")
        raise
```

### Integration Points
```yaml
ENVIRONMENT:
  - No new environment variables required
  - Existing WDH_DATABASE__* variables used for DB connections
  
CLI_ARGUMENTS:
  - PRESERVE: --plan-only for backward compatibility
  - ENHANCE: --execute flag takes precedence  
  - ADD: --max-files parameter (default: 1)
  - UPDATE: Help text to document flag precedence

JOB_WIRING:
  - STRATEGY: Conditional op usage based on max_files parameter
  - IF max_files == 1: Use existing 4-op chain (discover -> read -> process -> load)
  - IF max_files > 1: Use combined op chain (discover -> read_and_process_combined -> load)
  - PRESERVE: Backward compatibility for single-file scenarios
```

## Validation Loop

### Level 1: Syntax & Style
```bash
# Run these FIRST - fix any errors before proceeding
uv run ruff check src/ --fix
uv run mypy src/

# Expected: No errors. If errors, READ and fix before proceeding.
```

### Level 2: Unit Tests
```python
# Key test cases to implement:

# Test multi-file accumulation
async def test_read_and_process_trustee_files_op_multi_file():
    """Test combined op processes multiple files and accumulates correctly."""
    # Mock read_excel_rows and process functions
    file_paths = ["/path/file1.xlsx", "/path/file2.xlsx"] 
    config = ReadProcessConfig(sheet=0, max_files=2)
    
    with patch("...read_excel_rows") as mock_read, patch("...process") as mock_process:
        mock_read.side_effect = [
            [{"col": "data1"}],  # File 1 rows
            [{"col": "data2"}]   # File 2 rows  
        ]
        mock_process.side_effect = [
            [Mock(model_dump=lambda: {"processed": "data1"})],  # File 1 models
            [Mock(model_dump=lambda: {"processed": "data2"})]   # File 2 models
        ]
        
        result = read_and_process_trustee_files_op(build_op_context(), config, file_paths)
        
        assert len(result) == 2
        assert result[0]["processed"] == "data1"
        assert result[1]["processed"] == "data2"
        # Verify both read_excel_rows calls made
        assert mock_read.call_count == 2

# Test flag normalization 
def test_effective_plan_only_precedence():
    """Test --execute takes precedence over --plan-only."""
    # Case 1: --execute present -> should execute
    args1 = argparse.Namespace(execute=True, plan_only=True)
    config1 = build_run_config(args1)
    assert config1["ops"]["load_op"]["config"]["plan_only"] is False
    
    # Case 2: only --plan-only -> should plan
    args2 = argparse.Namespace(plan_only=True)
    delattr(args2, 'execute')  # Remove execute attribute
    config2 = build_run_config(args2) 
    assert config2["ops"]["load_op"]["config"]["plan_only"] is True

# Test DB context manager
def test_load_op_db_context_manager():
    """Test load_op uses context manager for DB connections."""
    processed_rows = [{"col": "value"}]
    
    mock_conn = Mock()
    mock_result = {"mode": "delete_insert", "table": "test", "deleted": 1, "inserted": 1, "batches": 1}
    
    with patch("src.work_data_hub.orchestration.ops.psycopg2") as mock_psycopg2:
        mock_psycopg2.connect.return_value.__enter__.return_value = mock_conn
        
        with patch("src.work_data_hub.orchestration.ops.load") as mock_load:
            mock_load.return_value = mock_result
            
            with patch("src.work_data_hub.orchestration.ops.get_settings") as mock_settings:
                mock_settings.return_value.database.get_connection_string.return_value = "postgresql://test"
                
                config = LoadConfig(plan_only=False, table="test", pk=["id"])
                result = load_op(build_op_context(), config, processed_rows)
                
                # Verify context manager was used
                mock_psycopg2.connect.assert_called_once_with("postgresql://test")
                mock_psycopg2.connect.return_value.__enter__.assert_called_once()
                mock_load.assert_called_once_with(
                    table="test", rows=processed_rows, mode="delete_insert", pk=["id"], conn=mock_conn
                )
```

```bash
# Run tests iteratively until passing:
uv run pytest tests/orchestration/ -v
# If failing: Read error, understand root cause, fix code, re-run
```

### Level 3: Integration Test
```bash
# Test CLI behavior with new multi-file functionality
# IMPORTANT: Activate PowerShell venv first
.\.venv\Scripts\Activate.ps1

# Test 1: Plan-only multi-file (safe, no DB required)
uv run python -m src.work_data_hub.orchestration.jobs --max-files 2

# Expected output should show:
# - "Plan-only: True" 
# - Processing of up to 2 discovered files
# - SQL execution plans displayed
# - No actual database connection

# Test 2: Execute mode single file (requires DB env vars)
uv run python -m src.work_data_hub.orchestration.jobs --execute --max-files 1

# Expected output should show:
# - "Execute: True"
# - "Plan-only: False"  
# - Actual database operations if DB available
```

## Final Validation Checklist
- [ ] All tests pass: `uv run pytest tests/ -v`
- [ ] No linting errors: `uv run ruff check src/`
- [ ] No type errors: `uv run mypy src/`
- [ ] Multi-file processing works: CLI processes multiple files when --max-files > 1
- [ ] Flag precedence works: --execute overrides --plan-only
- [ ] DB context manager works: Connections properly managed in execute mode
- [ ] Display consistency: CLI output matches effective execution mode
- [ ] Backward compatibility: Existing single-file behavior unchanged
- [ ] Error handling: DB connection failures handled gracefully

---

## Anti-Patterns to Avoid
- ❌ Don't use Dagster dynamic mapping - keep static graph with internal iteration
- ❌ Don't break JSON-serializable contracts between ops
- ❌ Don't ignore psycopg2 connection lifecycle - use proper context management
- ❌ Don't create separate flag logic for display vs execution - use single source of truth
- ❌ Don't change existing op signatures - maintain backward compatibility
- ❌ Don't skip mocking psycopg2 in tests - avoid requiring live database for unit tests
- ❌ Don't hardcode max_files limits - make them configurable with validation
- ❌ Don't forget transaction rollback - let psycopg2 context manager handle it

## Confidence Score: 9/10

High confidence due to:
- Clear understanding of existing codebase patterns and architecture
- Well-defined requirements with specific acceptance criteria
- Established testing patterns already in place
- External research confirming best practices for multi-file processing, flag normalization, and DB context management
- Comprehensive validation gates with executable commands

Minor uncertainty on exact job graph wiring strategy, but the pseudocode provides clear implementation guidance.