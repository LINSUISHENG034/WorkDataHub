name: "Company Enrichment MVP Hardening - P-030"
description: |

## Purpose
Fix remaining company_id enrichment MVP gaps by addressing budget handling, temp-ID sequencing, queue SQL patterns, and loader append mode to close S-004 validation requirements.

## Core Principles
1. **Context is King**: Include ALL necessary documentation, examples, and caveats
2. **Validation Loops**: Provide executable tests/lints the AI can run and fix
3. **Information Dense**: Use keywords and patterns from the codebase
4. **Progressive Success**: Start simple, validate, then enhance
5. **Global rules**: Be sure to follow all rules in CLAUDE.md

---

## Goal
Close the remaining company_id enrichment MVP gaps by fixing queue temp-ID sequencing, safe append loading, and synchronous budget handling so S-004 can exit VALIDATING status. All fixes must maintain plan-only compatibility and existing logging patterns.

## Why
- **Business value**: Enables production deployment of company enrichment service
- **Integration**: Completes S-003 MVP requirements for queue processing and caching
- **Problems solved**: Eliminates blocking technical debt preventing S-004 validation completion

## What
Fix 4 specific technical issues in the company enrichment pipeline:
1. Budget default handling in CompanyEnrichmentService.resolve_company_id
2. Temp ID sequence schema and DAO alignment for TEMP_000001 format
3. Queue dequeue SQL pattern to avoid FOR UPDATE in subquery limitations
4. Company mapping loader append mode with conflict handling

### Success Criteria
- [ ] resolve_company_id respects instance budgets; new unit test covers both overridden and default scenarios
- [ ] Temp ID generator returns sequential TEMP_000001, works in plan-only mode, and DDL + DAO stay aligned
- [ ] Queue dequeue executes without Postgres errors and marks rows atomically; test covers SQL execution via mock cursor
- [ ] load_company_mappings(..., mode="append") handles duplicates gracefully and reports accurate stats
- [ ] Updated tests pass alongside existing enrichment/loader suites without regressions

## All Needed Context

### Documentation & References
```yaml
# MUST READ - Include these in your context window
- url: https://www.postgresql.org/docs/current/explicit-locking.html#FOR-UPDATE-SHARE
  why: FOR UPDATE SKIP LOCKED reference for SQL rewrite

- file: src/work_data_hub/domain/company_enrichment/service.py
  line: 335
  why: Current budget handling logic to preserve while fixing default

- file: src/work_data_hub/domain/company_enrichment/lookup_queue.py
  line: 233
  why: Existing dequeue logic to replace with CTE pattern

- file: scripts/create_table/ddl/lookup_requests.sql
  line: 39
  why: DDL to align with new temp_id_sequence structure

- file: tests/io/test_company_mapping_loader.py
  why: Loader test style to mirror when adding append-mode coverage

- file: src/work_data_hub/io/loader/warehouse_loader.py
  line: 115
  why: build_insert_sql pattern to extend for ON CONFLICT handling

- doc: docs/company_id/simplified/S-003_基础缓存机制.md
  why: MVP expectations for queue, temp IDs, and caching

- doc: CLAUDE.md
  why: Repo style & testing expectations
```

### Implementation-Facing Research Notes
```yaml
sources:
  - url: https://www.postgresql.org/docs/current/explicit-locking.html#FOR-UPDATE-SHARE
    section: Row-level locking
    why: FOR UPDATE SKIP LOCKED usage patterns and limitations
    type: docs

tldr:
  - FOR UPDATE cannot be used in subqueries that are targets of IN clauses
  - CTE with UPDATE...FROM pattern avoids this Postgres limitation
  - SKIP LOCKED prevents blocking on already-locked rows in queue processing
  - Atomic sequence increment needs UPDATE...RETURNING for thread safety

setup_commands:
  - "# No new dependencies required - using existing psycopg2 and Pydantic patterns"

api_decisions:
  - name: dequeue_sql_pattern
    choice: "WITH pending AS (...) UPDATE ... FROM pending RETURNING"
    rationale: Avoids FOR UPDATE in subquery limitation while maintaining atomicity

  - name: temp_id_schema
    choice: "last_number INTEGER, updated_at TIMESTAMPTZ"
    rationale: Aligns with data contract expectations and provides audit trail

  - name: append_mode_strategy
    choice: "ON CONFLICT (alias_name, match_type) DO NOTHING"
    rationale: Simple conflict resolution matching PK constraint

versions:
  - library: psycopg2
    constraint: "existing version in requirements"
    compatibility: Works with existing cursor patterns

pitfalls_and_mitigations:
  - issue: "Plan-only paths must stay side-effect free"
    mitigation: "Use mock returns in LookupQueue when connection=None"

  - issue: "psycopg2 cursors return tuples unless RealDictCursor requested"
    mitigation: "Handle both dict and tuple shapes when parsing results"

  - issue: "DDL changes require migration for existing deployments"
    mitigation: "Provide ALTER statements and data migration in code comments"

open_questions:
  - "Should temp_id_sequence reset to 0 or preserve existing values during migration?"
  - "Need confirmation on exact format: TEMP_000001 vs other padding?"
```

### Current Codebase Structure
```bash
src/work_data_hub/
  domain/company_enrichment/
    service.py                      # Line 335: budget default issue
    lookup_queue.py                 # Line 233: dequeue SQL issue
  io/loader/
    company_mapping_loader.py       # Line 300: missing append mode
    warehouse_loader.py             # Line 115: build_insert_sql helper
  orchestration/ops.py
scripts/create_table/ddl/
  lookup_requests.sql               # Line 39: temp_id_sequence schema
  company_mapping.sql
tests/
  domain/company_enrichment/
    test_enrichment_service.py      # Budget path testing
    test_lookup_queue.py            # Dequeue SQL testing
  io/test_company_mapping_loader.py # Append mode testing
```

### Desired Codebase Structure (no new files)
```bash
# All changes are modifications to existing files
# No new files required - this is a hardening/bug fix PRP
```

### Known Gotchas & Library Quirks
```python
# CRITICAL: Plan-only paths must stay side-effect free
# (no DB connections for enrichment in process_annuity_performance_op)

# CRITICAL: psycopg2 cursors return tuples unless RealDictCursor is requested
# Handle both shapes when parsing results

# CRITICAL: Postgres forbids FOR UPDATE inside a subselect that is the target of an IN (...) filter
# Use CTEs or UPDATE ... FROM to avoid "ERROR: FOR UPDATE is not allowed in subqueries"

# CRITICAL: When altering temp_id_sequence, ensure existing deployments either reset or migrate
# the stored value to avoid ID regressions

# CRITICAL: Keep logging structure (use extra={...}) consistent with existing code paths

# CRITICAL: Primary key for company_mapping is (alias_name, match_type)
```

## Implementation Blueprint

### Data models and structure
```python
# No new models required - using existing:
# - CompanyIdResult (status increment when budget consumed)
# - CompanyMappingRecord (for loader operations)
# - LookupRequest (for queue operations)

# Schema change for temp_id_sequence:
# OLD: id INTEGER PRIMARY KEY
# NEW: last_number INTEGER, updated_at TIMESTAMPTZ
```

### List of tasks to be completed in order

```yaml
Task 1: Fix budget handling in CompanyEnrichmentService
MODIFY src/work_data_hub/domain/company_enrichment/service.py:
  - FIND line 335: "budget = sync_lookup_budget if sync_lookup_budget is not None else 0"
  - REPLACE with: "budget = sync_lookup_budget if sync_lookup_budget is not None else self.sync_lookup_budget"
  - PRESERVE existing logging structure with extra={}
  - VERIFY plan-only paths remain unaffected

Task 2: Update temp_id_sequence DDL schema
MODIFY scripts/create_table/ddl/lookup_requests.sql:
  - FIND line 39-46: temp_id_sequence table definition
  - REPLACE with new schema: last_number INTEGER DEFAULT 0, updated_at TIMESTAMPTZ DEFAULT now()
  - ADD migration comments for existing deployments
  - REMOVE PRIMARY KEY constraint and INSERT statement

Task 3: Fix temp ID DAO implementation
MODIFY src/work_data_hub/domain/company_enrichment/lookup_queue.py:
  - FIND get_next_temp_id method around line 414
  - REPLACE SQL: "UPDATE enterprise.temp_id_sequence SET last_number = last_number + 1, updated_at = now() RETURNING last_number"
  - PRESERVE plan-only mock behavior returning "TEMP_000001"
  - ENSURE format remains TEMP_NNNNNN with 6-digit padding

Task 4: Rewrite dequeue SQL with CTE pattern
MODIFY src/work_data_hub/domain/company_enrichment/lookup_queue.py:
  - FIND dequeue method around line 233
  - REPLACE problematic SQL with CTE pattern:
    ```sql
    WITH pending AS (
        SELECT id FROM enterprise.lookup_requests
        WHERE status = 'pending'
        ORDER BY created_at ASC
        LIMIT %s
        FOR UPDATE SKIP LOCKED
    )
    UPDATE enterprise.lookup_requests
    SET status = 'processing', updated_at = now()
    FROM pending
    WHERE enterprise.lookup_requests.id = pending.id
    RETURNING id, name, normalized_name, status, attempts, last_error, created_at, updated_at
    ```
  - PRESERVE existing row parsing logic for both dict and tuple shapes
  - MAINTAIN plan-only mock behavior

Task 5: Implement append mode in company mapping loader
MODIFY src/work_data_hub/io/loader/company_mapping_loader.py:
  - FIND load_company_mappings function around line 300
  - ADD elif mode == "append" branch after delete_insert handling
  - EXTEND build_insert_sql helper to support ON CONFLICT clause
  - USE ON CONFLICT (alias_name, match_type) DO NOTHING pattern
  - TRACK accurate stats (inserted count = actual inserts, not attempted)

Task 6: Add comprehensive test coverage
UPDATE tests/domain/company_enrichment/test_enrichment_service.py:
  - ADD test for budget default vs override scenarios
  - VERIFY both sync_lookup_budget=None and explicit values work correctly

UPDATE tests/domain/company_enrichment/test_lookup_queue.py:
  - ADD test for temp ID sequential generation (TEMP_000001, TEMP_000002)
  - ADD test for dequeue SQL execution via mock cursor
  - VERIFY plan-only mode compatibility

UPDATE tests/io/test_company_mapping_loader.py:
  - ADD test for append mode with duplicate handling
  - VERIFY stats accuracy with ON CONFLICT scenarios
  - TEST plan-only mode for append operations
```

### Per task pseudocode

```python
# Task 1: Budget fix pseudocode
class CompanyEnrichmentService:
    def resolve_company_id(self, *, sync_lookup_budget: Optional[int] = None):
        # BEFORE: budget = sync_lookup_budget if sync_lookup_budget is not None else 0
        # AFTER: budget = sync_lookup_budget if sync_lookup_budget is not None else self.sync_lookup_budget
        budget = sync_lookup_budget if sync_lookup_budget is not None else self.sync_lookup_budget

        # Rest of method unchanged - preserve existing logging patterns
        logger.debug("Starting unified company ID resolution", extra={"sync_lookup_budget": budget})

# Task 4: Dequeue SQL rewrite pseudocode
class LookupQueue:
    def dequeue(self, batch_size: int = 50):
        if self.plan_only:
            return [mock_requests]  # Keep existing mock behavior

        # NEW: CTE pattern to avoid FOR UPDATE in subquery
        sql = """
            WITH pending AS (
                SELECT id FROM enterprise.lookup_requests
                WHERE status = 'pending'
                ORDER BY created_at ASC
                LIMIT %s
                FOR UPDATE SKIP LOCKED
            )
            UPDATE enterprise.lookup_requests
            SET status = 'processing', updated_at = now()
            FROM pending
            WHERE enterprise.lookup_requests.id = pending.id
            RETURNING id, name, normalized_name, status, attempts, last_error, created_at, updated_at
        """
        # PRESERVE: existing cursor handling and row parsing logic

# Task 5: Append mode pseudocode
def load_company_mappings(mappings, conn, mode="delete_insert"):
    if mode == "delete_insert":
        # Existing logic unchanged
        pass
    elif mode == "append":
        # NEW: Insert with conflict handling
        for chunk in chunks:
            sql = build_insert_sql_with_conflict(table, cols, chunk,
                                               conflict_cols=["alias_name", "match_type"],
                                               conflict_action="DO NOTHING")
            cursor.execute(sql, params)
            # Track actual insertions, not attempts
            stats["inserted"] += cursor.rowcount
```

### Integration Points
```yaml
DATABASE:
  - migration: "ALTER TABLE enterprise.temp_id_sequence RENAME COLUMN id TO last_number; ADD COLUMN updated_at TIMESTAMPTZ"
  - index: "Existing indexes on lookup_requests remain unchanged"

CONFIG:
  - no new environment variables required
  - existing WDH_* settings cover all dependencies

JOBS:
  - process_company_lookup_queue_job should continue working after SQL updates
  - no changes to orchestration layer required
```

## Validation Loop

### Level 1: Syntax & Style
```bash
# Run these FIRST - fix any errors before proceeding
uv run ruff check src/ --fix        # Auto-fix style issues
uv run mypy src/                    # Type checking

# Expected: No errors. If errors, READ and fix.
```

### Level 2: Unit Tests
```python
# NEW test cases to add:

# tests/domain/company_enrichment/test_enrichment_service.py
def test_resolve_company_id_budget_defaults():
    """Test budget uses instance default when not overridden"""
    service = CompanyEnrichmentService(loader, queue, eqc_client, sync_lookup_budget=5)
    # Test with override
    result = service.resolve_company_id(customer_name="Test", sync_lookup_budget=1)
    # Test without override (should use instance default of 5)
    result = service.resolve_company_id(customer_name="Test")

# tests/domain/company_enrichment/test_lookup_queue.py
def test_temp_id_sequence_plan_only():
    """Test temp ID generation in plan-only mode"""
    queue = LookupQueue(connection=None, plan_only=True)
    assert queue.get_next_temp_id() == "TEMP_000001"
    assert queue.get_next_temp_id() == "TEMP_000001"  # Should be consistent in plan-only

def test_dequeue_sql_execution():
    """Test dequeue SQL executes without FOR UPDATE errors"""
    queue = LookupQueue(mock_connection, plan_only=False)
    with patch.object(mock_connection, 'cursor') as mock_cursor:
        mock_cursor.return_value.fetchall.return_value = [mock_row_data]
        requests = queue.dequeue(batch_size=10)
        # Verify SQL contains CTE pattern, not nested FOR UPDATE

# tests/io/test_company_mapping_loader.py
def test_load_company_mappings_append_mode():
    """Test append mode handles duplicates gracefully"""
    mappings = [duplicate_records]
    stats = load_company_mappings(mappings, mock_conn, mode="append")
    assert stats["inserted"] == len(unique_records)  # Only unique inserts counted
```

```bash
# Run tests iteratively until passing:
uv run pytest -v -k "company_enrichment or company_mapping"
uv run pytest -v tests/orchestration/test_ops.py::TestProcessAnnuityPerformanceOp

# If failing: Debug specific test, understand root cause, fix code, re-run
```

### Level 3: Integration Test
```bash
# Test plan-only mode works end-to-end
WDH_DATA_BASE_DIR="tests/fixtures" uv run python -m src.work_data_hub.orchestration.jobs --domain annuity_performance --plan-only --max-files 1

# Test execute mode after fixes
WDH_DATA_BASE_DIR="tests/fixtures" WDH_DATABASE__URI="postgres://..." uv run python -m src.work_data_hub.orchestration.jobs --domain annuity_performance --execute --max-files 1

# Expected: No SQL errors, temp IDs in TEMP_000001 format, append mode works
```

## Final Validation Checklist
- [ ] All tests pass: `uv run pytest -v -k "company_enrichment or company_mapping"`
- [ ] No linting errors: `uv run ruff check src/`
- [ ] No type errors: `uv run mypy src/`
- [ ] Budget respects instance defaults in service
- [ ] Temp ID format is TEMP_000001 sequential
- [ ] Dequeue SQL executes without FOR UPDATE errors
- [ ] Append mode prevents PK violations gracefully
- [ ] Plan-only mode works for all changes
- [ ] Orchestration jobs continue working
- [ ] Migration path documented for schema changes

---

## Anti-Patterns to Avoid
- ❌ Don't change orchestration patterns - keep ops thin
- ❌ Don't break plan-only compatibility - essential for validation
- ❌ Don't ignore existing logging patterns - use extra={} consistently
- ❌ Don't skip migration considerations - document ALTER statements
- ❌ Don't assume cursor types - handle both dict and tuple returns
- ❌ Don't create new files - this is a hardening PRP for existing code

## Confidence Score: 8/10

High confidence due to:
- Clear technical issues identified with specific line numbers
- Existing patterns analyzed and documented
- Postgres FOR UPDATE limitations well understood
- Migration strategy defined for schema changes
- Comprehensive test coverage planned

Minor uncertainty around exact DDL migration approach, but alternatives documented with clear fallback strategies.