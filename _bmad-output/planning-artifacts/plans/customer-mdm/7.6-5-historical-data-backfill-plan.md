# Implementation Plan: Story 7.6-5 Historical Data Backfill

## Overview

| Attribute | Value |
|-----------|-------|
| **Story** | 7.6-5: Historical Data Backfill |
| **Goal** | Backfill 12-24 months of historical 中标/流失 data into customer MDM tables |
| **Type** | Data-only story (no code changes required) |
| **Risk** | Medium - requires data source validation and sheet name mapping |

## Pre-Implementation Analysis

### Source Data Assessment

**File Location**: `data/real_data/Manual Data/【导入模板】台账登记.xlsx`
- File exists and is accessible (393KB)
- Contains historical 中标 and 流失 data

**Expected Sheets** (per data_sources.yml config):
- `企年受托中标(空白)` - Trustee Award
- `企年投资中标(空白)` - Investee Award
- `企年受托流失(解约)` - Trustee Loss
- `企年投资流失(解约)` - Investee Loss

**Potential Issue**: Historical file sheet names may differ from config expectations. Must inspect actual sheet names before ETL execution.

### Target Infrastructure

**Tables** (already exist from Stories 7.6-1, 7.6-2):
- `customer.当年中标` - Annual Award table
- `customer.当年流失` - Annual Loss table

**Domain Implementations** (already exist):
- `src/work_data_hub/domain/annual_award/` - Pipeline-based processing
- `src/work_data_hub/domain/annual_loss/` - Pipeline-based processing

**ETL Configuration** (config/data_sources.yml):
- `annual_award` domain configured with multi-sheet support
- `annual_loss` domain configured with multi-sheet support
- Both use `tests/fixtures/real_data/{YYYYMM}/收集数据/业务收集` base path

### Key Constraint

The ETL pipeline expects files in `tests/fixtures/real_data/{YYYYMM}/收集数据/业务收集/` directory structure. The historical file must be copied to this location before running ETL.

## Implementation Steps

### Step 1: Inspect Source Data Structure

**Objective**: Validate sheet names and column structure before ETL execution.

**Actions**:
1. Run Python script to list actual sheet names in historical file
2. Compare with expected sheet names in `data_sources.yml`
3. Identify any sheet name mismatches requiring config adjustment

**Validation Script**:
```python
import pandas as pd
xl = pd.ExcelFile("data/real_data/Manual Data/【导入模板】台账登记.xlsx")
print("Sheet names:", xl.sheet_names)
```

**Decision Point**: If sheet names differ from config, we have two options:
- Option A: Temporarily modify `data_sources.yml` sheet_names
- Option B: Use `--sheet` CLI parameter to override

### Step 2: Prepare Data Directory Structure

**Objective**: Copy historical file to ETL-expected location.

**Actions**:
1. Create target directory: `tests/fixtures/real_data/202501/收集数据/业务收集/`
2. Copy historical file to target location
3. Verify file is accessible

**Commands** (Unix/bash):
```bash
mkdir -p tests/fixtures/real_data/202501/收集数据/业务收集
cp "data/real_data/Manual Data/【导入模板】台账登记.xlsx" \
   "tests/fixtures/real_data/202501/收集数据/业务收集/【导入模板】台账登记.xlsx"
```

### Step 3: Execute Annual Award Backfill (AC-1)

**Objective**: Load historical 当年中标 data into `customer.当年中标` table.

**Actions**:
1. Run dry-run first to validate data discovery
2. Execute ETL with enrichment enabled
3. Monitor company_id enrichment progress
4. Verify record counts

**Commands**:
```bash
# Dry run (preview only)
uv run --env-file .wdh_env python -m work_data_hub.cli etl \
  --domains annual_award --period 202501 --plan-only

# Execute with enrichment
uv run --env-file .wdh_env python -m work_data_hub.cli etl \
  --domains annual_award --period 202501 --execute
```

**Expected Outcome**: 200-500+ records loaded per year of historical data.

### Step 4: Execute Annual Loss Backfill (AC-2)

**Objective**: Load historical 当年流失 data into `customer.当年流失` table.

**Actions**:
1. Run dry-run first to validate data discovery
2. Execute ETL with enrichment enabled
3. Monitor company_id enrichment progress
4. Verify record counts

**Commands**:
```bash
# Dry run (preview only)
uv run --env-file .wdh_env python -m work_data_hub.cli etl \
  --domains annual_loss --period 202501 --plan-only

# Execute with enrichment
uv run --env-file .wdh_env python -m work_data_hub.cli etl \
  --domains annual_loss --period 202501 --execute
```

**Expected Outcome**: 100-300+ records loaded per year of historical data.

### Step 5: Validate Data Quality (AC-3, AC-4)

**Objective**: Verify data completeness, integrity, and FK relationships.

**Validation Queries**:

```sql
-- Query 1: Count records by month (当年中标)
SELECT
    "上报月份",
    COUNT(*) AS total_records,
    COUNT(company_id) AS with_company_id,
    COUNT(*) - COUNT(company_id) AS null_company_id
FROM customer."当年中标"
GROUP BY "上报月份"
ORDER BY "上报月份" DESC;

-- Query 2: Count records by month (当年流失)
SELECT
    "上报月份",
    COUNT(*) AS total_records,
    COUNT(company_id) AS with_company_id,
    COUNT(*) - COUNT(company_id) AS null_company_id
FROM customer."当年流失"
GROUP BY "上报月份"
ORDER BY "上报月份" DESC;

-- Query 3: Check FK relationships (产品线)
SELECT DISTINCT a."产品线代码"
FROM customer."当年中标" a
LEFT JOIN mapping."产品线" p ON a."产品线代码" = p."产品线代码"
WHERE p."产品线代码" IS NULL AND a."产品线代码" IS NOT NULL;

-- Query 4: Business type distribution
SELECT "业务类型", COUNT(*) AS count
FROM customer."当年中标"
GROUP BY "业务类型";

-- Query 5: Monthly trend via aggregation view
SELECT * FROM customer.v_customer_business_monthly_status_by_type
ORDER BY "上报月份" DESC
LIMIT 24;
```

**Success Criteria**:
- company_id fill rate > 70%
- No orphan FK references for 产品线代码
- 12-24 consecutive months covered

### Step 6: Documentation Update (AC-5)

**Objective**: Update project documentation with backfill results.

**Actions**:
1. Update `docs/database-schema-panorama.md` with actual row counts
2. Update sprint-status.yaml to mark story complete
3. Create validation report in `docs/sprint-artifacts/reviews/`

## Risk Mitigation

### Risk 1: Sheet Name Mismatch
**Mitigation**: Inspect sheet names first (Step 1). If mismatch found, use `--file` parameter with explicit sheet override.

### Risk 2: EQC API Budget Exhaustion
**Mitigation**:
- Monitor API calls during execution
- If budget concerns, use `--no-enrichment` for initial load, then enrich incrementally
- Expected: 50-200 EQC API calls for historical backfill

### Risk 3: Data Quality Issues
**Mitigation**:
- Run validation queries after each ETL execution
- Document any gaps or anomalies in validation report
- Rollback available via TRUNCATE if needed

## Rollback Strategy

```sql
-- WARNING: This will delete ALL data in both tables
TRUNCATE customer."当年中标" RESTART IDENTITY;
TRUNCATE customer."当年流失" RESTART IDENTITY;

-- Re-run ETL to restore data
```

## Success Metrics

| Metric | Target | Validation Method |
|--------|--------|-------------------|
| 当年中标 records | 200-500+ per year | Query 1 |
| 当年流失 records | 100-300+ per year | Query 2 |
| company_id fill rate | >70% | Query 1, 2 |
| Months covered | 12-24 consecutive | Query 1, 2 |
| FK violations | 0 for 产品线代码 | Query 3 |

## Files Affected

This is a **data-only story** - no code modifications required.

**Configuration files** (read-only reference):
- `config/data_sources.yml` - Domain ETL configuration

**Data files**:
- Source: `data/real_data/Manual Data/【导入模板】台账登记.xlsx`
- Target: `tests/fixtures/real_data/202501/收集数据/业务收集/【导入模板】台账登记.xlsx`

**Documentation updates**:
- `docs/database-schema-panorama.md` - Row count estimates
- `docs/sprint-artifacts/sprint-status.yaml` - Story status
- `docs/sprint-artifacts/reviews/validation-report-7.6-5-*.md` - Validation report
