"""
End-to-end parity testing for pipeline vs legacy transformation.

This module provides comprehensive DataFrame comparison between the new pipeline
framework and the legacy transformation logic, ensuring 100% behavioral parity
for safe migration to the shared pipeline architecture.

The test uses the golden baseline generated by scripts/tools/run_legacy_annuity_cleaner.py
and compares it against pipeline output using pandas.testing.assert_frame_equal()
for exact parity validation.
"""

import logging
import pandas as pd
import pytest
from pathlib import Path
from typing import Dict, Any, List

from work_data_hub.domain.annuity_performance.pipeline_steps import (
    build_annuity_pipeline,
    load_mappings_from_json_fixture,
)
from work_data_hub.io.readers.excel_reader import read_excel_rows

logger = logging.getLogger(__name__)

# Test data paths
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
GOLDEN_BASELINE_PATH = PROJECT_ROOT / "tests/fixtures/annuity_performance/golden_legacy.parquet"
SAMPLE_DATA_DIR = PROJECT_ROOT / "tests/fixtures/sample_data/annuity_subsets"
MAPPING_FIXTURE_PATH = PROJECT_ROOT / "tests/fixtures/sample_legacy_mappings.json"

# Excel sheet name for consistent processing
SHEET_NAME = "规模明细"


class PipelineParityTestError(Exception):
    """Raised when pipeline output doesn't match legacy baseline."""
    pass


def load_golden_baseline() -> pd.DataFrame:
    """
    Load the golden baseline generated by the legacy cleaner.

    Returns:
        DataFrame containing the deterministic legacy transformation output

    Raises:
        FileNotFoundError: If golden baseline file doesn't exist
        Exception: If baseline cannot be loaded
    """
    if not GOLDEN_BASELINE_PATH.exists():
        raise FileNotFoundError(
            f"Golden baseline not found: {GOLDEN_BASELINE_PATH}\n"
            f"Run: python scripts/tools/run_legacy_annuity_cleaner.py "
            f"--inputs {SAMPLE_DATA_DIR} --output {GOLDEN_BASELINE_PATH} "
            f"--mappings {MAPPING_FIXTURE_PATH}"
        )

    try:
        df = pd.read_parquet(GOLDEN_BASELINE_PATH)
        logger.info(f"Loaded golden baseline: {len(df)} rows, {len(df.columns)} columns")
        return df
    except Exception as e:
        raise Exception(f"Failed to load golden baseline: {e}") from e


def discover_test_excel_files() -> List[Path]:
    """
    Discover Excel files in the test data directory.

    Returns:
        List of Excel file paths in deterministic order

    Raises:
        FileNotFoundError: If test data directory doesn't exist
    """
    if not SAMPLE_DATA_DIR.exists():
        raise FileNotFoundError(f"Test data directory not found: {SAMPLE_DATA_DIR}")

    excel_files = []
    for pattern in ("*.xlsx", "*.xls"):
        excel_files.extend(SAMPLE_DATA_DIR.glob(pattern))

    # Sort for deterministic ordering (same as baseline script)
    excel_files = sorted(excel_files)

    if not excel_files:
        raise FileNotFoundError(f"No Excel files found in {SAMPLE_DATA_DIR}")

    logger.info(f"Discovered {len(excel_files)} Excel files for testing")
    return excel_files


def load_test_data() -> List[Dict[str, Any]]:
    """
    Load test data from Excel files using the same logic as baseline generation.

    Returns:
        List of raw Excel row dictionaries

    Raises:
        Exception: If test data cannot be loaded
    """
    excel_files = discover_test_excel_files()
    all_rows = []

    for excel_file in excel_files:
        try:
            logger.debug(f"Loading {excel_file.name} with sheet '{SHEET_NAME}'")

            # Use same Excel reading logic as the golden baseline script
            file_rows = read_excel_rows(excel_file, sheet=SHEET_NAME)

            if file_rows:
                # Add source tracking for debugging (same as baseline script)
                for row in file_rows:
                    row["_source_file"] = excel_file.name

                all_rows.extend(file_rows)
                logger.debug(f"Loaded {len(file_rows)} rows from {excel_file.name}")
            else:
                logger.warning(f"No rows loaded from {excel_file.name}")

        except Exception as e:
            logger.error(f"Error loading {excel_file}: {e}")
            # Continue with other files instead of failing completely
            continue

    if not all_rows:
        raise Exception("No test data was successfully loaded from any Excel files")

    logger.info(f"Loaded {len(all_rows)} total rows from {len(excel_files)} Excel files")
    return all_rows


def process_with_pipeline(test_rows: List[Dict[str, Any]]) -> pd.DataFrame:
    """
    Process test data through the new pipeline framework.

    Args:
        test_rows: Raw Excel row dictionaries

    Returns:
        DataFrame containing pipeline transformation output

    Raises:
        Exception: If pipeline processing fails
    """
    try:
        # Load mappings using the same fixture as baseline
        mappings = load_mappings_from_json_fixture(str(MAPPING_FIXTURE_PATH))
        pipeline = build_annuity_pipeline(mappings)

        logger.info(f"Built pipeline with {len(pipeline.steps)} steps")

        # Process each row through pipeline
        pipeline_results = []
        total_warnings = 0
        total_errors = 0

        for i, row in enumerate(test_rows):
            try:
                # Execute pipeline transformation
                result = pipeline.execute(row)

                if not result.errors:
                    pipeline_results.append(result.row)
                    total_warnings += len(result.warnings)
                else:
                    total_errors += len(result.errors)
                    logger.warning(f"Pipeline errors for row {i}: {result.errors}")

            except Exception as e:
                total_errors += 1
                logger.error(f"Pipeline execution failed for row {i}: {e}")

        logger.info(
            f"Pipeline processing completed: {len(pipeline_results)} successful rows, "
            f"{total_warnings} warnings, {total_errors} errors"
        )

        if not pipeline_results:
            raise Exception("Pipeline produced no successful output")

        # Convert to DataFrame with deterministic sorting (same as baseline)
        pipeline_df = pd.DataFrame(pipeline_results)

        # Apply same sorting as golden baseline for consistent comparison
        sort_columns = [col for col in ("月度", "计划代码", "company_id") if col in pipeline_df.columns]
        if sort_columns:
            pipeline_df = pipeline_df.sort_values(sort_columns, na_position="last")

        pipeline_df = pipeline_df.reset_index(drop=True)

        logger.info(f"Pipeline output: {len(pipeline_df)} rows, {len(pipeline_df.columns)} columns")
        return pipeline_df

    except Exception as e:
        raise Exception(f"Pipeline processing failed: {e}") from e


def normalize_dataframes_for_comparison(legacy_df: pd.DataFrame, pipeline_df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:
    """
    Normalize DataFrames for accurate comparison.

    Handles data type consistency, column ordering, and index alignment
    to ensure accurate parity testing.

    Args:
        legacy_df: Golden baseline DataFrame
        pipeline_df: Pipeline output DataFrame

    Returns:
        Tuple of (normalized_legacy_df, normalized_pipeline_df)
    """
    # Make copies to avoid modifying originals
    legacy_norm = legacy_df.copy()
    pipeline_norm = pipeline_df.copy()

    # Get common columns (pipeline might have different column order)
    legacy_cols = set(legacy_norm.columns)
    pipeline_cols = set(pipeline_norm.columns)

    common_cols = legacy_cols.intersection(pipeline_cols)
    legacy_only = legacy_cols - pipeline_cols
    pipeline_only = pipeline_cols - legacy_cols

    if legacy_only:
        logger.warning(f"Columns only in legacy: {sorted(legacy_only)}")
    if pipeline_only:
        logger.warning(f"Columns only in pipeline: {sorted(pipeline_only)}")

    # Use only common columns for comparison, in legacy order
    common_cols_ordered = [col for col in legacy_norm.columns if col in common_cols]

    legacy_norm = legacy_norm[common_cols_ordered]
    pipeline_norm = pipeline_norm[common_cols_ordered]

    # Handle data type inconsistencies
    for col in common_cols_ordered:
        legacy_dtype = legacy_norm[col].dtype
        pipeline_dtype = pipeline_norm[col].dtype

        if legacy_dtype != pipeline_dtype:
            logger.debug(f"Converting {col}: {pipeline_dtype} -> {legacy_dtype}")
            try:
                # Try to convert pipeline column to match legacy dtype
                if legacy_dtype == 'object' and pipeline_dtype != 'object':
                    pipeline_norm[col] = pipeline_norm[col].astype(str)
                elif pd.api.types.is_numeric_dtype(legacy_dtype) and pd.api.types.is_numeric_dtype(pipeline_dtype):
                    pipeline_norm[col] = pipeline_norm[col].astype(legacy_dtype)
                elif pd.api.types.is_datetime64_any_dtype(legacy_dtype):
                    pipeline_norm[col] = pd.to_datetime(pipeline_norm[col])
            except Exception as e:
                logger.warning(f"Could not convert {col} dtype: {e}")

    # Reset indices to ensure alignment
    legacy_norm = legacy_norm.reset_index(drop=True)
    pipeline_norm = pipeline_norm.reset_index(drop=True)

    logger.info(f"Normalized DataFrames for comparison: {len(common_cols_ordered)} common columns")
    return legacy_norm, pipeline_norm


def generate_detailed_diff_report(legacy_df: pd.DataFrame, pipeline_df: pd.DataFrame) -> str:
    """
    Generate detailed difference report for failed parity test.

    Args:
        legacy_df: Golden baseline DataFrame
        pipeline_df: Pipeline output DataFrame

    Returns:
        Detailed string report of differences
    """
    report_lines = ["=" * 80, "PIPELINE PARITY FAILURE REPORT", "=" * 80]

    # Shape comparison
    report_lines.extend([
        f"Shape Comparison:",
        f"  Legacy:   {legacy_df.shape} (rows, columns)",
        f"  Pipeline: {pipeline_df.shape} (rows, columns)",
        ""
    ])

    # Column comparison
    legacy_cols = set(legacy_df.columns)
    pipeline_cols = set(pipeline_df.columns)

    if legacy_cols != pipeline_cols:
        report_lines.extend([
            "Column Differences:",
            f"  Legacy only:   {sorted(legacy_cols - pipeline_cols)}",
            f"  Pipeline only: {sorted(pipeline_cols - legacy_cols)}",
            ""
        ])

    # Data comparison for common columns and rows
    common_cols = sorted(legacy_cols.intersection(pipeline_cols))
    min_rows = min(len(legacy_df), len(pipeline_df))

    if min_rows > 0 and common_cols:
        report_lines.append("Data Differences (first 10 mismatches):")

        diff_count = 0
        for i in range(min_rows):
            if diff_count >= 10:
                break

            for col in common_cols:
                legacy_val = legacy_df.iloc[i][col]
                pipeline_val = pipeline_df.iloc[i][col]

                # Handle NaN comparison
                legacy_is_na = pd.isna(legacy_val)
                pipeline_is_na = pd.isna(pipeline_val)

                if legacy_is_na and pipeline_is_na:
                    continue  # Both NaN, considered equal
                elif legacy_is_na != pipeline_is_na:
                    report_lines.append(f"  Row {i}, {col}: Legacy='{legacy_val}' vs Pipeline='{pipeline_val}' (NaN mismatch)")
                    diff_count += 1
                elif str(legacy_val) != str(pipeline_val):
                    report_lines.append(f"  Row {i}, {col}: Legacy='{legacy_val}' vs Pipeline='{pipeline_val}'")
                    diff_count += 1

                if diff_count >= 10:
                    break

        if diff_count == 0:
            report_lines.append("  No data differences found in sampled rows")
        elif diff_count >= 10:
            report_lines.append("  ... (showing first 10 differences)")

    report_lines.extend(["", "=" * 80])
    return "\n".join(report_lines)


@pytest.mark.e2e
def test_pipeline_vs_legacy_parity():
    """
    Test that pipeline transformation produces identical output to legacy cleaner.

    This is the critical parity test that ensures the new pipeline framework
    produces exactly the same transformation results as the legacy AnnuityPerformanceCleaner.

    The test loads the golden baseline, processes the same test data through the pipeline,
    and compares DataFrames using pandas.testing.assert_frame_equal() for exact parity.

    Raises:
        PipelineParityTestError: If pipeline output doesn't match legacy baseline
        AssertionError: If DataFrames are not equal (with detailed diff report)
    """
    logger.info("Starting pipeline vs legacy parity test")

    try:
        # Load golden baseline (legacy transformation output)
        golden_baseline = load_golden_baseline()
        logger.info(f"Loaded golden baseline: {golden_baseline.shape}")

        # Load same test data used for baseline generation
        test_data = load_test_data()
        logger.info(f"Loaded test data: {len(test_data)} rows")

        # Process test data through new pipeline
        pipeline_output = process_with_pipeline(test_data)
        logger.info(f"Generated pipeline output: {pipeline_output.shape}")

        # Normalize DataFrames for accurate comparison
        legacy_norm, pipeline_norm = normalize_dataframes_for_comparison(
            golden_baseline, pipeline_output
        )

        # CRITICAL: Assert exact DataFrame equality
        try:
            pd.testing.assert_frame_equal(
                legacy_norm,
                pipeline_norm,
                check_dtype=True,
                check_index_type=True,
                check_column_type=True,
                check_names=True,
                check_exact=False,  # Allow minor floating point differences
                rtol=1e-5,  # Relative tolerance for numeric comparison
                atol=1e-8,  # Absolute tolerance for numeric comparison
            )

            logger.info("✅ PARITY TEST PASSED: Pipeline output matches legacy baseline exactly")

        except AssertionError as e:
            # Generate detailed diff report for debugging
            diff_report = generate_detailed_diff_report(legacy_norm, pipeline_norm)

            error_message = (
                f"Pipeline output does not match legacy baseline!\n\n"
                f"Pandas assertion error:\n{str(e)}\n\n"
                f"{diff_report}"
            )

            logger.error("❌ PARITY TEST FAILED")
            logger.error(diff_report)

            raise PipelineParityTestError(error_message) from e

    except Exception as e:
        if isinstance(e, PipelineParityTestError):
            raise
        else:
            raise PipelineParityTestError(f"Parity test execution failed: {e}") from e


@pytest.mark.e2e
def test_pipeline_basic_functionality():
    """
    Test basic pipeline functionality without legacy comparison.

    This test verifies that the pipeline can successfully process test data
    and produce reasonable output, even if exact parity testing fails.

    This provides a safety net to ensure the pipeline itself is functional.
    """
    logger.info("Starting pipeline basic functionality test")

    try:
        # Load test data
        test_data = load_test_data()
        assert len(test_data) > 0, "No test data loaded"

        # Process through pipeline
        pipeline_output = process_with_pipeline(test_data)

        # Basic assertions
        assert not pipeline_output.empty, "Pipeline produced no output"
        assert len(pipeline_output) > 0, "Pipeline output has no rows"

        # Verify expected columns exist (basic sanity check)
        expected_columns = ["月度", "计划代码", "company_id", "_source_file"]
        missing_columns = [col for col in expected_columns if col not in pipeline_output.columns]

        if missing_columns:
            logger.warning(f"Missing expected columns: {missing_columns}")

        logger.info(f"✅ Pipeline basic functionality test passed: {len(pipeline_output)} rows processed")

    except Exception as e:
        logger.error(f"❌ Pipeline basic functionality test failed: {e}")
        raise


if __name__ == "__main__":
    """Allow running parity test directly for debugging."""
    import sys

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s"
    )

    try:
        test_pipeline_vs_legacy_parity()
        print("✅ Parity test passed!")
        sys.exit(0)
    except PipelineParityTestError as e:
        print(f"❌ Parity test failed:\n{e}")
        sys.exit(1)
    except Exception as e:
        print(f"❌ Test execution error: {e}")
        sys.exit(1)