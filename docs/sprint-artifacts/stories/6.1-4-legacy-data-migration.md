# Story 6.1.4: Legacy Data Migration to Enrichment Index

Status: Done

## Story

As a **data engineer**,
I want **to migrate existing legacy data (company_id_mapping ~19,141 rows, eqc_search_result ~11,820 rows) to the new `enrichment_index` table**,
so that **Layer 2 (Database Cache) has a rich initial dataset for multi-priority lookups, significantly improving cache hit rates from day one and reducing dependency on EQC API calls**.

## Acceptance Criteria

1. **AC1: company_id_mapping Migration** - All valid records from `legacy.company_id_mapping` (~19,141 rows) are migrated to `enterprise.enrichment_index`:
   - `company_name` → normalized as `lookup_key` using `normalize_for_temp_id()`
   - `lookup_type` = `customer_name` (DB-P4)
   - `company_id` → `company_id`
   - `type='current'` → `confidence=1.00`, `type='former'` → `confidence=0.90`
   - `source` = `legacy_migration`
   - `source_table` = `legacy.company_id_mapping`

2. **AC2: eqc_search_result Migration** - All successful records from `legacy.eqc_search_result` (~11,820 rows) are migrated:
   - Only records where `result='Success'` AND `company_id IS NOT NULL`
   - `key_word` → normalized as `lookup_key` using `normalize_for_temp_id()`
   - `lookup_type` = `customer_name` (DB-P4)
   - `company_id` → `company_id`
   - `confidence` = 1.00
   - `source` = `legacy_migration`
   - `source_table` = `legacy.eqc_search_result`

3. **AC3: Deduplication** - When same `(lookup_key, lookup_type)` exists from multiple sources:
   - Use ON CONFLICT DO UPDATE with GREATEST confidence semantics
   - Higher confidence wins (current > former, eqc_api = current)
   - Increment `hit_count` on conflict to track popularity

4. **AC4: Normalization Consistency** - All `lookup_key` values use `normalize_for_temp_id()` to ensure cache hits match Layer 2 lookup queries

5. **AC5: Migration Script** - Create Python migration script (NOT Alembic) in `scripts/migrations/`:
   - Reads from legacy tables via SQL
   - Transforms data in Python (normalization, confidence mapping)
   - Batch inserts using `insert_enrichment_index_batch()`
   - Idempotent: safe to re-run (ON CONFLICT handles duplicates)

6. **AC6: Validation Report** - Migration script outputs validation report:
   - Total records processed from each source
   - Records inserted vs updated (conflict)
   - Records skipped (null company_id, empty names)
   - Sample of migrated records for spot-check

7. **AC7: Data Integrity** - No data loss during migration:
   - All valid mappings preserved
   - Original legacy tables remain unchanged (read-only migration)
   - Rollback capability via DELETE WHERE source='legacy_migration'

8. **AC8: Performance** - Migration completes in reasonable time:
   - Batch processing (1000 records per batch)
   - Progress logging every 5000 records
   - Total ~31,000 records should complete in <5 minutes

9. **AC9: Unit Tests** - Migration logic has unit tests with >90% coverage:
   - Test normalization transformation
   - Test confidence mapping (current vs former)
   - Test deduplication logic
   - Test batch processing

10. **AC10: Integration Test** - End-to-end test with sample data:
    - Create test legacy tables with sample data
    - Run migration
    - Verify enrichment_index populated correctly
    - Verify Layer 2 cache hits after migration
11. **AC11: 环境与兼容性验证 + 基线记录**
    - 运行前完成兼容性检查：Python >=3.10 且使用 `PYTHONPATH=src uv run`；Postgres 14+（或项目默认版本）可访问；`enterprise.enrichment_index` 表与索引存在，`lookup_type='customer_name'` 可用；`insert_enrichment_index_batch()` 具备 GREATEST/`hit_count+1` 的 ON CONFLICT 语义
    - 在影子/预生产环境以 `--dry-run` 小样本（<=1000 行）验证查询、插入路径与日志格式
    - 记录一次基线耗时与吞吐（如 ~31k 行、batch 1000，预期 <5 分钟；日志每 5000 行输出），写入验证报告

## Tasks / Subtasks

- [x] Task 0: Pre-flight validation (AC: 11)
  - [x] 0.1 检查运行环境：Python >=3.10，使用 `PYTHONPATH=src uv run`；Postgres 14+ 或项目默认版本可连通
  - [x] 0.2 验证依赖：`enterprise.enrichment_index` 存在且含唯一约束/索引；`lookup_type` 包含 `customer_name`
  - [x] 0.3 验证仓储 API：`insert_enrichment_index_batch()` 具备 GREATEST confidence、`hit_count+1`、冲突更新语义
  - [x] 0.4 影子演练：`--dry-run` 小样本（<=1000 行）执行，确认日志、吞吐与报告输出路径

- [x] Task 1: Create migration script structure (AC: 5)
  - [x] 1.1 Create `scripts/migrations/migrate_legacy_to_enrichment_index.py`
  - [x] 1.2 Define `LegacyMigrationConfig` dataclass with batch size, source tables
  - [x] 1.3 Implement CLI interface with argparse (--dry-run, --batch-size, --verbose)
  - [x] 1.4 Add logging with structlog

- [x] Task 2: Implement company_id_mapping migration (AC: 1, 4)
  - [x] 2.1 Create `_migrate_company_id_mapping()` method
  - [x] 2.2 Query legacy.company_id_mapping with pagination
  - [x] 2.3 Transform: normalize company_name, map type to confidence
  - [x] 2.4 Create EnrichmentIndexRecord objects
  - [x] 2.5 Batch insert using repository method

- [x] Task 3: Implement eqc_search_result migration (AC: 2, 4)
  - [x] 3.1 Create `_migrate_eqc_search_result()` method
  - [x] 3.2 Query legacy.eqc_search_result WHERE result='Success'
  - [x] 3.3 Transform: normalize key_word, set confidence=1.00
  - [x] 3.4 Create EnrichmentIndexRecord objects
  - [x] 3.5 Batch insert using repository method

- [x] Task 4: Implement deduplication and conflict handling (AC: 3)
  - [x] 4.1 Rely on `insert_enrichment_index_batch()` ON CONFLICT behavior
  - [x] 4.2 Verify GREATEST confidence semantics work correctly
  - [x] 4.3 Track inserted vs updated counts from batch result

- [x] Task 5: Implement validation report (AC: 6)
  - [x] 5.1 Create `MigrationReport` dataclass
  - [x] 5.2 Track counts: processed, inserted, updated, skipped per source
  - [x] 5.3 Generate summary report at end
  - [x] 5.4 Output sample records for verification

- [x] Task 6: Add rollback capability (AC: 7)
  - [x] 6.1 Implement `--rollback` CLI option
  - [x] 6.2 DELETE FROM enrichment_index WHERE source='legacy_migration'
  - [x] 6.3 Confirm before rollback (unless --force)

- [x] Task 7: Write unit tests (AC: 9)
  - [x] 7.1 Test normalization transformation
  - [x] 7.2 Test confidence mapping (current=1.00, former=0.90)
  - [x] 7.3 Test null/empty value filtering
  - [x] 7.4 Test batch processing logic
  - [x] 7.5 Test report generation

- [x] Task 8: Write integration test (AC: 10)
  - [x] 8.1 Create test fixtures with sample legacy data
  - [x] 8.2 Run migration against test database
  - [x] 8.3 Verify enrichment_index contents
  - [x] 8.4 Verify Layer 2 cache hits work

## Dev Notes

### Runtime / Dependency Baseline
- Python: >=3.10（项目 `pyproject.toml`），使用 `PYTHONPATH=src uv run`
- DB: Postgres 14+（或项目默认版本），可访问 `enterprise` schema
- 依赖：重用 `psycopg2-binary`、`sqlalchemy>=2.0`；不新增库

### Compatibility Checklist（执行前必须完成）
1) `enterprise.enrichment_index` 表与唯一键/索引存在；`lookup_type` 覆盖 `customer_name`  
2) `insert_enrichment_index_batch()` 具备 ON CONFLICT GREATEST(confidence)、`hit_count = hit_count + 1` 语义  
3) 配置正确加载：`normalize_for_temp_id()` 可用；连接字符串指向目标库（避免写入生产）  
4) 权限：具备 SELECT legacy 源表、INSERT/UPDATE enterprise.enrichment_index 权限  
5) 日志与报告目录可写（scripts/migrations/*，tests/*）

### Shadow Run & Performance Baseline
- 建议命令：`PYTHONPATH=src uv run python scripts/migrations/migrate_legacy_to_enrichment_index.py --dry-run --batch-size 500 --verbose`（影子库/小样本）
- 记录基线：总行数、batch 大小、耗时、每 5000 行日志打点、吞吐（行/秒）；作为回归对照
- 目标：全量 ~31k 行，batch 1000，<5 分钟；若超时，调整 batch 或索引/事务配置

#### 实际基线数据 (2025-12-08 Dry-Run)

| 指标 | 值 |
|------|-----|
| **环境** | PostgreSQL 14+, localhost:5432/postgres |
| **company_id_mapping 读取** | 19,141 行 |
| **eqc_search_result 读取** | 11,657 行 |
| **总行数** | 30,798 行 |
| **批处理大小** | 1,000 |
| **总耗时** | 5.019 秒 |
| **吞吐量** | ~6,137 行/秒 |
| **日志打点** | 每 5,000 行 |
| **跳过数** | 0 |
| **错误数** | 0 |
| **结论** | ✅ 远超目标 (<5分钟)，实际仅需 ~5秒 |

### Regression Watchpoints
- `mapping_repository.py` 冲突语义若变更，需更新脚本假设
- `enrichment_index` 索引/列名若调整，需同步更新脚本字段映射
- 若引入新 `lookup_type`，需确认默认 `customer_name` 路径不受影响

### Architecture Compliance

**CRITICAL: This story creates a standalone migration script, NOT an Alembic migration**

1. **Script Location**: `scripts/migrations/migrate_legacy_to_enrichment_index.py`
2. **Repository Reuse**: Use existing `CompanyMappingRepository.insert_enrichment_index_batch()` from Story 6.1.1
3. **Normalizer Reuse**: Use `normalize_for_temp_id()` from `src/work_data_hub/infrastructure/enrichment/normalizer.py`
4. **No Schema Changes**: This story only migrates data, does not modify database schema
5. **Idempotent**: Script can be re-run safely; ON CONFLICT handles duplicates

### Legacy Data Structure Analysis

**legacy.company_id_mapping (~19,141 rows)**:
```sql
CREATE TABLE legacy.company_id_mapping (
    company_name VARCHAR(255) NOT NULL,  -- Source for lookup_key (needs normalization)
    company_id VARCHAR(255),              -- Target company_id
    unite_code VARCHAR(255),              -- Unified social credit code (not used)
    type VARCHAR(10),                     -- 'current' or 'former' (maps to confidence)
    id SERIAL PRIMARY KEY
);
```

**Key observations**:
- `company_name` contains Chinese company names, some with leading/trailing spaces
- `type='current'` indicates active mapping (confidence=1.00)
- `type='former'` indicates historical mapping (confidence=0.90)
- Some records have same company_name with different types (dedup needed)

**legacy.eqc_search_result (~11,820 rows)**:
```sql
CREATE TABLE legacy.eqc_search_result (
    _id VARCHAR(255),                     -- MongoDB ObjectId (not used)
    key_word VARCHAR(255) NOT NULL,       -- Source for lookup_key (needs normalization)
    company_id VARCHAR(255),              -- Target company_id
    company_name VARCHAR(255),            -- Resolved company name (not used)
    unite_code VARCHAR(255),              -- Unified social credit code (not used)
    result VARCHAR(255),                  -- 'Success' or error message
    PRIMARY KEY (key_word)
);
```

**Key observations**:
- `key_word` can be company name, unified code, or other identifiers
- Only migrate records where `result='Success'` AND `company_id IS NOT NULL`
- `company_id` format varies: some are numeric IDs, some are unified codes

### Migration Flow

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    Legacy Data Migration Flow                                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Step 1: Read from legacy.company_id_mapping                                 │
│       ↓                                                                      │
│  Step 2: Transform each record:                                              │
│    - lookup_key = normalize_for_temp_id(company_name)                        │
│    - lookup_type = LookupType.CUSTOMER_NAME                                  │
│    - confidence = 1.00 if type='current' else 0.90                           │
│    - source = SourceType.LEGACY_MIGRATION                                    │
│    - source_table = 'legacy.company_id_mapping'                              │
│       ↓                                                                      │
│  Step 3: Filter invalid records:                                             │
│    - Skip if company_id is NULL or empty                                     │
│    - Skip if normalized lookup_key is empty                                  │
│       ↓                                                                      │
│  Step 4: Batch insert (1000 records per batch)                               │
│    - insert_enrichment_index_batch() handles ON CONFLICT                     │
│       ↓                                                                      │
│  Step 5: Repeat for legacy.eqc_search_result                                 │
│    - lookup_key = normalize_for_temp_id(key_word)                            │
│    - confidence = 1.00 (all successful EQC results)                          │
│    - source_table = 'legacy.eqc_search_result'                               │
│       ↓                                                                      │
│  Step 6: Generate validation report                                          │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Confidence Mapping Rules

| Source | Type | Confidence | Rationale |
|--------|------|------------|-----------|
| company_id_mapping | current | 1.00 | Active, verified mapping |
| company_id_mapping | former | 0.90 | Historical, may be outdated |
| eqc_search_result | Success | 1.00 | API-verified mapping |

### Deduplication Strategy

When same `(lookup_key, lookup_type)` appears multiple times:

1. **Within company_id_mapping**: Same company_name with both 'current' and 'former' types
   - Process 'current' first (confidence=1.00)
   - 'former' will conflict but won't overwrite (GREATEST keeps 1.00)

2. **Between sources**: Same normalized name in both tables
   - Both have confidence=1.00, first insert wins
   - hit_count increments on conflict

3. **ON CONFLICT behavior** (from Story 6.1.1):
```sql
ON CONFLICT (lookup_key, lookup_type) DO UPDATE SET
    confidence = GREATEST(enrichment_index.confidence, EXCLUDED.confidence),
    hit_count = enrichment_index.hit_count + 1,
    updated_at = NOW()
```

### Script Usage

```bash
# Dry run (no actual inserts)
uv run python scripts/migrations/migrate_legacy_to_enrichment_index.py --dry-run

# Full migration with verbose logging
uv run python scripts/migrations/migrate_legacy_to_enrichment_index.py --verbose

# Custom batch size
uv run python scripts/migrations/migrate_legacy_to_enrichment_index.py --batch-size 500

# Rollback (delete all legacy_migration records)
uv run python scripts/migrations/migrate_legacy_to_enrichment_index.py --rollback

# Force rollback without confirmation
uv run python scripts/migrations/migrate_legacy_to_enrichment_index.py --rollback --force

# Write validation report to a custom path (includes preflight + insert/update stats)
uv run python scripts/migrations/migrate_legacy_to_enrichment_index.py --report-path /tmp/legacy-migration-validation.md

# Skip preflight checks (not recommended)
uv run python scripts/migrations/migrate_legacy_to_enrichment_index.py --skip-preflight
``` 

### Code Location Reference

| Component | Path |
|-----------|------|
| Migration Script (NEW) | `scripts/migrations/migrate_legacy_to_enrichment_index.py` |
| Types | `src/work_data_hub/infrastructure/enrichment/types.py` |
| Repository | `src/work_data_hub/infrastructure/enrichment/mapping_repository.py` |
| Normalizer | `src/work_data_hub/infrastructure/enrichment/normalizer.py` |
| Tests (NEW) | `tests/unit/scripts/test_migrate_legacy_to_enrichment_index.py` |
| Integration Test (NEW) | `tests/integration/scripts/test_legacy_migration_integration.py` |
| Legacy Data | `reference/archive/db_migration/sqls/enterprise/` |

### Existing Repository Methods to Use

From Story 6.1.1 (`mapping_repository.py:783-912`):
```python
def insert_enrichment_index_batch(
    self,
    records: List[EnrichmentIndexRecord],
) -> InsertBatchResult:
    """
    Batch insert into enterprise.enrichment_index with conflict handling.

    Uses ON CONFLICT DO UPDATE with:
    - confidence: GREATEST(existing, new)
    - hit_count: existing + 1
    - timestamps: update on conflict

    Returns:
        InsertBatchResult with inserted and updated counts
    """
```

### Existing Types to Use

From Story 6.1.1 (`types.py:253-348`):
```python
class LookupType(Enum):
    PLAN_CODE = "plan_code"
    ACCOUNT_NAME = "account_name"
    ACCOUNT_NUMBER = "account_number"
    CUSTOMER_NAME = "customer_name"  # Use this for legacy migration
    PLAN_CUSTOMER = "plan_customer"

class SourceType(Enum):
    YAML = "yaml"
    EQC_API = "eqc_api"
    MANUAL = "manual"
    BACKFLOW = "backflow"
    DOMAIN_LEARNING = "domain_learning"
    LEGACY_MIGRATION = "legacy_migration"  # Use this!

@dataclass
class EnrichmentIndexRecord:
    lookup_key: str
    lookup_type: LookupType
    company_id: str
    source: SourceType
    confidence: Decimal = field(default_factory=lambda: Decimal("1.00"))
    source_domain: Optional[str] = None
    source_table: Optional[str] = None
    # ...
```

### Normalization Reference

**CRITICAL: Use the same normalizer as Layer 2 lookup to ensure cache hits**

```python
from work_data_hub.infrastructure.enrichment.normalizer import normalize_for_temp_id

# For company_name from company_id_mapping:
normalized_key = normalize_for_temp_id(company_name)

# For key_word from eqc_search_result:
normalized_key = normalize_for_temp_id(key_word)
```

### Sample Migration Code

```python
from decimal import Decimal
from typing import List, Generator
from dataclasses import dataclass

from work_data_hub.infrastructure.enrichment.types import (
    EnrichmentIndexRecord,
    LookupType,
    SourceType,
)
from work_data_hub.infrastructure.enrichment.normalizer import normalize_for_temp_id
from work_data_hub.infrastructure.enrichment.mapping_repository import CompanyMappingRepository

@dataclass
class MigrationReport:
    source_table: str
    total_read: int = 0
    inserted: int = 0
    updated: int = 0
    skipped: int = 0
    errors: int = 0

def migrate_company_id_mapping(
    repo: CompanyMappingRepository,
    batch_size: int = 1000,
    dry_run: bool = False,
) -> MigrationReport:
    """Migrate legacy.company_id_mapping to enrichment_index."""
    report = MigrationReport(source_table="legacy.company_id_mapping")

    # Query legacy data (process 'current' first for higher confidence)
    query = """
        SELECT company_name, company_id, type
        FROM legacy.company_id_mapping
        WHERE company_id IS NOT NULL AND company_name IS NOT NULL
        ORDER BY CASE WHEN type = 'current' THEN 0 ELSE 1 END
    """

    records: List[EnrichmentIndexRecord] = []

    for row in execute_query(query):
        report.total_read += 1

        # Normalize lookup key
        normalized = normalize_for_temp_id(row["company_name"])
        if not normalized:
            report.skipped += 1
            continue

        # Map confidence based on type
        confidence = Decimal("1.00") if row["type"] == "current" else Decimal("0.90")

        record = EnrichmentIndexRecord(
            lookup_key=normalized,
            lookup_type=LookupType.CUSTOMER_NAME,
            company_id=row["company_id"],
            confidence=confidence,
            source=SourceType.LEGACY_MIGRATION,
            source_table="legacy.company_id_mapping",
        )
        records.append(record)

        # Batch insert
        if len(records) >= batch_size:
            if not dry_run:
                result = repo.insert_enrichment_index_batch(records)
                report.inserted += result.inserted
                report.updated += result.updated
            records = []

    # Final batch
    if records and not dry_run:
        result = repo.insert_enrichment_index_batch(records)
        report.inserted += result.inserted
        report.updated += result.updated

    return report
```

### Testing Standards

- Use pytest fixtures for database connections
- Mock `CompanyMappingRepository` for unit tests
- Use real database for integration tests
- Target >90% coverage for new code
- Include edge cases: null values, empty strings, special characters, duplicates

### Previous Story Learnings

**From Story 6.1.1:**
- Repository methods use UNNEST for batch efficiency
- ON CONFLICT DO UPDATE implements GREATEST confidence semantics
- `EnrichmentIndexRecord.from_dict()` handles database row conversion
- All 207 enrichment module tests pass (no regressions)

**From Story 6.1.2:**
- `_resolve_via_enrichment_index()` implements DB-P1 to DB-P5 priority lookup
- `ResolutionStatistics.db_cache_hits` is `Dict[str, int]` with all 5 priority keys
- Decision path logging implemented with DEBUG level per-row and INFO level summary

**From Story 6.1.3:**
- `DomainLearningService` provides pattern for batch data extraction and insertion
- Config safeguards: domain gating, threshold enforcement
- Structured logging with all thresholds and counts

### Git Commit Patterns

Recent commits follow this pattern:
```
feat(story-6.1.X): <description>
```

Example:
```
feat(story-6.1.4): implement legacy data migration to enrichment_index
```

### Project Structure Notes

- New script: `scripts/migrations/migrate_legacy_to_enrichment_index.py`
- New test file: `tests/unit/scripts/test_migrate_legacy_to_enrichment_index.py`
- Follow existing code style in enrichment module
- Use structlog for logging

### Expected Migration Results

| Source | Total Records | Expected Migrated | Notes |
|--------|---------------|-------------------|-------|
| company_id_mapping | ~19,141 | ~18,000+ | Some may have null company_id |
| eqc_search_result | ~11,820 | ~11,000+ | Only Success records |
| **Total** | ~30,961 | ~29,000+ | After deduplication |

### Post-Migration Verification

After migration, verify with:

```sql
-- Count by source
SELECT source, source_table, COUNT(*)
FROM enterprise.enrichment_index
WHERE source = 'legacy_migration'
GROUP BY source, source_table;

-- Sample records
SELECT lookup_key, lookup_type, company_id, confidence, source_table
FROM enterprise.enrichment_index
WHERE source = 'legacy_migration'
LIMIT 10;

-- Check for Layer 2 cache hits
-- Run a test resolution and verify DB-P4 hits
```

### References

- [Source: docs/specific/company-enrichment-service/golden-dataset-testing-plan.md#5]
- [Source: docs/guides/company-enrichment-service.md#5.1]
- [Source: docs/sprint-artifacts/sprint-change-proposal/sprint-change-proposal-2025-12-08-layer2-enrichment-enhancement.md#4.2]
- [Source: reference/archive/db_migration/sqls/enterprise/company_id_mapping_converted.sql]
- [Source: reference/archive/db_migration/sqls/enterprise/eqc_search_result_converted.sql]
- [Source: src/work_data_hub/infrastructure/enrichment/mapping_repository.py:783-912] - insert_enrichment_index_batch()
- [Source: src/work_data_hub/infrastructure/enrichment/types.py:253-348] - EnrichmentIndexRecord
- [Source: docs/sprint-artifacts/stories/6.1-1-enrichment-index-schema-enhancement.md] - Schema and repository methods
- [Source: docs/sprint-artifacts/stories/6.1-3-domain-learning-mechanism.md] - Batch processing pattern

## Dev Agent Record

### Context Reference

<!-- Path(s) to story context XML will be added here by context workflow -->

### Agent Model Used

Claude Opus 4.5 (claude-opus-4-5-20251101)

### Debug Log References

### Completion Notes List

- Story created from Sprint Change Proposal: Layer 2 Enrichment Index Enhancement
- This is the fourth and final story in Epic 6.1
- Depends on Story 6.1.1 (schema and repository methods)
- Creates standalone Python migration script (NOT Alembic migration)
- Migrates ~31,000 legacy records to enrichment_index table
- Uses existing `insert_enrichment_index_batch()` with ON CONFLICT handling
- All lookup_keys normalized using `normalize_for_temp_id()` for cache hit consistency
- Confidence mapping: current=1.00, former=0.90, eqc_success=1.00
- Idempotent: safe to re-run, ON CONFLICT handles duplicates
- Ultimate context engine analysis completed - comprehensive developer guide created
- **Implementation completed (2025-12-08):**
- Migration script with CLI interface (--dry-run, --batch-size, --verbose, --rollback, --force)
- `LegacyMigrationConfig` dataclass for configuration
- `MigrationReport` and `FullMigrationReport` for validation reporting
- `migrate_company_id_mapping()` - migrates legacy.company_id_mapping with confidence mapping
- `migrate_eqc_search_result()` - migrates legacy.eqc_search_result (Success records only)
- `rollback_migration()` - DELETE WHERE source='legacy_migration'
- 36 unit tests covering normalization, confidence mapping, batch processing, report generation
- 10 integration tests for end-to-end validation with sample data
- All 848 unit tests pass (no regressions)
- Added preflight validation (Python/Postgres/table/index) and conflict-aware insert/update reporting with optional `--report-path` output

### File List

**Files Created:**
- `scripts/migrations/migrate_legacy_to_enrichment_index.py` - Main migration script
- `scripts/migrations/__init__.py` - Package init
- `tests/unit/scripts/test_migrate_legacy_to_enrichment_index.py` - 36 unit tests
- `tests/unit/scripts/__init__.py` - Package init
- `tests/integration/scripts/test_legacy_migration_integration.py` - 10 integration tests
- `tests/integration/scripts/__init__.py` - Package init
- `docs/templates/migration-baseline-template.md` - Baseline template for validation runs
- `docs/sprint-artifacts/stories/validation-report-20251208-154951.md` - Validation report snapshot

**Files Referenced (NOT MODIFIED):**
- `src/work_data_hub/infrastructure/enrichment/types.py`
- `src/work_data_hub/infrastructure/enrichment/mapping_repository.py`
- `src/work_data_hub/infrastructure/enrichment/normalizer.py`

**Files Modified:**
- `docs/sprint-artifacts/sprint-status.yaml`

## Change Log

| Date | Change | Author |
|------|--------|--------|
| 2025-12-08 | Story drafted with comprehensive developer context | Claude Opus 4.5 |
| 2025-12-08 | Implementation completed: migration script, unit tests (36), integration tests (10), all 848 tests pass | Claude Opus 4.5 |
| 2025-12-08 | Baseline data recorded: 30,798 rows in 5.019s (~6,137 rows/s), validation report recommendations all addressed | Claude Opus 4.5 |
