# Story 5.6.1: Dry Run Failed Records Export

Status: Ready for Review

## Story

As a **data engineer**,
I want **failed validation records exported to CSV during Dry Run mode**,
so that **I can quickly identify and fix data quality issues**.

## Acceptance Criteria

1. **AC1**: When `dropped_count > 0` during `convert_dataframe_to_models()`, failed rows are exported to `logs/failed_records_*.csv`
2. **AC2**: CSV contains all original columns from the source DataFrame for debugging
3. **AC3**: Log message includes count and file path of exported failed records
4. **AC4**: No changes to existing function signatures in `helpers.py` (backward compatible)
5. **AC5**: Implementation applies to both `annuity_performance` and `annuity_income` domains
6. **AC6**: Unit tests verify failed record export functionality

## Tasks / Subtasks

- [x] Task 1: Add failed records export to `annuity_performance/service.py` (AC: #1, #2, #3, #4)
  - [x] 1.1: After `convert_dataframe_to_models()` in `service.py`, identify failed rows by comparing input vs output
  - [x] 1.2: Use existing `export_error_csv()` from `infrastructure.validation` to export failed DataFrame
  - [x] 1.3: Add structured log with count and path
- [x] Task 2: Add failed records export to `annuity_income/service.py` (AC: #5)
  - [x] 2.1: Apply same pattern as annuity_performance
- [x] Task 3: Write unit tests (AC: #6)
  - [x] 3.1: Test that failed rows are exported when validation drops records
  - [x] 3.2: Test that no export occurs when all records pass validation

## Dev Notes

### Implementation Approach (KISS/YAGNI Simplified)

**Principle:** Reuse existing infrastructure. No new dataclasses, no function signature changes.

The sprint change proposal identified that the original complex approach (~80 LOC) can be simplified to ~10 LOC by reusing existing `export_error_csv()` function.

### Key Code Locations

| File | Purpose | Lines to Modify |
|------|---------|-----------------|
| `src/work_data_hub/domain/annuity_performance/service.py` | Main service | After line 144 (dropped rows logging) |
| `src/work_data_hub/domain/annuity_income/service.py` | Income service | After line 210 (dropped rows logging) |
| `src/work_data_hub/infrastructure/validation/report_generator.py` | Existing export function | No changes needed |

### Implementation Pattern

Add ~10 lines after the existing dropped rows logging in `process_with_enrichment()`:

```python
# After: dropped_count = len(rows) - len(records)
# After: if dropped_count > 0: ... (existing warning log)

# NEW: Export failed rows to CSV for debugging
if dropped_count > 0:
    # Get indices of successfully converted records
    success_indices = {r.计划代码 for r in records}
    # Filter to get failed rows from original DataFrame
    failed_df = result_df[~result_df["计划代码"].isin(success_indices)]

    if not failed_df.empty:
        csv_path = export_error_csv(
            failed_df,
            filename_prefix=f"failed_records_{data_source}",
            output_dir=Path("logs"),
        )
        event_logger.info("Exported failed records", csv_path=str(csv_path), count=len(failed_df))
```

### Existing Infrastructure to Reuse

**`export_error_csv()`** from `work_data_hub.infrastructure.validation`:
- Already handles timestamped filenames
- Creates output directory if needed
- Writes metadata header with date and count
- Returns Path to generated file

[Source: src/work_data_hub/infrastructure/validation/report_generator.py#L42-95]

### Domain-Specific Key Fields

| Domain | Primary Key Field | Used for Identifying Failed Rows |
|--------|-------------------|----------------------------------|
| `annuity_performance` | `计划代码` | Compare input vs output records |
| `annuity_income` | `计划号` + `组合代码` | Composite key comparison |

### Project Structure Notes

- Follows existing pattern of using `export_error_csv()` for CSV exports
- Aligns with `export_unknown_names_csv()` pattern in helpers.py
- Output directory: `logs/` (consistent with existing exports)
- No new dependencies required

### Testing Strategy

1. **Unit Test**: Mock `export_error_csv` and verify it's called with correct DataFrame when records are dropped
2. **Unit Test**: Verify no export when all records pass validation
3. **Integration Test** (optional): Run with sample data that has validation failures

### References

- [Source: docs/sprint-artifacts/sprint-change-proposal/sprint-change-proposal-2025-12-06-post-mvp-optimizations.md#OPT-001]
- [Source: src/work_data_hub/domain/annuity_performance/service.py#L110-173]
- [Source: src/work_data_hub/domain/annuity_income/service.py#L152-238]
- [Source: src/work_data_hub/infrastructure/validation/report_generator.py#L42-95]

## Dev Agent Record

### Context Reference

<!-- Path(s) to story context XML will be added here by context workflow -->

### Agent Model Used

Claude Opus 4.5 (claude-opus-4-5-20251101)

### Debug Log References

### Completion Notes List

- ✅ Added failed records export to `annuity_performance/service.py` using existing `export_error_csv()` infrastructure
- ✅ Added failed records export to `annuity_income/service.py` with domain-specific key field (`计划号`)
- ✅ Both implementations use ~10 LOC as planned, reusing existing infrastructure
- ✅ Structured logging added with csv_path and count fields
- ✅ No function signature changes - fully backward compatible
- ✅ 10 unit tests added covering export on validation failures and no-export on success
- ✅ All 41 related tests pass (1325 total tests pass, pre-existing failures unrelated to this story)

### Change Log

- 2025-12-06: Story 5.6.1 implementation complete - failed records export for both domains

### File List

- `src/work_data_hub/domain/annuity_performance/service.py` (modified)
- `src/work_data_hub/domain/annuity_income/service.py` (modified)
- `tests/unit/domain/annuity_performance/test_failed_records_export.py` (new)
- `tests/unit/domain/annuity_income/test_income_failed_records_export.py` (new)
