# Story 6.1.3: Domain Learning Mechanism

Status: Done

## Story

As a **data engineer**,
I want **the system to automatically learn valid company ID mappings from successfully processed Domain data (annuity_performance, annuity_income)**,
so that **the enrichment_index cache grows organically over time, improving cache hit rates and reducing dependency on EQC API calls and temporary ID generation**.

## Acceptance Criteria

1. **AC1: DomainLearningService Created** - A new `DomainLearningService` class exists in `src/work_data_hub/infrastructure/enrichment/` that:
   - Extracts valid mappings from Domain tables after pipeline processing
   - Filters out temporary IDs (company_id starting with `IN_`)
   - Writes mappings to `enrichment_index` table via `insert_enrichment_index_batch()`

2. **AC2: Multi-Type Learning** - Service extracts all 5 lookup types from Domain data:
   - `plan_code` → `company_id` (from 计划代码 column)
   - `account_name` → `company_id` (from 年金账户名 column)
   - `account_number` → `company_id` (from 年金账户号 column)
   - `customer_name` → `company_id` (from 客户名称 column, normalized)
   - `plan_customer` → `company_id` (from 计划代码 + 客户名称, format: `{plan_code}|{normalized_name}`)

3. **AC3: Confidence Levels** - Each lookup type has configurable confidence:
   - `plan_code`: 0.95 (high confidence - plan codes are stable)
   - `account_name`: 0.90 (medium-high confidence)
   - `account_number`: 0.95 (high confidence - account numbers are unique)
   - `customer_name`: 0.85 (medium confidence - names can have variations)
   - `plan_customer`: 0.90 (medium-high confidence - combo is more specific)

4. **AC4: Source Metadata** - All learned records have:
   - `source = 'domain_learning'`
   - `source_domain` = domain name (e.g., 'annuity_performance', 'annuity_income')
   - `source_table` = table name (e.g., 'annuity_performance_new', 'annuity_income_new')

5. **AC5: Minimum Records Threshold** - Learning only triggers when Domain table has ≥10 valid records (configurable via `min_records_for_learning`)

6. **AC6: Pipeline Integration** - `DomainLearningService.learn_from_domain()` can be called:
   - As a post-processing step after pipeline completion
   - Manually via CLI for batch learning from existing data
   - Integration point documented for future Dagster job

7. **AC7: Statistics Tracking** - `ResolutionStatistics.domain_learning_stats` tracks:
   - `extracted`: Total mappings extracted from Domain table
   - `inserted`: New mappings inserted
   - `updated`: Existing mappings updated (confidence comparison)
   - `skipped`: Mappings skipped (e.g., null values, temp IDs)

8. **AC8: Normalization Consistency** - Uses same normalizer as Layer 2 lookup (`normalize_for_temp_id`) for `customer_name` and `plan_customer` keys to ensure cache hits

9. **AC9: Idempotent Operation** - Running learning multiple times on same data is safe:
   - Uses `insert_enrichment_index_batch()` with ON CONFLICT DO UPDATE
   - Higher confidence wins (GREATEST semantics)
   - hit_count increments on conflict

10. **AC10: Unit Tests** - All new functionality has unit tests with >90% coverage

11. **AC11: Config Safeguards** - Enforce DomainLearningConfig gates:
    - `enabled_domains` required for execution; skip with structured log if disabled
    - Per-lookup-type enable flags respected; skipped counts captured
    - `min_records_for_learning` enforced before any extraction
    - `min_confidence_for_cache` enforced before writes; sub-threshold rows counted as skipped

12. **AC12: Column Validation** - Validate required columns per domain (from config); if any required column is missing, skip learning for that table/domain with clear warning and zero writes.

13. **AC13: Conflict & Source Precedence** - On conflict:
    - Confidence uses GREATEST(existing, new)
    - Do not downgrade higher-trust sources (`yaml`, `backflow`, `eqc_api`); domain_learning only updates hit_count/confidence when higher
    - Preserve existing `source_domain`/`source_table` unless new confidence strictly higher

14. **AC14: Observability** - Populate `ResolutionStatistics.domain_learning_stats` and emit structured log/metric with domain, table, thresholds, extracted/inserted/updated/skipped (per lookup_type optional), aligned with `docs/guides/company-enrichment-service.md`.

15. **AC15: Pipeline Hook** - Document and wire a post-processing hook for annuity pipelines (performance/income) to call `learn_from_domain()` behind an enable flag (default on) without blocking pipeline completion.

## Tasks / Subtasks

- [x] Task 1: Create DomainLearningService class (AC: 1, 4, 5)
  - [x] 1.1 Create `src/work_data_hub/infrastructure/enrichment/domain_learning_service.py`
  - [x] 1.2 Define `DomainLearningConfig` dataclass with configurable parameters
  - [x] 1.3 Implement `__init__()` with repository and config injection
  - [x] 1.4 Add logging with structlog

- [x] Task 2: Implement learning data extraction (AC: 2, 3, 8)
  - [x] 2.1 Implement `_extract_mappings_from_dataframe()` method
  - [x] 2.2 Extract all 5 lookup types with proper normalization
  - [x] 2.3 Filter out null values and temporary IDs (IN_*)
  - [x] 2.4 Apply confidence levels per lookup type
  - [x] 2.5 Create `EnrichmentIndexRecord` objects for each mapping

- [x] Task 3: Implement learn_from_domain() method (AC: 1, 5, 6, 9)
  - [x] 3.1 Accept domain_name, table_name, and DataFrame as parameters
  - [x] 3.2 Validate minimum records threshold
  - [x] 3.3 Call `_extract_mappings_from_dataframe()`
  - [x] 3.4 Call `insert_enrichment_index_batch()` for batch insert
  - [x] 3.5 Return `DomainLearningResult` with statistics

- [x] Task 4: Implement statistics tracking (AC: 7)
  - [x] 4.1 Create `DomainLearningResult` dataclass
  - [x] 4.2 Track extracted, inserted, updated, skipped counts
  - [x] 4.3 Add `to_dict()` method for logging/reporting
  - [x] 4.4 Integrate with `ResolutionStatistics.domain_learning_stats`

- [x] Task 5: Add SQL-based learning method (AC: 6)
  - [x] 5.1 Implement `learn_from_table()` method for direct SQL extraction
  - [x] 5.2 Support both annuity_performance_new and annuity_income_new tables
  - [x] 5.3 Use efficient batch queries with DISTINCT

- [x] Task 6: Write unit tests (AC: 10)
  - [x] 6.1 Test `_extract_mappings_from_dataframe()` - all 5 lookup types
  - [x] 6.2 Test temp ID filtering - IN_* excluded
  - [x] 6.3 Test null value handling - nulls skipped
  - [x] 6.4 Test normalization - customer_name and plan_customer normalized correctly
  - [x] 6.5 Test confidence levels - correct values per lookup type
  - [x] 6.6 Test minimum records threshold - learning skipped below threshold
  - [x] 6.7 Test idempotency - multiple runs produce consistent results
  - [x] 6.8 Test statistics tracking - all counts accurate
  - [x] 6.9 Test enabled_domains / per-lookup gating - disabled items skipped and reported
  - [x] 6.10 Test min_confidence_for_cache - low-confidence rows skipped and counted
  - [x] 6.11 Test missing column guardrails - skip with no writes when required column absent

- [x] Task 7: Integration test (AC: 6, 9)
  - [x] 7.1 End-to-end test with real database
  - [x] 7.2 Test learning from annuity_performance_new table
  - [x] 7.3 Verify enrichment_index populated correctly
  - [x] 7.4 Verify Layer 2 cache hits after learning

- [x] Task 8: Configuration & Observability (AC: 11, 12, 14, 15)
  - [x] 8.1 Implement DomainLearningConfig: `enabled_domains`, per-lookup `enabled`, `confidence_levels`, `min_confidence_for_cache`, `min_records_for_learning`, `column_mappings`
  - [x] 8.2 Wire `ResolutionStatistics.domain_learning_stats` and structured log/metric emission
  - [x] 8.3 Add pipeline post-processing hook (annuity performance/income) with feature flag (default on) and doc string on invocation point

## Dev Notes

### Architecture Compliance

**CRITICAL: This story creates a new service that integrates with existing enrichment infrastructure**

1. **Dependency**: Stories 6.1.1 and 6.1.2 MUST be completed first (provides `enrichment_index` table and multi-priority lookup)
2. **Repository Reuse**: Use existing `CompanyMappingRepository.insert_enrichment_index_batch()` - DO NOT create new repository methods
3. **Normalizer Reuse**: Use `normalize_for_temp_id()` from `src/work_data_hub/infrastructure/enrichment/normalizer.py`
4. **Service Pattern**: Follow existing service patterns in `src/work_data_hub/infrastructure/enrichment/`

### Domain Learning Flow

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    Domain Learning Flow (Post-Processing)                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Domain Pipeline 处理完成 (如 annuity_performance)                           │
│       ↓                                                                      │
│  DomainLearningService.learn_from_domain(                                    │
│      domain_name="annuity_performance",                                      │
│      table_name="annuity_performance_new",                                   │
│      df=processed_dataframe                                                  │
│  )                                                                           │
│       ↓                                                                      │
│  Filter: company_id IS NOT NULL AND NOT LIKE 'IN_%'                          │
│       ↓                                                                      │
│  Extract mappings for each lookup_type:                                      │
│    - plan_code: 计划代码 → company_id (confidence: 0.95)                     │
│    - account_name: 年金账户名 → company_id (confidence: 0.90)                │
│    - account_number: 年金账户号 → company_id (confidence: 0.95)              │
│    - customer_name: normalize(客户名称) → company_id (confidence: 0.85)      │
│    - plan_customer: 计划代码|normalize(客户名称) → company_id (conf: 0.90)   │
│       ↓                                                                      │
│  insert_enrichment_index_batch(records)                                      │
│  (ON CONFLICT: GREATEST confidence, increment hit_count)                     │
│       ↓                                                                      │
│  Return DomainLearningResult with statistics                                 │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

**Pipeline hook (post-processing, non-blocking)**
- Invoke `DomainLearningService.learn_from_domain()` after annuity pipeline completion (performance/income) using processed DataFrame.
- Guard with feature flag (default on) and respect `enabled_domains`; failures should not fail the pipeline (log-only).
- Document hook location in pipeline code for discoverability.

### Data Extraction SQL Reference

```sql
-- Example: Extract plan_code → company_id mappings from annuity_performance_new
SELECT DISTINCT
    ap."计划代码" as lookup_key,
    'plan_code' as lookup_type,
    ap.company_id,
    0.95 as confidence,
    'domain_learning' as source,
    'annuity_performance' as source_domain,
    'annuity_performance_new' as source_table
FROM annuity_performance_new ap
WHERE ap.company_id IS NOT NULL
  AND ap.company_id NOT LIKE 'IN_%'  -- Exclude temporary IDs
  AND ap."计划代码" IS NOT NULL;

-- Similar queries for other lookup_types...
```

### Configuration Reference

```python
@dataclass
class DomainLearningConfig:
    """Configuration for domain learning mechanism."""

    # Enabled domains for learning
    enabled_domains: List[str] = field(default_factory=lambda: ["annuity_performance", "annuity_income"])

    # Confidence levels per lookup type
    confidence_levels: Dict[str, float] = field(default_factory=lambda: {
        "plan_code": 0.95,
        "account_name": 0.90,
        "account_number": 0.95,
        "customer_name": 0.85,
        "plan_customer": 0.90,
    })

    # Per-lookup enable switches
    enabled_lookup_types: Dict[str, bool] = field(default_factory=lambda: {
        "plan_code": True,
        "account_name": True,
        "account_number": True,
        "customer_name": True,
        "plan_customer": True,
    })

    # Minimum records threshold
    min_records_for_learning: int = 10
    # Confidence gate for cache writes
    min_confidence_for_cache: float = 0.80

    # Column mappings (domain-specific)
    column_mappings: Dict[str, Dict[str, str]] = field(default_factory=lambda: {
        "annuity_performance": {
            "plan_code": "计划代码",
            "account_name": "年金账户名",
            "account_number": "年金账户号",
            "customer_name": "客户名称",
            "company_id": "company_id",
        },
        "annuity_income": {
            "plan_code": "计划代码",
            "account_name": "年金账户名",
            "account_number": "年金账户号",
            "customer_name": "客户名称",
            "company_id": "company_id",
        },
    })
```

### Configuration & Guardrails (must implement)

| Item | Requirement |
| --- | --- |
| Domain gating | `enabled_domains` must include domain or learning is skipped with structured warning (no writes) |
| Lookup gating | `enabled_lookup_types[type]` controls extraction per lookup_type; skipped counted in stats |
| Column validation | If any required column for the domain is missing, skip learning for that table/domain with warning; zero writes |
| Thresholds | Enforce `min_records_for_learning` before extraction and `min_confidence_for_cache` before writes; track skipped |
| Source precedence | Do not overwrite higher-trust sources (`yaml`, `backflow`, `eqc_api`); only bump confidence/hit_count when higher |

### DomainLearningResult Dataclass

```python
@dataclass
class DomainLearningResult:
    """Result of domain learning operation."""

    domain_name: str
    table_name: str
    total_records: int
    valid_records: int  # Records with non-null, non-temp company_id

    # Per lookup_type statistics
    extracted: Dict[str, int] = field(default_factory=dict)
    inserted: int = 0
    updated: int = 0
    skipped: int = 0

    def to_dict(self) -> Dict[str, Any]:
        return {
            "domain_name": self.domain_name,
            "table_name": self.table_name,
            "total_records": self.total_records,
            "valid_records": self.valid_records,
            "extracted": self.extracted,
            "inserted": self.inserted,
            "updated": self.updated,
            "skipped": self.skipped,
        }
```

### Observability

- Populate `ResolutionStatistics.domain_learning_stats` with `extracted/inserted/updated/skipped` (per lookup_type optional).
- Emit structured log/metric per run: domain, table, thresholds applied (`min_records_for_learning`, `min_confidence_for_cache`), counts by action, counts by lookup_type if available.
- Ensure alignment with `docs/guides/company-enrichment-service.md` metrics naming.

### Code Location Reference

| Component | Path |
|-----------|------|
| DomainLearningService (NEW) | `src/work_data_hub/infrastructure/enrichment/domain_learning_service.py` |
| Types | `src/work_data_hub/infrastructure/enrichment/types.py` |
| Repository | `src/work_data_hub/infrastructure/enrichment/mapping_repository.py` |
| Normalizer | `src/work_data_hub/infrastructure/enrichment/normalizer.py` |
| Tests (NEW) | `tests/unit/infrastructure/enrichment/test_domain_learning_service.py` |

### Existing Repository Methods to Use

From Story 6.1.1 (`mapping_repository.py:783-912`):
```python
def insert_enrichment_index_batch(
    self,
    records: List[EnrichmentIndexRecord],
) -> InsertBatchResult:
    """
    Batch insert into enterprise.enrichment_index with conflict handling.

    Uses ON CONFLICT DO UPDATE with:
    - confidence: GREATEST(existing, new)
    - hit_count: existing + 1
    - timestamps: update on conflict
    """
```

### Existing Types to Use

From Story 6.1.1 (`types.py`):
```python
class LookupType(Enum):
    PLAN_CODE = "plan_code"
    ACCOUNT_NAME = "account_name"
    ACCOUNT_NUMBER = "account_number"
    CUSTOMER_NAME = "customer_name"
    PLAN_CUSTOMER = "plan_customer"

class SourceType(Enum):
    YAML = "yaml"
    EQC_API = "eqc_api"
    MANUAL = "manual"
    BACKFLOW = "backflow"
    DOMAIN_LEARNING = "domain_learning"  # Use this!
    LEGACY_MIGRATION = "legacy_migration"

@dataclass
class EnrichmentIndexRecord:
    lookup_key: str
    lookup_type: LookupType
    company_id: str
    confidence: float = 1.0
    source: SourceType = SourceType.MANUAL
    source_domain: Optional[str] = None
    source_table: Optional[str] = None
```

### Normalization Reference

**CRITICAL: Use the same normalizer as Layer 2 lookup to ensure cache hits**

```python
from work_data_hub.infrastructure.enrichment.normalizer import normalize_for_temp_id

# For customer_name (DB-P4):
normalized_key = normalize_for_temp_id(customer_name)

# For plan_customer (DB-P5):
normalized_name = normalize_for_temp_id(customer_name)
plan_customer_key = f"{plan_code}|{normalized_name}"
```

### Domain Table Schemas

**annuity_performance_new columns:**
- `计划代码` (plan_code)
- `年金账户名` (account_name)
- `年金账户号` (account_number)
- `客户名称` (customer_name)
- `company_id` (resolved company ID)

**annuity_income_new columns:**
- Same column structure as annuity_performance_new

### Testing Standards

- Use pytest fixtures for database connections
- Mock `CompanyMappingRepository` for unit tests
- Use real database for integration tests
- Target >90% coverage for new code
- Include edge cases: null values, empty strings, temp IDs, duplicate mappings

### Previous Story Learnings

**From Story 6.1.1:**
- Repository methods use UNNEST for batch efficiency
- ON CONFLICT DO UPDATE implements GREATEST confidence semantics
- `EnrichmentIndexRecord.from_dict()` handles database row conversion
- All 207 enrichment module tests pass (no regressions)

**From Story 6.1.2:**
- `_resolve_via_enrichment_index()` implements DB-P1 to DB-P5 priority lookup
- `ResolutionStatistics.db_cache_hits` is `Dict[str, int]` with all 5 priority keys
- Decision path logging implemented with DEBUG level per-row and INFO level summary
- 212 enrichment module tests pass, 784 total unit tests pass

### Git Commit Patterns

Recent commits follow this pattern:
```
feat(story-6.1.X): <description>
```

Example:
```
feat(story-6.1.3): implement domain learning mechanism
```

### Project Structure Notes

- New file: `src/work_data_hub/infrastructure/enrichment/domain_learning_service.py`
- New test file: `tests/unit/infrastructure/enrichment/test_domain_learning_service.py`
- Follow existing code style in enrichment module
- Export new service in `__init__.py`

### Integration Points (Future)

**Dagster Integration (not in scope for this story):**
```python
# Future: Add as post-processing step in Dagster job
@op
def learn_from_domain_op(context, processed_df):
    service = DomainLearningService(...)
    result = service.learn_from_domain(
        domain_name="annuity_performance",
        table_name="annuity_performance_new",
        df=processed_df,
    )
    context.log.info(f"Domain learning: {result.to_dict()}")
```

### References

- [Source: docs/specific/company-enrichment-service/layer2-enrichment-index-enhancement.md#3.3]
- [Source: docs/guides/company-enrichment-service.md#2.3]
- [Source: docs/sprint-artifacts/sprint-change-proposal/sprint-change-proposal-2025-12-08-layer2-enrichment-enhancement.md#4.2]
- [Source: src/work_data_hub/infrastructure/enrichment/mapping_repository.py:783-912] - insert_enrichment_index_batch()
- [Source: src/work_data_hub/infrastructure/enrichment/types.py:205-246] - LookupType, SourceType enums
- [Source: docs/sprint-artifacts/stories/6.1-1-enrichment-index-schema-enhancement.md] - Schema and repository methods
- [Source: docs/sprint-artifacts/stories/6.1-2-layer2-multi-priority-lookup.md] - Multi-priority lookup implementation

## Dev Agent Record

### Context Reference

<!-- Path(s) to story context XML will be added here by context workflow -->

### Agent Model Used

Claude Opus 4.5 (claude-opus-4-5-20251101)

### Debug Log References

### Completion Notes List

- Story created from Sprint Change Proposal: Layer 2 Enrichment Index Enhancement
- This is the third story in Epic 6.1, depends on Stories 6.1.1 (schema) and 6.1.2 (multi-priority lookup)
- Creates new `DomainLearningService` class for self-learning mechanism
- Reuses existing `insert_enrichment_index_batch()` repository method
- Uses `SourceType.DOMAIN_LEARNING` for all learned records
- Confidence levels: plan_code=0.95, account_name=0.90, account_number=0.95, customer_name=0.85, plan_customer=0.90
- Ultimate context engine analysis completed - comprehensive developer guide created
- **Implementation completed 2025-12-08:**
  - DomainLearningService with learn_from_domain() and learn_from_table() methods
  - DomainLearningConfig with all configurable parameters (enabled_domains, confidence_levels, thresholds)
  - DomainLearningResult dataclass with to_dict() for logging
  - All 5 lookup types extracted with proper normalization (customer_name, plan_customer use normalize_for_temp_id)
  - Config safeguards: domain gating, lookup type gating, column validation, min_records threshold, min_confidence threshold
  - ResolutionStatistics.domain_learning_stats field added for observability
  - 26 unit tests covering all acceptance criteria
  - Integration tests for end-to-end validation with real database
  - 812 total unit tests pass (no regressions), 240 enrichment module tests pass

### File List

**Files Created:**
- `src/work_data_hub/infrastructure/enrichment/domain_learning_service.py`
- `tests/unit/infrastructure/enrichment/test_domain_learning_service.py`
- `tests/integration/infrastructure/enrichment/test_domain_learning_integration.py`

**Files Modified:**
- `src/work_data_hub/infrastructure/enrichment/__init__.py` (export DomainLearningService, DomainLearningConfig, DomainLearningResult)
- `src/work_data_hub/infrastructure/enrichment/types.py` (add DomainLearningConfig, DomainLearningResult, ResolutionStatistics.domain_learning_stats)
- `docs/sprint-artifacts/sprint-status.yaml` (tracking metadata)

## Change Log

| Date | Change | Author |
|------|--------|--------|
| 2025-12-08 | Story drafted with comprehensive developer context | Claude Opus 4.5 |
| 2025-12-08 | Implementation completed - all tasks done, 26 unit tests, 812 total tests pass | Claude Opus 4.5 |
