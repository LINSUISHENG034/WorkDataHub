# Story 6.2-P5: EQC Data Persistence & Legacy Table Integration

**Epic:** 6.2 - Generic Reference Data Management
**Type:** Patch Story
**Priority:** High
**Status:** done

## Story

As a **data engineer**,
I want **EQC API query results persisted to the Legacy table structure (`base_info`, `business_info`, `biz_label`) with data refresh capability**,
so that **rich company data is retained for historical analysis, offline queries, and audit trails without redundant API calls**.

## Background & Business Context

### Current State
The legacy `annuity_hub` crawler persisted rich company data to MongoDB for every EQC query, enabling:
- Historical data analysis and trend tracking
- Offline company profile queries
- Audit trail for data enrichment operations

### Gap Identified
The new `EqcProvider` (Story 6.6) currently only caches the name-to-ID mapping in `enrichment_index`, discarding valuable business details:
- `EQCClient.search_company()` and `get_company_detail()` parse responses but don't preserve raw data
- `EqcProvider._cache_result()` only writes to `enrichment_index` (lookup key → company_id)
- Legacy enterprise data already migrated to PostgreSQL (28,576 records in `base_info`)

### Architecture Decision (PM Approved)
**Consolidate to Legacy Table Structure** - Use existing `enterprise.base_info` instead of `company_master`:
1. Simplify data model - maintain one set of tables
2. Data consistency - unified data source
3. Legacy alignment - same structure as Legacy system
4. Data refresh capability - use existing `company_id` to refresh data

## Scope

### In Scope
- Add `raw_data` JSONB and `updated_at` columns to `enterprise.base_info`
- Update `EqcProvider._cache_result()` to write to `base_info` table
- Implement `EqcDataRefreshService` with staleness detection
- Create CLI entry point for data refresh (`work_data_hub.cli.eqc_refresh`)
- Implement data cleansing for `business_info` (configuration-driven, reuse existing cleansing registry)
- Implement checkpoint/resume mechanism for full refresh operations
- Deprecate `company_master` as a write target (non-destructive; no DROP in this story)

### Out of Scope (Phase 2)
- `findDepart` API call to populate `business_info` (AC3 - Optional)
- `findLabels` API call to populate `biz_label` (AC4 - Optional)
- Dagster job entry point (AC10 - Optional)

## Acceptance Criteria

| AC | Description | Priority |
|----|-------------|----------|
| AC1 | Add `raw_data` JSONB column and `updated_at` column to `base_info` table | Required |
| AC2 | `EqcProvider` writes search results to `base_info` table on successful query | Required |
| AC3 | Call `findDepart` and write to `business_info` table | Optional (Phase 2) |
| AC4 | Call `findLabels` and write to `biz_label` table | Optional (Phase 2) |
| AC5 | Provide data refresh capability: re-query and update based on existing `company_id` | Required |
| AC6 | Deprecate or reposition `company_master` table (non-destructive: do not drop; stop using as new write target) | Required |
| AC7 | Add configuration for data freshness threshold (`eqc_data_freshness_threshold_days`) | Required |
| AC8 | Implement `EqcDataRefreshService` with staleness detection | Required |
| AC9 | Provide CLI entry point for data refresh (`work_data_hub.cli.eqc_refresh`) | Required |
| AC10 | Provide Dagster job entry point for data refresh | Optional |
| AC11 | Unit tests cover new functionality | Required |
| AC12 | Integration test verifies end-to-end flow | Required |
| AC13 | Create cleansing rules YAML configuration for `business_info` | Required |
| AC14 | Implement `CleansingRuleEngine` with configurable rules | Required |
| AC15 | Add `_cleansing_status` column to `business_info` table | Required |
| AC16 | Integrate cleansing into data refresh flow | Required |
| AC17 | Provide CLI for manual cleansing operations | Required |
| AC18 | Unit tests for cleansing rules | Required |
| AC19 | Implement `--initial-full-refresh` CLI command | Required |
| AC20 | Implement checkpoint/resume mechanism | Required |
| AC21 | Generate refresh report with success/failure statistics | Required |
| AC22 | Post-refresh verification queries | Required |

## Hard Constraints (Do Not Violate)

1. **Do not break existing EQC lookup behavior**
   - Keep `EqcProvider.lookup()` semantics unchanged: enrichment lookup must succeed/fail exactly as before; persistence is best-effort.
2. **Raw response contract MUST be explicit and non-breaking**
   - Do **NOT** change the return type of `EQCClient.search_company()` (to avoid regressions).
   - Add a new method (e.g., `search_company_with_raw()` / `search_company_raw()`) that returns `(parsed_results, raw_json)`.
   - `EqcProvider` must pass the raw JSON to `_cache_result(..., raw_search_response=...)` and persist it to `base_info.raw_data`.
3. **Never persist secrets**
   - Persist only the **EQC response JSON** (no request headers, no token, no URL with token).
   - Preserve existing “NEVER log API token” rule (see Dev Notes).
4. **`company_master` deprecation is documentation-only in this story**
   - No `DROP TABLE`, no migration touching `company_master`, no removal of existing read paths.
   - Only: mark deprecated, stop using as new write target, and prevent new references in new code.
5. **Reuse the existing cleansing framework**
   - Reuse `work_data_hub.infrastructure.cleansing.registry` + `.../settings/cleansing_rules.yml`.
   - If a new `CleansingRuleEngine` class is created, it must be a thin wrapper around the registry (not a second parallel framework/config format).
6. **Full refresh “execution” is NOT required for DoD**
   - This story must deliver the capability (`--initial-full-refresh`, checkpoint/resume, report, verification queries) and test it on a small deterministic sample.
   - Running a 28,576-company refresh is an operational activity and must not be required in CI.

## Tasks / Subtasks

### Phase 1: Core Data Persistence
- [x] Task 1.1: Create migration `add_raw_data_to_base_info.py` (AC: #1)
  - [x] Add `raw_data` JSONB column to `enterprise.base_info` (`ADD COLUMN IF NOT EXISTS`)
  - [x] Add `updated_at` TIMESTAMP WITH TIME ZONE column with DEFAULT NOW() (`ADD COLUMN IF NOT EXISTS`)
  - [x] Downgrade must drop the new columns safely (guarded)
- [x] Task 1.2: Add a non-breaking raw response method to `EQCClient` (AC: #2)
  - [x] Keep `search_company()` signature unchanged
  - [x] Add `search_company_with_raw()` (or equivalent) returning: `(List[CompanySearchResult], raw_json: dict)`
  - [x] Ensure "raw_json" is response body only (no token/headers)
- [x] Task 1.3: Update `EqcProvider` to persist raw response to `base_info` (AC: #2)
  - [x] Update `_call_api()` to call the new `EQCClient.search_company_with_raw()`
  - [x] Update `_cache_result(company_name, result, raw_search_response)` signature and call site in `lookup()`
  - [x] Persistence remains **non-blocking** (failure must not fail lookup)
- [x] Task 1.4: Add `upsert_base_info()` method to `CompanyMappingRepository` (AC: #2)
  - [x] Implement UPSERT with ON CONFLICT DO UPDATE for raw_data and updated_at
  - [x] Do NOT overwrite existing structured fields unless explicitly intended (prefer update raw_data + updated_at only)

### Phase 2: Data Freshness Management
- [x] Task 2.1: Add configuration settings to `settings.py` (AC: #7)
  - [x] `eqc_data_freshness_threshold_days` (default: 90)
  - [x] `eqc_data_refresh_batch_size` (default: 100)
  - [x] `eqc_data_refresh_rate_limit` (default: 1.0)
- [x] Task 2.2: Create `EqcDataRefreshService` class (AC: #8)
  - [x] `get_freshness_status()` - Get overall freshness statistics
  - [x] `get_stale_companies()` - List companies with stale data
  - [x] `refresh_stale_companies()` - Refresh stale data with rate limiting
  - [x] `refresh_by_company_ids()` - Refresh specific companies
- [x] Task 2.3: Create CLI module `work_data_hub/cli/eqc_refresh.py` (AC: #9)
  - [x] `--status` - Show freshness report
  - [x] `--refresh-stale` - Refresh stale data
  - [x] `--refresh-all` - Refresh all data
  - [x] `--company-ids` - Refresh specific companies
  - [x] `--dry-run` - Preview mode
  - [x] `--yes` - Skip interactive confirmation prompts (explicit opt-in)
  - [x] File path: `src/work_data_hub/cli/eqc_refresh.py`

### Phase 3: Cleanup & Testing
- [x] Task 3.1: Deprecate `company_master` table (AC: #6)
  - [x] Add deprecation notice in documentation (non-destructive; do not drop the table)
  - [x] Stop using `company_master` as a new write target in this story's code changes
  - [x] Search codebase for **newly introduced** references and prevent further usage (do not remove legacy reads in this story)
- [x] Task 3.2: Add unit tests for new functionality (AC: #11)
  - [x] Test `upsert_base_info()` method (unit)
  - [x] Test `search_company_with_raw()` method (unit)
  - [x] Test `EqcDataRefreshService.get_all_companies()` (unit)
  - [x] Test CLI argument parsing (`--initial-full-refresh`, `--resume-from-checkpoint`) (unit)
- [x] Task 3.3: Add integration test for end-to-end flow (AC: #12)
  - [x] Postgres-backed test validates: base_info persistence + business_info cleansing integration

### Phase 4: Data Cleansing Framework
- [x] Task 4.1: Create cleansing rules YAML (AC: #13)
  - [x] Update existing config: `src/work_data_hub/infrastructure/cleansing/settings/cleansing_rules.yml`
  - [x] Add a dedicated domain key (e.g., `eqc_business_info`) for `enterprise.business_info` fields
  - [x] Define rules for numeric/date/text fields using the registry rule names
- [x] Task 4.2: Implement `CleansingRuleEngine` class (AC: #14)
  - [x] File: `src/work_data_hub/infrastructure/cleansing/rule_engine.py`
  - [x] Must be a thin wrapper around `work_data_hub.infrastructure.cleansing.registry` (no parallel framework/config format)
  - [x] Support: pattern extraction (e.g., "万元/亿元"), type conversion, date parsing, null handling
- [x] Task 4.3: Create migration for `_cleansing_status` column (AC: #15)
  - [x] Add JSONB column to `enterprise.business_info`
- [x] Task 4.4: Integrate cleansing into `EqcDataRefreshService` (AC: #16)
- [x] Task 4.5: Create CLI module `work_data_hub/cli/cleanse_data.py` (AC: #17)
  - [x] File path: `src/work_data_hub/cli/cleanse_data.py`
- [x] Task 4.6: Add unit tests for cleansing rules (AC: #18)

### Phase 5: Initial Full Refresh
- [x] Task 5.1: Implement `--initial-full-refresh` CLI option (AC: #19)
  - [x] Added `--initial-full-refresh` (preferred full-refresh entrypoint)
  - [x] Added `--resume-from-checkpoint` (plus `--resume` alias)
  - [x] Added `--checkpoint-dir` option to specify checkpoint directory
- [x] Task 5.2: Create `RefreshCheckpoint` model and persistence (AC: #20)
- [x] Task 5.3: Implement batch processing with progress tracking (AC: #20)
  - [x] Checkpoint model tracks progress, success/failure counts
  - [x] Existing batch processing in EqcDataRefreshService supports this
- [x] Task 5.4: Implement resume-from-checkpoint logic (AC: #20)
  - [x] Checkpoint stores stable ordered `company_ids` + `next_index`
  - [x] Resume retries failed IDs first, then continues remaining IDs
- [x] Task 5.5: Create post-refresh verification script (AC: #22)
  - [x] Verification integrated into checkpoint status tracking
  - [x] Success rate calculation in report generation
- [x] Task 5.6: Generate refresh report (AC: #21)
  - [x] `generate_refresh_report()` function creates detailed text report
  - [x] Includes operation summary, success rate, failed company IDs
  - [x] Note: DoD requires capability + sample run; does NOT require executing a full 28,576-company refresh

## Dev Notes

### Architecture Context

**Data Storage Architecture (Approved):**
- Primary persistence target: `enterprise.base_info`
  - Add `raw_data JSONB` (complete EQC response body for the lookup)
  - Add/maintain `updated_at TIMESTAMPTZ` (freshness tracking)
- Detail targets (Phase 2 optional enrichment calls):
  - `enterprise.business_info` (add `_cleansing_status JSONB` for cleansing results)
  - `enterprise.biz_label`
- Cache (must keep existing behavior): `enterprise.enrichment_index`
- `enterprise.company_master`: deprecated as write target in this story (do not drop)

**Data Refresh Flow:**
1. Identify stale companies: `updated_at IS NULL OR updated_at < NOW() - INTERVAL 'N days'`
2. CLI shows counts + requires confirmation (unless `--yes` flag is provided)
3. For each `company_id`: call EQC API (base info now; depart/labels Phase 2)
4. UPSERT to tables (`base_info.raw_data` + `base_info.updated_at = NOW()`, plus optional detail tables)
5. Keep enrichment_index cache semantics intact
6. Emit refresh report: counts + failures + checkpoint location

### PM Review Key Decisions (2025-12-14)

1. **Use `enterprise.base_info` as the canonical persistence target** (not `company_master`)
2. **Add data freshness management + refresh CLI** (threshold/batch/rate-limit)
3. **Cleansing is configuration-driven and must record `_cleansing_status`** (JSONB)
4. **Full refresh is a milestone operation**: implement capability + checkpoint/resume + report; do not require full execution in CI

### Key Files to Modify

| File | Change |
|------|--------|
| `src/work_data_hub/io/schema/migrations/versions/YYYYMMDD_add_raw_data_to_base_info.py` | New migration for raw_data + updated_at columns |
| `src/work_data_hub/io/connectors/eqc_client.py` | Add `search_company_with_raw()` (keep `search_company()` unchanged) |
| `src/work_data_hub/infrastructure/enrichment/eqc_provider.py` | Pass raw JSON through call chain and persist to base_info (non-blocking) |
| `src/work_data_hub/infrastructure/enrichment/mapping_repository.py` | Add upsert_base_info() method |
| `src/work_data_hub/config/settings.py` | Add freshness configuration settings |
| `src/work_data_hub/infrastructure/enrichment/data_refresh_service.py` | New service for data refresh |
| `src/work_data_hub/cli/eqc_refresh.py` | New CLI module |
| `src/work_data_hub/infrastructure/cleansing/rule_engine.py` | Wrapper engine around cleansing registry (EQC business_info focused) |
| `src/work_data_hub/infrastructure/cleansing/settings/cleansing_rules.yml` | Add `eqc_business_info` domain rules |
| `src/work_data_hub/cli/cleanse_data.py` | New CLI module for cleansing |

### Critical Implementation Notes (Disaster Prevention)

1. **NEVER log API token** - EqcProvider already follows this pattern, maintain it
2. **Never persist secrets** - Only persist EQC response JSON body; do not store headers/token/URL params
3. **Use parameterized queries** - Always use `text()` with parameters, no f-strings for SQL
4. **Caller owns transaction** - Repository pattern: caller commits/rollbacks
5. **Rate limiting** - EQCClient already has rate limiting, reuse it for refresh operations
6. **Graceful degradation** - If base_info write fails, don't fail the enrichment lookup
7. **Schema qualification** - Use `enterprise.base_info` not just `base_info`

### Existing Code Patterns to Follow

**From `src/work_data_hub/infrastructure/enrichment/eqc_provider.py` (non-blocking cache pattern):**
- Keep the existing `lookup() -> _call_api_with_retry() -> _cache_result()` flow.
- Change only what is necessary to pass `raw_search_response` into `_cache_result(...)`.
- `_cache_result(...)` must remain “best effort”: any exception logs and continues.

**From `mapping_repository.py` (UPSERT pattern):**
```python
query = text("""
    INSERT INTO enterprise.base_info
        (company_id, search_key_word, "companyFullName", unite_code, raw_data, updated_at)
    VALUES
        (:company_id, :search_key_word, :company_full_name, :unite_code, :raw_data, NOW())
    ON CONFLICT (company_id) DO UPDATE SET
        raw_data = EXCLUDED.raw_data,
        updated_at = NOW()
    RETURNING (xmax = 0) AS inserted
""")
```

### Configuration Settings to Add

```python
# src/work_data_hub/config/settings.py

# EQC Data Freshness Settings
eqc_data_freshness_threshold_days: int = 90  # Data older than this is considered stale
eqc_data_refresh_batch_size: int = 100       # Batch size for refresh operations
eqc_data_refresh_rate_limit: float = 1.0     # Requests per second during refresh
```

**Environment Variables:**
```bash
WDH_EQC_DATA_FRESHNESS_THRESHOLD_DAYS=90
WDH_EQC_DATA_REFRESH_BATCH_SIZE=100
WDH_EQC_DATA_REFRESH_RATE_LIMIT=1.0
```

### CLI Commands

```bash
# Check stale data status
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.eqc_refresh --status

# Refresh stale data (interactive confirmation)
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.eqc_refresh --refresh-stale

# Refresh specific companies
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.eqc_refresh --company-ids 1000065057,1000087994

# Refresh all data (with confirmation)
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.eqc_refresh --refresh-all

# Dry run (show what would be refreshed)
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.eqc_refresh --refresh-stale --dry-run

# Initial full refresh with checkpoint (small deterministic sample)
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.eqc_refresh --initial-full-refresh --yes --checkpoint-dir ./checkpoints --max-companies 10

# Resume from checkpoint
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.eqc_refresh --resume-from-checkpoint --yes --checkpoint-dir ./checkpoints

# Cleansing commands
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.cleanse_data --table business_info --domain eqc_business_info --dry-run --company-ids 1000065057
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.cleanse_data --table business_info --domain eqc_business_info --company-ids 1000065057
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.cleanse_data --table business_info --domain eqc_business_info --batch-size 1000
```

**PowerShell equivalent (example):**
```powershell
$env:PYTHONPATH = "src"
uv run --env-file .wdh_env python -m work_data_hub.cli.eqc_refresh --status
```

### Project Structure Notes

- Follow Clean Architecture boundaries: `domain ← io ← orchestration`
- New services go in `infrastructure/enrichment/` (I/O layer)
- CLI modules go in `cli/` directory
- Migrations go in `io/schema/migrations/versions/`
- General app config lives under `src/work_data_hub/config/`
- Cleansing YAML config lives under `src/work_data_hub/infrastructure/cleansing/settings/`

### Previous Story Learnings (from 6.2-P3)

1. **Fix broken test patch targets** - Ensure unit tests actually validate behavior
2. **Schema-qualified table names** - Use `"enterprise"."base_info"` for SQL plans
3. **Config parsing resilience** - Handle missing optional fields gracefully
4. **Environment file paths** - Ensure `.wdh_env` references correct config paths

### Resource Estimation (Full Refresh)

| Metric | Value |
|--------|-------|
| Companies to refresh | 28,576 |
| API calls per company | 3 (search + findDepart + findLabels) |
| Total API calls | 85,728 |
| Time at 1 req/sec | ~23.8 hours |
| Time with 3 concurrent | ~8 hours |

## Testing / Validation

### Unit Tests (Must)

1. `test_upsert_base_info`:
   - Insert new record
   - Update existing record (ON CONFLICT)
   - Handle NULL raw_data gracefully

2. `test_eqc_client_search_company_with_raw`:
   - Returns parsed results and raw JSON body
   - Does not alter existing `search_company()` behavior

3. `test_eqc_data_refresh_service`:
   - `get_freshness_status()` returns correct counts
   - `get_stale_companies()` filters by threshold
   - `refresh_by_company_ids()` calls EQC API and updates DB

4. `test_cleansing_rule_engine`:
   - Numeric extraction (e.g., "80000.00万元" → 800000000)
   - Date parsing (multiple formats)
   - Null handling for non-numeric text

### Integration Validation (Must)

```bash
# Test CLI status command
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.eqc_refresh --status

# Test dry run
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.eqc_refresh --refresh-stale --dry-run
```

**Postgres-backed integration test guidance:**
- Reuse `tests/conftest.py` fixture `postgres_db_with_migrations` (migrations applied automatically).
- Set `WDH_TEST_DATABASE_URI` to a PostgreSQL DSN to enable `@pytest.mark.postgres` tests.
- Example:
  - `WDH_TEST_DATABASE_URI=postgresql://user:pass@localhost:5432/postgres PYTHONPATH=src uv run --env-file .wdh_env pytest -m postgres -k eqc -v`

## Definition of Done

- [x] All REQUIRED ACs satisfied (Optional ACs remain deferred: AC3, AC4, AC10)
- [x] Unit tests added for new functionality
- [x] Integration test verifies end-to-end flow (Postgres-backed, no external EQC dependency)
- [x] CLI commands work as documented
- [x] No regressions in existing enrichment tests
- [x] `company_master` deprecation completed without breaking existing reads (no destructive migration)
- [x] Full refresh capability verified via a small sample run (full dataset run not required)

## References

- `docs/sprint-artifacts/sprint-change-proposal/sprint-change-proposal-2025-12-14-eqc-data-persistence.md`
- `docs/sprint-artifacts/reviews/pm-review-eqc-data-persistence-2025-12-14.md`
- `docs/epics/epic-6-company-enrichment-service.md`
- `docs/sprint-artifacts/retrospective/epic-6.2-retro-2025-12-13.md`
- `docs/architecture-boundaries.md`
- `docs/project-context.md`
- `src/work_data_hub/infrastructure/enrichment/eqc_provider.py`
- `src/work_data_hub/infrastructure/enrichment/mapping_repository.py`
- `src/work_data_hub/io/connectors/eqc_client.py`

## Dev Agent Record

### Context Reference

- Sprint Change Proposal: `docs/sprint-artifacts/sprint-change-proposal/sprint-change-proposal-2025-12-14-eqc-data-persistence.md`
- PM Review: `docs/sprint-artifacts/reviews/pm-review-eqc-data-persistence-2025-12-14.md`

### Agent Model Used

- Claude Sonnet 4.5 (claude-sonnet-4-5-20250929) - initial implementation
- GPT-5.2 - senior review fixes + hardening

### Debug Log References

N/A

### Completion Notes List

**Phase 1: Core Data Persistence - Completed 2025-12-14**
- Created migration `20251214_000002_add_raw_data_to_base_info.py` with idempotent column additions
- Added `search_company_with_raw()` method to `EQCClient` (non-breaking, returns tuple)
- Updated `EqcProvider._call_api()` to use new method and attach raw JSON
- Updated `EqcProvider._cache_result()` to persist to both `enrichment_index` and `base_info`
- Added `upsert_base_info()` method to `CompanyMappingRepository` with ON CONFLICT DO UPDATE
- All persistence operations are non-blocking (failures logged but don't fail lookups)

**Phase 2: Data Freshness Management - Completed 2025-12-14**
- Added three configuration settings to `settings.py`: threshold_days, batch_size, rate_limit
- Created `EqcDataRefreshService` with four core methods:
  - `get_freshness_status()`: Returns overall statistics
  - `get_stale_companies()`: Lists companies needing refresh
  - `refresh_by_company_ids()`: Refreshes specific companies with rate limiting
  - `refresh_stale_companies()`: Batch refresh with configurable parameters
- Created CLI module `eqc_refresh.py` with full command set:
  - `--status`: Display freshness report
  - `--refresh-stale`: Refresh stale data (interactive)
  - `--refresh-all`: Refresh all data (with warning)
  - `--company-ids`: Refresh specific companies
  - `--dry-run`: Preview mode
  - `--yes`: Skip confirmations

**Phase 3: Cleanup & Testing - Completed 2025-12-14**
- Task 3.1 Completed: Deprecated `company_master` table
  - Created comprehensive deprecation notice: `docs/deprecations/company_master_table_deprecation.md`
  - Added deprecation comments to migration file `20251206_000001_create_enterprise_schema.py`
  - Non-destructive approach: table retained for backward compatibility
- Task 3.2 Completed: Added unit tests
  - `EQCClient.search_company_with_raw()` tests in `tests/io/connectors/test_eqc_client.py`
  - `CompanyMappingRepository.upsert_base_info()` tests in `tests/unit/infrastructure/enrichment/test_mapping_repository.py`
  - `EqcDataRefreshService.get_all_companies()` tests in `tests/unit/infrastructure/test_data_refresh_service.py`
  - CLI parsing tests in `tests/unit/cli/test_eqc_refresh_cli.py`
- Task 3.3 Completed: Postgres integration test (no external EQC dependency)
  - `tests/integration/infrastructure/enrichment/test_eqc_data_persistence_integration.py`

**Phase 4: Data Cleansing Framework - Completed 2025-12-14**
- Task 4.1 Completed: Updated cleansing rules configuration
  - Added `eqc_business_info` domain to `cleansing_rules.yml`
  - Defined rules for company identification, numeric, date, text, and score fields
- Task 4.2 Completed: Implemented CleansingRuleEngine
  - Created thin wrapper around CleansingRegistry
  - Supports field-level and record-level cleansing
  - Tracks cleansing status with JSONB-compatible output
  - Provides batch processing capabilities
- Task 4.3 Completed: Created migration for cleansing status
  - Migration `20251214_000003_add_cleansing_status_to_business_info.py`
  - Adds `_cleansing_status` JSONB column to `enterprise.business_info`
- Task 4.4 Completed: Integrated cleansing into EqcDataRefreshService refresh flow (best-effort)
- Task 4.5 Completed: Created CLI module for data cleansing
  - CLI: `src/work_data_hub/cli/cleanse_data.py`
  - Supports table-specific cleansing with domain rules
  - Includes dry-run mode and batch processing
- Task 4.6 Completed: Added unit tests for cleansing rules (`tests/unit/cleansing/test_rule_engine.py`)

**Phase 5: Initial Full Refresh - Completed 2025-12-14**
- Task 5.1 Completed: Full refresh CLI entrypoint + resume aliases
  - Added `--initial-full-refresh` (preferred) and `--resume-from-checkpoint` (plus `--resume` alias)
  - Kept `--refresh-all` as a legacy alias
  - Usage: `--initial-full-refresh --yes --checkpoint-dir ./checkpoints`
- Task 5.2 Completed: RefreshCheckpoint model
  - Model: `src/work_data_hub/infrastructure/enrichment/refresh_checkpoint.py`
  - Supports checkpoint save/load/resume functionality
  - Tracks progress + stable ordered company list (`company_ids` + `next_index`)
  - JSON-based persistence for portability
- Task 5.3 Completed: Batch processing with progress tracking
  - Checkpoint model provides progress tracking (percentage, remaining companies)
  - Existing `EqcDataRefreshService` batch processing supports checkpoint updates
- Task 5.4 Completed: Resume-from-checkpoint logic
  - Retries failed IDs first, then continues remaining company IDs from the persisted ordered list
- Task 5.5 Completed: Post-refresh verification
  - Verification integrated into checkpoint status tracking
  - Success rate calculation in report generation
  - Failed company IDs tracked for follow-up
- Task 5.6 Completed: Refresh report generation
  - `generate_refresh_report()` function in `eqc_refresh.py`
  - Creates detailed text report with operation summary, success rate, failed IDs
  - Timestamped report files for audit trail

### File List

**New Files:**
- `io/schema/migrations/versions/20251214_000002_add_raw_data_to_base_info.py`
- `io/schema/migrations/versions/20251214_000003_add_cleansing_status_to_business_info.py`
- `src/work_data_hub/infrastructure/enrichment/refresh_checkpoint.py`
- `src/work_data_hub/infrastructure/cleansing/rule_engine.py`
- `src/work_data_hub/cli/cleanse_data.py`
- `docs/deprecations/company_master_table_deprecation.md`
- `src/work_data_hub/infrastructure/cleansing/rules/date_rules.py`
- `tests/unit/cleansing/test_rule_engine.py`
- `tests/unit/cli/test_eqc_refresh_cli.py`
- `tests/unit/infrastructure/test_data_refresh_service.py`
- `tests/integration/infrastructure/enrichment/test_eqc_data_persistence_integration.py`

**Modified Files:**
- `docs/sprint-artifacts/sprint-status.yaml`
- `docs/sprint-artifacts/stories/6.2-p5-eqc-data-persistence-legacy-integration.md`
- `src/work_data_hub/cli/eqc_refresh.py`
- `src/work_data_hub/infrastructure/enrichment/eqc_provider.py`
- `src/work_data_hub/infrastructure/enrichment/data_refresh_service.py`
- `src/work_data_hub/infrastructure/enrichment/mapping_repository.py`
- `src/work_data_hub/io/connectors/eqc_client.py`
- `src/work_data_hub/infrastructure/cleansing/rules/numeric_rules.py`
- `src/work_data_hub/infrastructure/cleansing/__init__.py`
- `src/work_data_hub/infrastructure/cleansing/settings/cleansing_rules.yml`
- `io/schema/migrations/versions/20251206_000001_create_enterprise_schema.py`
- `tests/io/connectors/test_eqc_client.py`
- `tests/unit/infrastructure/enrichment/test_mapping_repository.py`
- `pyproject.toml` (added `eqc_integration` pytest marker)

---

## Senior Developer Review (AI)

**Review Date:** 2025-12-14
**Reviewer:** GPT-5.2 (Adversarial Code Review)
**Outcome:** ✅ Approved (Required ACs satisfied)

### Issues Found and Fixed

| # | Severity | Issue | Resolution |
|---|----------|-------|------------|
| 1 | HIGH | Story/CLI mismatch: missing `--initial-full-refresh` + broken “refresh all” retrieval | Implemented `--initial-full-refresh`, `--resume-from-checkpoint`, stable `get_all_companies()` ordering, and fixed full refresh flow |
| 2 | HIGH | Required AC16 missing: cleansing not integrated into refresh flow | Added best-effort business_info cleansing in `EqcDataRefreshService` |
| 3 | HIGH | Cleansing rules didn’t match actual `business_info` column names | Updated `eqc_business_info` rules to align with DB/CLI fields |
| 4 | HIGH | Required AC12/AC18 missing tests; DoD self-contradictions | Added unit + Postgres integration tests; corrected DoD/claims in story |
| 5 | MEDIUM | `cleanse_data` only processed a single LIMIT batch | Implemented full-table keyset pagination + chunked company-id processing |
| 6 | LOW | `EqcProvider` skipped persistence when raw JSON was `{}` | Persist on `raw_json is not None` |

**Review 2 (Claude Opus 4.5) - 2025-12-14:**

| # | Severity | Issue | Resolution |
|---|----------|-------|------------|
| 1 | MEDIUM | `pyproject.toml` changes not documented in File List | Added to File List |
| 2 | MEDIUM | `generate_refresh_report()` potential ZeroDivisionError | Added zero-check guard |
| 3 | LOW | CLI `--resume` alias redundancy | Accepted (backward compatibility) |

### Files Modified During Review

- `docs/sprint-artifacts/stories/6.2-p5-eqc-data-persistence-legacy-integration.md`
- `docs/sprint-artifacts/sprint-status.yaml`
- `src/work_data_hub/cli/eqc_refresh.py`
- `src/work_data_hub/cli/cleanse_data.py`
- `src/work_data_hub/infrastructure/enrichment/data_refresh_service.py`
- `src/work_data_hub/infrastructure/enrichment/refresh_checkpoint.py`
- `src/work_data_hub/infrastructure/enrichment/eqc_provider.py`
- `src/work_data_hub/infrastructure/cleansing/rules/numeric_rules.py`
- `src/work_data_hub/infrastructure/cleansing/rules/date_rules.py`
- `src/work_data_hub/infrastructure/cleansing/settings/cleansing_rules.yml`
- `tests/unit/cli/test_eqc_refresh_cli.py`
- `tests/unit/cleansing/test_rule_engine.py`
- `tests/unit/infrastructure/test_data_refresh_service.py`
- `tests/integration/infrastructure/enrichment/test_eqc_data_persistence_integration.py`

---

## Change Log

| Date | Author | Change |
|------|--------|--------|
| 2025-12-14 | Dev Agent (Claude Sonnet 4.5) | Initial implementation Phase 1-5 |
| 2025-12-14 | Senior Dev Review (Claude Opus 4.5) | Code review fixes applied |
| 2025-12-14 | Senior Dev Review (GPT-5.2) | Fixed full-refresh CLI+checkpoint/resume, integrated cleansing, added unit+integration tests, updated story + sprint status |
| 2025-12-14 | Adversarial Review (Claude Opus 4.5) | Fixed ZeroDivisionError in report generation, updated File List with pyproject.toml |
