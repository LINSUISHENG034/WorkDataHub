# Story 6.2-P5: EQC Data Persistence & Legacy Table Integration

**Epic:** 6.2 - Generic Reference Data Management
**Type:** Patch Story
**Priority:** High
**Status:** ready-for-dev

## Story

As a **data engineer**,
I want **EQC API query results persisted to the Legacy table structure (`base_info`, `business_info`, `biz_label`) with data refresh capability**,
so that **rich company data is retained for historical analysis, offline queries, and audit trails without redundant API calls**.

## Background & Business Context

### Current State
The legacy `annuity_hub` crawler persisted rich company data to MongoDB for every EQC query, enabling:
- Historical data analysis and trend tracking
- Offline company profile queries
- Audit trail for data enrichment operations

### Gap Identified
The new `EqcProvider` (Story 6.6) currently only caches the name-to-ID mapping in `enrichment_index`, discarding valuable business details:
- `EQCClient.search_company()` and `get_company_detail()` parse responses but don't preserve raw data
- `EqcProvider._cache_result()` only writes to `enrichment_index` (lookup key → company_id)
- Legacy enterprise data already migrated to PostgreSQL (28,576 records in `base_info`)

### Architecture Decision (PM Approved)
**Consolidate to Legacy Table Structure** - Use existing `enterprise.base_info` instead of `company_master`:
1. Simplify data model - maintain one set of tables
2. Data consistency - unified data source
3. Legacy alignment - same structure as Legacy system
4. Data refresh capability - use existing `company_id` to refresh data

## Scope

### In Scope
- Add `raw_data` JSONB and `updated_at` columns to `enterprise.base_info`
- Update `EqcProvider._cache_result()` to write to `base_info` table
- Implement `EqcDataRefreshService` with staleness detection
- Create CLI entry point for data refresh (`work_data_hub.cli.eqc_refresh`)
- Implement data cleansing for `business_info` (configuration-driven, reuse existing cleansing registry)
- Implement checkpoint/resume mechanism for full refresh operations
- Deprecate `company_master` as a write target (non-destructive; no DROP in this story)

### Out of Scope (Phase 2)
- `findDepart` API call to populate `business_info` (AC3 - Optional)
- `findLabels` API call to populate `biz_label` (AC4 - Optional)
- Dagster job entry point (AC10 - Optional)

## Acceptance Criteria

| AC | Description | Priority |
|----|-------------|----------|
| AC1 | Add `raw_data` JSONB column and `updated_at` column to `base_info` table | Required |
| AC2 | `EqcProvider` writes search results to `base_info` table on successful query | Required |
| AC3 | Call `findDepart` and write to `business_info` table | Optional (Phase 2) |
| AC4 | Call `findLabels` and write to `biz_label` table | Optional (Phase 2) |
| AC5 | Provide data refresh capability: re-query and update based on existing `company_id` | Required |
| AC6 | Deprecate or reposition `company_master` table (non-destructive: do not drop; stop using as new write target) | Required |
| AC7 | Add configuration for data freshness threshold (`eqc_data_freshness_threshold_days`) | Required |
| AC8 | Implement `EqcDataRefreshService` with staleness detection | Required |
| AC9 | Provide CLI entry point for data refresh (`work_data_hub.cli.eqc_refresh`) | Required |
| AC10 | Provide Dagster job entry point for data refresh | Optional |
| AC11 | Unit tests cover new functionality | Required |
| AC12 | Integration test verifies end-to-end flow | Required |
| AC13 | Create cleansing rules YAML configuration for `business_info` | Required |
| AC14 | Implement `CleansingRuleEngine` with configurable rules | Required |
| AC15 | Add `_cleansing_status` column to `business_info` table | Required |
| AC16 | Integrate cleansing into data refresh flow | Required |
| AC17 | Provide CLI for manual cleansing operations | Required |
| AC18 | Unit tests for cleansing rules | Required |
| AC19 | Implement `--initial-full-refresh` CLI command | Required |
| AC20 | Implement checkpoint/resume mechanism | Required |
| AC21 | Generate refresh report with success/failure statistics | Required |
| AC22 | Post-refresh verification queries | Required |

## Hard Constraints (Do Not Violate)

1. **Do not break existing EQC lookup behavior**
   - Keep `EqcProvider.lookup()` semantics unchanged: enrichment lookup must succeed/fail exactly as before; persistence is best-effort.
2. **Raw response contract MUST be explicit and non-breaking**
   - Do **NOT** change the return type of `EQCClient.search_company()` (to avoid regressions).
   - Add a new method (e.g., `search_company_with_raw()` / `search_company_raw()`) that returns `(parsed_results, raw_json)`.
   - `EqcProvider` must pass the raw JSON to `_cache_result(..., raw_search_response=...)` and persist it to `base_info.raw_data`.
3. **Never persist secrets**
   - Persist only the **EQC response JSON** (no request headers, no token, no URL with token).
   - Preserve existing “NEVER log API token” rule (see Dev Notes).
4. **`company_master` deprecation is documentation-only in this story**
   - No `DROP TABLE`, no migration touching `company_master`, no removal of existing read paths.
   - Only: mark deprecated, stop using as new write target, and prevent new references in new code.
5. **Reuse the existing cleansing framework**
   - Reuse `work_data_hub.infrastructure.cleansing.registry` + `.../settings/cleansing_rules.yml`.
   - If a new `CleansingRuleEngine` class is created, it must be a thin wrapper around the registry (not a second parallel framework/config format).
6. **Full refresh “execution” is NOT required for DoD**
   - This story must deliver the capability (`--initial-full-refresh`, checkpoint/resume, report, verification queries) and test it on a small deterministic sample.
   - Running a 28,576-company refresh is an operational activity and must not be required in CI.

## Tasks / Subtasks

### Phase 1: Core Data Persistence
- [x] Task 1.1: Create migration `add_raw_data_to_base_info.py` (AC: #1)
  - [x] Add `raw_data` JSONB column to `enterprise.base_info` (`ADD COLUMN IF NOT EXISTS`)
  - [x] Add `updated_at` TIMESTAMP WITH TIME ZONE column with DEFAULT NOW() (`ADD COLUMN IF NOT EXISTS`)
  - [x] Downgrade must drop the new columns safely (guarded)
- [x] Task 1.2: Add a non-breaking raw response method to `EQCClient` (AC: #2)
  - [x] Keep `search_company()` signature unchanged
  - [x] Add `search_company_with_raw()` (or equivalent) returning: `(List[CompanySearchResult], raw_json: dict)`
  - [x] Ensure "raw_json" is response body only (no token/headers)
- [x] Task 1.3: Update `EqcProvider` to persist raw response to `base_info` (AC: #2)
  - [x] Update `_call_api()` to call the new `EQCClient.search_company_with_raw()`
  - [x] Update `_cache_result(company_name, result, raw_search_response)` signature and call site in `lookup()`
  - [x] Persistence remains **non-blocking** (failure must not fail lookup)
- [x] Task 1.4: Add `upsert_base_info()` method to `CompanyMappingRepository` (AC: #2)
  - [x] Implement UPSERT with ON CONFLICT DO UPDATE for raw_data and updated_at
  - [x] Do NOT overwrite existing structured fields unless explicitly intended (prefer update raw_data + updated_at only)

### Phase 2: Data Freshness Management
- [x] Task 2.1: Add configuration settings to `settings.py` (AC: #7)
  - [x] `eqc_data_freshness_threshold_days` (default: 90)
  - [x] `eqc_data_refresh_batch_size` (default: 100)
  - [x] `eqc_data_refresh_rate_limit` (default: 1.0)
- [x] Task 2.2: Create `EqcDataRefreshService` class (AC: #8)
  - [x] `get_freshness_status()` - Get overall freshness statistics
  - [x] `get_stale_companies()` - List companies with stale data
  - [x] `refresh_stale_companies()` - Refresh stale data with rate limiting
  - [x] `refresh_by_company_ids()` - Refresh specific companies
- [x] Task 2.3: Create CLI module `work_data_hub/cli/eqc_refresh.py` (AC: #9)
  - [x] `--status` - Show freshness report
  - [x] `--refresh-stale` - Refresh stale data
  - [x] `--refresh-all` - Refresh all data
  - [x] `--company-ids` - Refresh specific companies
  - [x] `--dry-run` - Preview mode
  - [x] `--yes` - Skip interactive confirmation prompts (explicit opt-in)
  - [x] File path: `src/work_data_hub/cli/eqc_refresh.py`

### Phase 3: Cleanup & Testing
- [ ] Task 3.1: Deprecate `company_master` table (AC: #6)
  - [ ] Add deprecation notice in documentation (non-destructive; do not drop the table)
  - [ ] Stop using `company_master` as a new write target in this story’s code changes
  - [ ] Search codebase for **newly introduced** references and prevent further usage (do not remove legacy reads in this story)
- [ ] Task 3.2: Add unit tests for new functionality (AC: #11)
  - [ ] Test `upsert_base_info()` method
  - [ ] Test `EqcDataRefreshService` methods
  - [ ] Test CLI argument parsing
- [ ] Task 3.3: Add integration test for end-to-end flow (AC: #12)

### Phase 4: Data Cleansing Framework
- [ ] Task 4.1: Create cleansing rules YAML (AC: #13)
  - [ ] Update existing config: `src/work_data_hub/infrastructure/cleansing/settings/cleansing_rules.yml`
  - [ ] Add a dedicated domain key (e.g., `eqc_business_info`) for `enterprise.business_info` fields
  - [ ] Define rules for numeric/date/text fields using the registry rule names
- [ ] Task 4.2: Implement `CleansingRuleEngine` class (AC: #14)
  - [ ] File: `src/work_data_hub/infrastructure/cleansing/rule_engine.py`
  - [ ] Must be a thin wrapper around `work_data_hub.infrastructure.cleansing.registry` (no parallel framework/config format)
  - [ ] Support: pattern extraction (e.g., “万元/亿元”), type conversion, date parsing, null handling
- [ ] Task 4.3: Create migration for `_cleansing_status` column (AC: #15)
  - [ ] Add JSONB column to `enterprise.business_info`
- [ ] Task 4.4: Integrate cleansing into `EqcDataRefreshService` (AC: #16)
- [ ] Task 4.5: Create CLI module `work_data_hub/cli/cleanse_data.py` (AC: #17)
  - [ ] File path: `src/work_data_hub/cli/cleanse_data.py`
- [ ] Task 4.6: Add unit tests for cleansing rules (AC: #18)

### Phase 5: Initial Full Refresh
- [ ] Task 5.1: Implement `--initial-full-refresh` CLI option (AC: #19)
- [ ] Task 5.2: Create `RefreshCheckpoint` model and persistence (AC: #20)
- [ ] Task 5.3: Implement batch processing with progress tracking (AC: #20)
- [ ] Task 5.4: Implement resume-from-checkpoint logic (AC: #20)
- [ ] Task 5.5: Create post-refresh verification script (AC: #22)
- [ ] Task 5.6: Generate refresh report (AC: #21)
  - [ ] Note: DoD requires capability + sample run; does NOT require executing a full 28,576-company refresh

## Dev Notes

### Architecture Context

**Data Storage Architecture (Approved):**
- Primary persistence target: `enterprise.base_info`
  - Add `raw_data JSONB` (complete EQC response body for the lookup)
  - Add/maintain `updated_at TIMESTAMPTZ` (freshness tracking)
- Detail targets (Phase 2 optional enrichment calls):
  - `enterprise.business_info` (add `_cleansing_status JSONB` for cleansing results)
  - `enterprise.biz_label`
- Cache (must keep existing behavior): `enterprise.enrichment_index`
- `enterprise.company_master`: deprecated as write target in this story (do not drop)

**Data Refresh Flow:**
1. Identify stale companies: `updated_at IS NULL OR updated_at < NOW() - INTERVAL 'N days'`
2. CLI shows counts + requires confirmation (unless `--yes` flag is provided)
3. For each `company_id`: call EQC API (base info now; depart/labels Phase 2)
4. UPSERT to tables (`base_info.raw_data` + `base_info.updated_at = NOW()`, plus optional detail tables)
5. Keep enrichment_index cache semantics intact
6. Emit refresh report: counts + failures + checkpoint location

### PM Review Key Decisions (2025-12-14)

1. **Use `enterprise.base_info` as the canonical persistence target** (not `company_master`)
2. **Add data freshness management + refresh CLI** (threshold/batch/rate-limit)
3. **Cleansing is configuration-driven and must record `_cleansing_status`** (JSONB)
4. **Full refresh is a milestone operation**: implement capability + checkpoint/resume + report; do not require full execution in CI

### Key Files to Modify

| File | Change |
|------|--------|
| `src/work_data_hub/io/schema/migrations/versions/YYYYMMDD_add_raw_data_to_base_info.py` | New migration for raw_data + updated_at columns |
| `src/work_data_hub/io/connectors/eqc_client.py` | Add `search_company_with_raw()` (keep `search_company()` unchanged) |
| `src/work_data_hub/infrastructure/enrichment/eqc_provider.py` | Pass raw JSON through call chain and persist to base_info (non-blocking) |
| `src/work_data_hub/infrastructure/enrichment/mapping_repository.py` | Add upsert_base_info() method |
| `src/work_data_hub/config/settings.py` | Add freshness configuration settings |
| `src/work_data_hub/infrastructure/enrichment/data_refresh_service.py` | New service for data refresh |
| `src/work_data_hub/cli/eqc_refresh.py` | New CLI module |
| `src/work_data_hub/infrastructure/cleansing/rule_engine.py` | Wrapper engine around cleansing registry (EQC business_info focused) |
| `src/work_data_hub/infrastructure/cleansing/settings/cleansing_rules.yml` | Add `eqc_business_info` domain rules |
| `src/work_data_hub/cli/cleanse_data.py` | New CLI module for cleansing |

### Critical Implementation Notes (Disaster Prevention)

1. **NEVER log API token** - EqcProvider already follows this pattern, maintain it
2. **Never persist secrets** - Only persist EQC response JSON body; do not store headers/token/URL params
3. **Use parameterized queries** - Always use `text()` with parameters, no f-strings for SQL
4. **Caller owns transaction** - Repository pattern: caller commits/rollbacks
5. **Rate limiting** - EQCClient already has rate limiting, reuse it for refresh operations
6. **Graceful degradation** - If base_info write fails, don't fail the enrichment lookup
7. **Schema qualification** - Use `enterprise.base_info` not just `base_info`

### Existing Code Patterns to Follow

**From `src/work_data_hub/infrastructure/enrichment/eqc_provider.py` (non-blocking cache pattern):**
- Keep the existing `lookup() -> _call_api_with_retry() -> _cache_result()` flow.
- Change only what is necessary to pass `raw_search_response` into `_cache_result(...)`.
- `_cache_result(...)` must remain “best effort”: any exception logs and continues.

**From `mapping_repository.py` (UPSERT pattern):**
```python
query = text("""
    INSERT INTO enterprise.base_info
        (company_id, search_key_word, "companyFullName", unite_code, raw_data, updated_at)
    VALUES
        (:company_id, :search_key_word, :company_full_name, :unite_code, :raw_data, NOW())
    ON CONFLICT (company_id) DO UPDATE SET
        raw_data = EXCLUDED.raw_data,
        updated_at = NOW()
    RETURNING (xmax = 0) AS inserted
""")
```

### Configuration Settings to Add

```python
# src/work_data_hub/config/settings.py

# EQC Data Freshness Settings
eqc_data_freshness_threshold_days: int = 90  # Data older than this is considered stale
eqc_data_refresh_batch_size: int = 100       # Batch size for refresh operations
eqc_data_refresh_rate_limit: float = 1.0     # Requests per second during refresh
```

**Environment Variables:**
```bash
WDH_EQC_DATA_FRESHNESS_THRESHOLD_DAYS=90
WDH_EQC_DATA_REFRESH_BATCH_SIZE=100
WDH_EQC_DATA_REFRESH_RATE_LIMIT=1.0
```

### CLI Commands

```bash
# Check stale data status
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.eqc_refresh --status

# Refresh stale data (interactive confirmation)
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.eqc_refresh --refresh-stale

# Refresh specific companies
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.eqc_refresh --company-ids 1000065057,1000087994

# Refresh all data (with confirmation)
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.eqc_refresh --refresh-all

# Dry run (show what would be refreshed)
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.eqc_refresh --refresh-stale --dry-run

# Initial full refresh with checkpoint
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.eqc_refresh --initial-full-refresh --resume-from-checkpoint

# Cleansing commands
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.cleanse_data --preview --company-id 1000065057
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.cleanse_data --table business_info
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.cleanse_data --table business_info --force
```

**PowerShell equivalent (example):**
```powershell
$env:PYTHONPATH = "src"
uv run --env-file .wdh_env python -m work_data_hub.cli.eqc_refresh --status
```

### Project Structure Notes

- Follow Clean Architecture boundaries: `domain ← io ← orchestration`
- New services go in `infrastructure/enrichment/` (I/O layer)
- CLI modules go in `cli/` directory
- Migrations go in `io/schema/migrations/versions/`
- General app config lives under `src/work_data_hub/config/`
- Cleansing YAML config lives under `src/work_data_hub/infrastructure/cleansing/settings/`

### Previous Story Learnings (from 6.2-P3)

1. **Fix broken test patch targets** - Ensure unit tests actually validate behavior
2. **Schema-qualified table names** - Use `"enterprise"."base_info"` for SQL plans
3. **Config parsing resilience** - Handle missing optional fields gracefully
4. **Environment file paths** - Ensure `.wdh_env` references correct config paths

### Resource Estimation (Full Refresh)

| Metric | Value |
|--------|-------|
| Companies to refresh | 28,576 |
| API calls per company | 3 (search + findDepart + findLabels) |
| Total API calls | 85,728 |
| Time at 1 req/sec | ~23.8 hours |
| Time with 3 concurrent | ~8 hours |

## Testing / Validation

### Unit Tests (Must)

1. `test_upsert_base_info`:
   - Insert new record
   - Update existing record (ON CONFLICT)
   - Handle NULL raw_data gracefully

2. `test_eqc_client_search_company_with_raw`:
   - Returns parsed results and raw JSON body
   - Does not alter existing `search_company()` behavior

3. `test_eqc_data_refresh_service`:
   - `get_freshness_status()` returns correct counts
   - `get_stale_companies()` filters by threshold
   - `refresh_by_company_ids()` calls EQC API and updates DB

4. `test_cleansing_rule_engine`:
   - Numeric extraction (e.g., "80000.00万元" → 800000000)
   - Date parsing (multiple formats)
   - Null handling for non-numeric text

### Integration Validation (Must)

```bash
# Test CLI status command
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.eqc_refresh --status

# Test dry run
PYTHONPATH=src uv run --env-file .wdh_env python -m work_data_hub.cli.eqc_refresh --refresh-stale --dry-run
```

**Postgres-backed integration test guidance:**
- Reuse `tests/conftest.py` fixture `postgres_db_with_migrations` (migrations applied automatically).
- Set `WDH_TEST_DATABASE_URI` to a PostgreSQL DSN to enable `@pytest.mark.postgres` tests.
- Example:
  - `WDH_TEST_DATABASE_URI=postgresql://user:pass@localhost:5432/postgres PYTHONPATH=src uv run --env-file .wdh_env pytest -m postgres -k eqc -v`

## Definition of Done

- [ ] AC1-AC22 all satisfied (Required ACs)
- [ ] Unit tests added for all new functionality
- [ ] Integration test verifies end-to-end flow
- [ ] CLI commands work as documented
- [ ] No regressions in existing enrichment tests
- [ ] `company_master` deprecation completed without breaking existing reads (no destructive migration)
- [ ] Full refresh capability verified via a small sample run (full dataset run not required)

## References

- `docs/sprint-artifacts/sprint-change-proposal/sprint-change-proposal-2025-12-14-eqc-data-persistence.md`
- `docs/sprint-artifacts/reviews/pm-review-eqc-data-persistence-2025-12-14.md`
- `docs/epics/epic-6-company-enrichment-service.md`
- `docs/sprint-artifacts/retrospective/epic-6.2-retro-2025-12-13.md`
- `docs/architecture-boundaries.md`
- `docs/project-context.md`
- `src/work_data_hub/infrastructure/enrichment/eqc_provider.py`
- `src/work_data_hub/infrastructure/enrichment/mapping_repository.py`
- `src/work_data_hub/io/connectors/eqc_client.py`

## Dev Agent Record

### Context Reference

- Sprint Change Proposal: `docs/sprint-artifacts/sprint-change-proposal/sprint-change-proposal-2025-12-14-eqc-data-persistence.md`
- PM Review: `docs/sprint-artifacts/reviews/pm-review-eqc-data-persistence-2025-12-14.md`

### Agent Model Used

Claude Sonnet 4.5 (claude-sonnet-4-5-20250929)

### Debug Log References

N/A

### Completion Notes List

**Phase 1: Core Data Persistence - Completed 2025-12-14**
- Created migration `20251214_000002_add_raw_data_to_base_info.py` with idempotent column additions
- Added `search_company_with_raw()` method to `EQCClient` (non-breaking, returns tuple)
- Updated `EqcProvider._call_api()` to use new method and attach raw JSON
- Updated `EqcProvider._cache_result()` to persist to both `enrichment_index` and `base_info`
- Added `upsert_base_info()` method to `CompanyMappingRepository` with ON CONFLICT DO UPDATE
- All persistence operations are non-blocking (failures logged but don't fail lookups)

**Phase 2: Data Freshness Management - Completed 2025-12-14**
- Added three configuration settings to `settings.py`: threshold_days, batch_size, rate_limit
- Created `EqcDataRefreshService` with four core methods:
  - `get_freshness_status()`: Returns overall statistics
  - `get_stale_companies()`: Lists companies needing refresh
  - `refresh_by_company_ids()`: Refreshes specific companies with rate limiting
  - `refresh_stale_companies()`: Batch refresh with configurable parameters
- Created CLI module `eqc_refresh.py` with full command set:
  - `--status`: Display freshness report
  - `--refresh-stale`: Refresh stale data (interactive)
  - `--refresh-all`: Refresh all data (with warning)
  - `--company-ids`: Refresh specific companies
  - `--dry-run`: Preview mode
  - `--yes`: Skip confirmations

### File List

**New Files:**
- `io/schema/migrations/versions/20251214_000002_add_raw_data_to_base_info.py`
- `src/work_data_hub/infrastructure/enrichment/data_refresh_service.py`
- `src/work_data_hub/cli/eqc_refresh.py`

**Modified Files:**
- `src/work_data_hub/config/settings.py`
- `src/work_data_hub/io/connectors/eqc_client.py`
- `src/work_data_hub/infrastructure/enrichment/eqc_provider.py`
- `src/work_data_hub/infrastructure/enrichment/mapping_repository.py`
