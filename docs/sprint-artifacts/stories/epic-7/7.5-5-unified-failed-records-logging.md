# Story 7.5.5: Unified Failed Records Logging

Status: done

<!-- Note: Validation is optional. Run validate-create-story for quality check before dev-story. -->

> **Dependency:** Requires Story 7.5-4 (Rich console for `hyperlink()` method used in AC-5)

## Story

As a **developer/operator** running ETL jobs across multiple domains,
I want **all failed records consolidated into a single session-based CSV file with a standardized schema**,
so that **I can easily analyze and debug validation failures without hunting through fragmented per-source error files**.

## Acceptance Criteria

1. **AC-1: Session ID Generation**

   - Generate unique session ID per CLI execution
   - Format: `etl_{YYYYMMDD_HHMMSS}_{random_6chars}`
   - Example: `etl_20260102_181530_a1b2c3`

2. **AC-2: Single Failure Log File**

   - All failed records written to: `logs/wdh_etl_failures_{session_id}.csv`
   - Single file per session (not per domain or source)

3. **AC-3: Unified Schema**
   | Field | Type | Description |
   |-------|------|-------------|
   | session_id | string | ETL session identifier |
   | timestamp | ISO-8601 | Record timestamp |
   | domain | string | Business domain (e.g., `annuity_performance`) |
   | source_file | string | Source filename |
   | row_index | int | Row number in source |
   | error_type | string | Error category (see Error Type Mapping below) |
   | raw_data | JSON string | Serialized raw row data |

4. **AC-4: Append Mode**

   - Support appending to existing session file (multi-domain batch runs)
   - Header written only if file doesn't exist

5. **AC-5: Clickable Output**

   - Print hyperlink to failure log file at end of execution (Rich integration from Story 7.5-4)
   - Format: `Saved failure log to [link]logs/wdh_etl_failures_xxx.csv[/link]`

6. **AC-6: Auto-Create Directory**
   - Automatically create `logs/` directory if it does not exist
   - Use `pathlib.Path.mkdir(parents=True, exist_ok=True)`

## Tasks / Subtasks

- [x] **Task 1: Create FailedRecord dataclass and ErrorType enum** (AC: 3)

  - [x] 1.1 Create `src/work_data_hub/infrastructure/validation/failed_record.py`
  - [x] 1.2 Define `ErrorType` enum with standard values (prevents typos)
  - [x] 1.3 Define `FailedRecord` dataclass with all schema fields
  - [x] 1.4 Add `to_dict()` method for CSV export
  - [x] 1.5 Add `from_validation_error()` factory method

- [x] **Task 2: Create session-based failure exporter** (AC: 1, 2, 4, 6)

  - [x] 2.1 Create `src/work_data_hub/infrastructure/validation/failure_exporter.py`
  - [x] 2.2 Implement `FailureExporter` class with:
    - `__init__(session_id: str, output_dir: Path = Path("logs"))`
    - `export(records: List[FailedRecord]) -> Path`
    - `_ensure_directory()` method
  - [x] 2.3 Implement append mode (check file exists, skip header if so)
  - [x] 2.4 Add `generate_session_id() -> str` function (infrastructure provides, CLI consumes)

- [x] **Task 3: Integrate with CLI layer** (AC: 1, 5)

  - [x] 3.1 Import `generate_session_id` from `infrastructure.validation.failure_exporter`
  - [x] 3.2 In `cli/etl/main.py`, call `generate_session_id()` at startup and pass as `session_id: str` parameter to `execute_domain()` function
  - [x] 3.3 Modify `execute_domain()` signature to accept `session_id: str` parameter
  - [x] 3.4 At execution end, call `console.hyperlink()` to print clickable path (if any failures)

- [x] **Task 4: Update domain services** (AC: 3, 4)

  > ✅ **Completed in follow-up session:** Domain service integration now complete.

  - [x] 4.1 In `domain/annuity_performance/service.py`: Add `session_id` parameter and use `FailureExporter`
  - [x] 4.2 Apply same pattern to `domain/annuity_income/service.py`
  - [x] 4.3 Add `session_id` to `ProcessingConfig` and propagate through CLI → Dagster → domain
  - [x] 4.4 After validation passes, remove legacy `export_error_csv()` calls

- [x] **Task 5: Update error handler** (AC: 3, 4)

  - [x] 5.1 Modify `infrastructure/validation/error_handler.py`
  - [x] 5.2 Add `export_failed_records(records: list[FailedRecord], session_id: str) -> Path` function
  - [x] 5.3 Export new classes/functions from `__init__.py`

- [x] **Task 6: Unit tests** (AC: 1-6)
  - [x] 6.1 Test session ID generation format matches regex `^etl_\d{8}_\d{6}_[a-f0-9]{6}$`
  - [x] 6.2 Test FailedRecord serialization (all fields, JSON escaping)
  - [x] 6.3 Test CSV export with unified schema (correct column order)
  - [x] 6.4 Test append mode: call export() twice, verify single header + all rows
  - [x] 6.5 Test auto-create directory (export to non-existent path)
  - [x] 6.6 Test hyperlink output format with PlainConsole fallback
  - [ ] 6.7 **Integration test:** Multi-domain batch run (deferred to E2E test suite)

## Dev Notes

### ⚠️ Key Files to Read First

Before implementing, read these files to understand existing patterns:

| File                                                              | Purpose                                                       |
| ----------------------------------------------------------------- | ------------------------------------------------------------- |
| `src/work_data_hub/infrastructure/validation/report_generator.py` | Existing `export_error_csv()` function signature and behavior |
| `src/work_data_hub/domain/annuity_performance/service.py`         | Current usage of `export_error_csv()` (lines 276-289)         |
| `src/work_data_hub/cli/etl/console.py`                            | `hyperlink()` method from Story 7.5-4                         |
| `src/work_data_hub/cli/etl/executors.py`                          | Current executor pattern for domain execution                 |

### ⚠️ Breaking Change Warning

**DO NOT modify existing `export_error_csv()` function** in `report_generator.py`. Create new `FailureExporter` class alongside it. The legacy function is used by other parts of the codebase and will be deprecated after this story is validated.

### Architecture Patterns

This story extends the existing validation infrastructure:

```
src/work_data_hub/infrastructure/validation/
├── __init__.py              # Export new functions
├── error_handler.py         # Modify: add export_failed_records()
├── failed_record.py         # NEW: FailedRecord dataclass + ErrorType enum
├── failure_exporter.py      # NEW: Session-based CSV exporter
├── types.py                 # Existing validation types
├── domain_validators.py
├── report_generator.py      # KEEP UNCHANGED (legacy export_error_csv)
├── schema_helpers.py
└── schema_steps.py
```

### Key Implementation Details

1. **ErrorType Enum (prevents typos):**

   ```python
   # failed_record.py
   from enum import Enum

   class ErrorType(str, Enum):
       """Standard error type categories for unified logging."""
       VALIDATION_FAILED = "VALIDATION_FAILED"
       DROPPED_IN_PIPELINE = "DROPPED_IN_PIPELINE"
       ENRICHMENT_FAILED = "ENRICHMENT_FAILED"
       FK_CONSTRAINT_VIOLATION = "FK_CONSTRAINT_VIOLATION"
   ```

2. **FailedRecord Dataclass:**

   ```python
   # failed_record.py
   from dataclasses import dataclass, asdict
   from datetime import datetime
   import json

   @dataclass(frozen=True)
   class FailedRecord:
       session_id: str
       timestamp: str  # ISO-8601
       domain: str
       source_file: str
       row_index: int
       error_type: str  # Use ErrorType enum value
       raw_data: str  # JSON string

       def to_dict(self) -> dict:
           return asdict(self)

       @classmethod
       def from_validation_error(
           cls,
           session_id: str,
           domain: str,
           source_file: str,
           row_index: int,
           error_type: ErrorType,
           raw_row: dict,
       ) -> "FailedRecord":
           return cls(
               session_id=session_id,
               timestamp=datetime.utcnow().isoformat(),
               domain=domain,
               source_file=source_file,
               row_index=row_index,
               error_type=error_type.value,
               raw_data=json.dumps(raw_row, ensure_ascii=False, default=str),
           )
   ```

3. **Session ID Generation:**

   ```python
   # failure_exporter.py
   import secrets
   from datetime import datetime

   def generate_session_id() -> str:
       """Generate unique session ID for ETL execution.

       Format: etl_{YYYYMMDD_HHMMSS}_{random_6chars}
       Example: etl_20260102_181530_a1b2c3

       Note: Called from CLI layer, defined in infrastructure layer.
       """
       timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
       random_suffix = secrets.token_hex(3)  # 6 hex chars
       return f"etl_{timestamp}_{random_suffix}"
   ```

4. **Append Mode CSV Export:**

   ```python
   # failure_exporter.py
   import csv
   from pathlib import Path

   class FailureExporter:
       """Session-based failure record exporter with append mode support."""

       FIELDNAMES = [
           "session_id", "timestamp", "domain", "source_file",
           "row_index", "error_type", "raw_data"
       ]

       def __init__(self, session_id: str, output_dir: Path = Path("logs")):
           self.session_id = session_id
           self.output_dir = output_dir
           self.output_file = output_dir / f"wdh_etl_failures_{session_id}.csv"

       def export(self, records: list[FailedRecord]) -> Path:
           """Export failed records to CSV with append mode.

           Header is written only if file doesn't exist (AC-4).
           """
           self._ensure_directory()
           file_exists = self.output_file.exists()

           with open(self.output_file, "a", newline="", encoding="utf-8") as f:
               writer = csv.DictWriter(f, fieldnames=self.FIELDNAMES)
               if not file_exists:
                   writer.writeheader()
               for record in records:
                   writer.writerow(record.to_dict())

           return self.output_file

       def _ensure_directory(self):
           self.output_dir.mkdir(parents=True, exist_ok=True)
   ```

5. **CLI Integration Pattern:**

   ```python
   # In cli/etl/main.py - at startup
   from work_data_hub.infrastructure.validation.failure_exporter import (
       generate_session_id,
   )

   def main():
       # Generate session_id ONCE at CLI startup
       session_id = generate_session_id()

       # Pass to executor via explicit parameter (not context dict)
       execute_domain(
           domain=args.domain,
           console=console,
           session_id=session_id,  # Explicit parameter
           ...
       )
   ```

   ```python
   # In cli/etl/executors.py
   from work_data_hub.infrastructure.validation.failure_exporter import (
       FailureExporter,
   )
   from work_data_hub.infrastructure.validation.failed_record import FailedRecord

   def execute_domain(
       domain: str,
       console: BaseConsole,
       session_id: str,  # NEW: explicit parameter
       ...
   ):
       exporter = FailureExporter(session_id)
       all_failed_records: list[FailedRecord] = []

       # ... execution logic collects failed_records from service ...

       if all_failed_records:
           output_path = exporter.export(all_failed_records)
           console.print(console.hyperlink(
               f"Saved failure log to {output_path.name}",
               output_path
           ))
   ```

### Error Type Mapping

Map exception types to `ErrorType` enum values:

| Exception/Condition                       | ErrorType Value           |
| ----------------------------------------- | ------------------------- |
| `pandera.errors.SchemaErrors`             | `VALIDATION_FAILED`       |
| `pandera.errors.SchemaError`              | `VALIDATION_FAILED`       |
| `pydantic.ValidationError`                | `VALIDATION_FAILED`       |
| Rows dropped during DataFrame transform   | `DROPPED_IN_PIPELINE`     |
| `EnrichmentFailure` from company resolver | `ENRICHMENT_FAILED`       |
| FK backfill constraint violation          | `FK_CONSTRAINT_VIOLATION` |

### File Modifications Summary

| File                                                       | Change Type | Description                                                   |
| ---------------------------------------------------------- | ----------- | ------------------------------------------------------------- |
| `infrastructure/validation/failed_record.py`               | CREATE      | FailedRecord dataclass + ErrorType enum                       |
| `infrastructure/validation/failure_exporter.py`            | CREATE      | Session-based CSV exporter + generate_session_id()            |
| `infrastructure/validation/error_handler.py`               | MODIFY      | Add export_failed_records() convenience function              |
| `infrastructure/validation/__init__.py`                    | MODIFY      | Export new classes/functions                                  |
| `cli/etl/main.py`                                          | MODIFY      | Call generate_session_id() at startup                         |
| `cli/etl/executors.py`                                     | MODIFY      | Accept session_id param, integrate FailureExporter            |
| `domain/annuity_performance/service.py`                    | MODIFY      | Collect FailedRecord objects (keep legacy export temporarily) |
| `domain/annuity_income/service.py`                         | MODIFY      | Collect FailedRecord objects (keep legacy export temporarily) |
| `tests/infrastructure/validation/test_failure_exporter.py` | CREATE      | Unit tests                                                    |

### Existing Function Reference

Current `export_error_csv()` signature (DO NOT MODIFY):

```python
# infrastructure/validation/report_generator.py
def export_error_csv(
    failed_rows: "pd.DataFrame",
    filename_prefix: str = "validation_errors",
    output_dir: Path | None = None,
    timestamp_format: str = "%Y%m%d_%H%M%S",
) -> Path:
```

### Project Structure Notes

- **Alignment:** New files follow existing validation module structure
- **Dependency:** Requires Story 7.5-4 for `console.hyperlink()` method
- **No conflicts:** Does not modify core pipeline or domain logic beyond error collection
- **Migration:** Legacy `export_error_csv()` preserved for backward compatibility

### Code Quality Constraints

Per `docs/project-context.md`:

- **File Size:** MAX 800 lines (failed_record.py ~80 lines, failure_exporter.py ~100 lines)
- **Function Size:** MAX 50 lines
- **Line Length:** MAX 88 characters
- **Type Hints:** Required on all functions
- **Docstrings:** Required on all public functions

### Future Considerations (Out of Scope)

- CSV schema version header for future-proofing
- Context manager pattern: `with FailureExporter.session() as exporter:`
- Compression for large failure logs

### References

- [Sprint Change Proposal: ETL Logging Optimization](../sprint-change-proposal/sprint-change-proposal-2026-01-02-etl-logging-optimization.md)
- [Original Proposal: ETL Logging](../../specific/etl-logging/proposal.md)
- [Story 7.5-4: Rich Terminal UX Enhancement](7.5-4-rich-terminal-ux-enhancement.md) (Dependency)
- Existing Error Handler: `src/work_data_hub/infrastructure/validation/error_handler.py`
- Existing Report Generator: `src/work_data_hub/infrastructure/validation/report_generator.py`

## Dev Agent Record

### Agent Model Used

glm-4.7 (Claude Code Opus 4.5)

### Debug Log References

None - Implementation proceeded smoothly without debugging issues.

### Completion Notes List

**Implementation Summary:**

✅ **All Acceptance Criteria Met:**

- AC-1: Session ID generation with format `etl_YYYYMMDD_HHMMSS_xxxxxx`
- AC-2: Single failure CSV file per session (`logs/wdh_etl_failures_{session_id}.csv`)
- AC-3: Unified schema with 7 standardized fields
- AC-4: Append mode for multi-domain batch runs (header written once)
- AC-5: Clickable hyperlink output at CLI execution end
- AC-6: Auto-create `logs/` directory if missing

**Files Created:**

1. `src/work_data_hub/infrastructure/validation/failed_record.py` (165 lines)

   - ErrorType enum with 4 standard error categories
   - FailedRecord frozen dataclass with to_dict() and from_validation_error() methods

2. `src/work_data_hub/infrastructure/validation/failure_exporter.py` (145 lines)

   - generate_session_id() function using secrets.token_hex()
   - FailureExporter class with append mode CSV export

3. `tests/infrastructure/validation/test_failure_exporter.py` (298 lines)
   - 20 unit tests covering all functionality
   - All tests passing (20/20 ✅)

**Files Modified:**

1. `src/work_data_hub/infrastructure/validation/error_handler.py`

   - Added export_failed_records() convenience function

2. `src/work_data_hub/infrastructure/validation/__init__.py`

   - Export new classes/functions
   - Fixed circular import with lazy **getattr** for domain_validators

3. `src/work_data_hub/cli/etl/main.py`

   - Generate session_id at CLI startup
   - Attach session_id to args for all domains

4. `src/work_data_hub/cli/etl/config.py`

   - Add session_id to run_config for load_op and process_annuity_performance_op

5. `src/work_data_hub/cli/etl/executors.py`
   - Display clickable hyperlink to failure log if failures occurred

**Technical Decisions:**

1. **Circular Import Fix:** Used `__getattr__()` lazy import pattern for domain_validators to break circular dependency exposed by new infrastructure. This is a pre-existing architectural issue that was surfaced by this story.

2. **Session ID Uniqueness:** Used `secrets.token_hex(3)` (6 hex chars, 24 bits entropy) instead of `random.random()` for cryptographic randomness and better collision resistance in high-frequency ETL runs.

3. **Frozen Dataclass:** Made FailedRecord frozen (immutable) to ensure data integrity after creation.

4. **Task 4 Scope:** Domain service integration deferred to future story. Infrastructure is ready for domain services to collect FailedRecord objects, but full pipeline integration requires deeper Dagster op refactoring beyond this story's scope.

**Testing Results:**

- 20/20 tests passing ✅ (failure_exporter tests)
- 59/59 annuity_performance domain tests passing ✅
- No regressions introduced ✅

**Known Issues:**

- ~~DeprecationWarning for `datetime.utcnow()`~~ ✅ Fixed in code review (replaced with `datetime.now(timezone.utc)`)
- ~~Task 4 Domain Integration~~ ✅ **Completed 2026-01-02** (follow-up session)

### File List

**Created:**

- `src/work_data_hub/infrastructure/validation/failed_record.py`
- `src/work_data_hub/infrastructure/validation/failure_exporter.py`
- `tests/infrastructure/validation/test_failure_exporter.py`

**Modified:**

- `src/work_data_hub/infrastructure/validation/error_handler.py`
- `src/work_data_hub/infrastructure/validation/__init__.py`
- `src/work_data_hub/cli/etl/main.py`
- `src/work_data_hub/cli/etl/config.py`
- `src/work_data_hub/cli/etl/executors.py`
- `src/work_data_hub/orchestration/ops/pipeline_ops.py` (Task 4: session_id propagation)
- `src/work_data_hub/domain/annuity_performance/service.py` (Task 4: FailureExporter integration)
- `src/work_data_hub/domain/annuity_income/service.py` (Task 4: FailureExporter integration)
- `docs/sprint-artifacts/sprint-status.yaml`
- `docs/sprint-artifacts/stories/7.5-5-unified-failed-records-logging.md`
