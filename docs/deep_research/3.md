Here is the step-by-step implementation guide for the Strangler Fig pattern, complete with Python pseudocode for a data pipeline scenario.

This guide assumes we are refactoring a single, monolithic script (`legacy_sales_job.py`) that generates a `daily_sales_report.csv`.

-----

### ? Step-by-Step Guide: Strangler Fig for Data Pipelines

#### 1\. ? Identify the Seam

First, identify your target. It must be a distinct, isolatable part of the system.

  * **Target:** The `daily_sales_report.csv` file.
  * **Legacy Code:** `legacy_sales_job.py`
  * **Consumers:** A BI tool and an email script that read `reports/daily_sales_report.csv`.

#### 2\. ?? Create the Fa?ade

Create a new "entry point" script (the Fa?ade). All orchestration (e.g., your cron job) must be updated immediately to call **this script** instead of the old one.

This is the "router" that will let us control the migration.

```python
# new_entrypoint.py
# Your cron job/scheduler now calls this script.

import legacy_sales_job
import logging

def generate_sales_report_facade():
    """
    This is the fa?ade. For now, it just acts as a simple
    pass-through to the old legacy job.
    """
    logging.info("Fa?ade: Running legacy sales job...")
    try:
        # Call the old, legacy code
        legacy_sales_job.run() 
        logging.info("Fa?ade: Legacy job completed.")
    except Exception as e:
        logging.error(f"Fa?ade: Legacy job failed: {e}")
        raise

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    generate_sales_report_facade()
```

#### 3\. ?? Build the New Pipeline

In your new, clean codebase (e.g., `workdatahub_v2`), build the modern, modular, and tested pipeline as discussed in the main report.

  * **New Code:** `workdatahub_v2.pipelines.run_sales_report()`
  * **Critical:** Configure this new pipeline to write its output to a **different location** to avoid conflicts.
  * **New Output:** `reports/daily_sales_report_NEW.csv`

#### 4\. ? Implement Parallel Execution (Shadow Mode)

Now, update the fa?ade to run the new pipeline *in parallel* with the old one. This is "shadow mode." The new pipeline's success or failure has no impact on the official, legacy output.

```python
# new_entrypoint.py (Updated)

import legacy_sales_job
from workdatahub_v2.pipelines import run_sales_report
import reconciliation_util  # We will create this next
import logging

def generate_sales_report_facade():
    """
    Fa?ade (Phase 2): Run both pipelines in parallel.
    The legacy pipeline is still the "source of truth."
    """
    LEGACY_PATH = "reports/daily_sales_report.csv"
    NEW_PATH = "reports/daily_sales_report_NEW.csv"

    # 1. Run Legacy Pipeline (Source of Truth)
    try:
        logging.info("Fa?ade: Running legacy sales job...")
        legacy_sales_job.run(output_path=LEGACY_PATH)
        logging.info("Fa?ade: Legacy job completed.")
    except Exception as e:
        logging.error(f"Fa?ade: Legacy job FAILED: {e}")
        # If the legacy job fails, we stop. We can't reconcile.
        raise

    # 2. Run New Pipeline (Shadow Mode)
    try:
        logging.info("Fa?ade: Running new pipeline in shadow mode...")
        run_sales_report(output_path=NEW_PATH) # New modular code
        logging.info("Fa?ade: New pipeline completed.")
        
        # 3. Reconcile the outputs
        logging.info("Fa?ade: Reconciling outputs...")
        reconciliation_util.compare_outputs(LEGACY_PATH, NEW_PATH)

    except Exception as e:
        # CRITICAL: We log the error but DO NOT raise it.
        # A failure in the new pipeline must not break the 
        # entire system.
        logging.error(f"Fa?ade: NEW pipeline or reconciliation FAILED: {e}")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    generate_sales_report_facade()
```

#### 5\. ?? Implement Reconciliation

This is the most important part. You need a script that compares the two outputs and tells you if they are identical. For CSVs, using Pandas is ideal.

```python
# reconciliation_util.py

import pandas as pd
import pandas.testing as pd_testing
import logging

def compare_outputs(path_old, path_new):
    """
    Compares two data outputs. Logs success or failure.
    """
    try:
        # Load data. Add parameters as needed for your CSVs
        # (e.g., separators, headers).
        df_old = pd.read_csv(path_old)
        df_new = pd.read_csv(path_new)

        # Basic checks
        if df_old.shape != df_new.shape:
            logging.warning(
                f"RECONCILE FAILED: Shape mismatch. "
                f"Old={df_old.shape}, New={df_new.shape}"
            )
            return False

        # Use pandas' built-in testing utility
        # This checks dtypes, column names, and all values.
        # We wrap it in a try/except because it raises an
        # AssertionError on failure.
        pd_testing.assert_frame_equal(df_old, df_new)
        
        logging.info("RECONCILE SUCCESS: Outputs are identical.")
        return True

    except AssertionError as e:
        # This means the data is different
        logging.warning(f"RECONCILE FAILED: Data mismatch. Details: {e}")
        return False
    except Exception as e:
        logging.error(f"RECONCILE FAILED: Error during comparison. {e}")
        return False

```

**Monitor this\!** You run in this parallel mode for several days or weeks. You must not proceed until your reconciliation script logs `RECONCILE SUCCESS` consistently.

#### 6\. ?? "Strangle" (The Cutover)

Once you have 100% confidence, you flip the switch in the fa?ade. This is the "strangulation" step.

```python
# new_entrypoint.py (Final Version)

# import legacy_sales_job  <-- We no longer need this
from workdatahub_v2.pipelines import run_sales_report
# import reconciliation_util <-- No longer needed
import logging

def generate_sales_report_facade():
    """
    Fa?ade (Phase 3): The new pipeline is now the source of truth.
    The legacy code is no longer called.
    """
    # The new pipeline now writes to the OFFICIAL path
    OFFICIAL_PATH = "reports/daily_sales_report.csv"
    
    try:
        logging.info("Fa?ade: Running NEW pipeline as source of truth...")
        run_sales_report(output_path=OFFICIAL_PATH)
        logging.info("Fa?ade: New pipeline completed successfully.")
    except Exception as e:
        logging.error(f"Fa?ade: NEW pipeline FAILED: {e}")
        raise

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    generate_sales_report_facade()
```

#### 7\. ?? Decommission

After the new pipeline has run successfully as the source of truth for a safe period (e.g., one week), you can **delete** the old `legacy_sales_job.py` file and the `reconciliation_util.py` script. The refactoring for this seam is complete.

Now, you go back to Step 1 and pick your next seam.

-----

This incremental process is the safest way to migrate a critical system. It isolates risk, proves correctness with data, and delivers value without a single "Big Bang" cutover.

Would you like to drill down into the data validation aspect, such as creating data "contracts" (e.g., with `pandera`) to test the data *inside* the new pipeline?