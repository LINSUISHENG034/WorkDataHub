Here is the guide to implementing data contracts using `pandera`. This practice is the "glue" that makes a modular, refactored pipeline safe and maintainable.

-----

### ?? Creating Data "Contracts" with `pandera`

The single biggest risk in a data pipeline is not code bugs¡ªit's **bad data**. "Garbage In, Garbage Out" (GIGO) is the primary cause of failed runs and incorrect reports.

A **Data Contract** is a simple, non-negotiable agreement, enforced in code, that defines the *expected shape and rules* of your data. It's like a function signature, but for your DataFrame.

`pandera` is a Python library that lets you define these contracts and validate your pandas DataFrames against them.

#### Why Use `pandera`?

  * **Fail Fast:** You immediately stop the pipeline the *moment* bad data is detected (e.g., a `user_id` column suddenly contains `NULL`s). This prevents corruption from silently flowing downstream.
  * **Documentation:** The schema is a perfect, machine-readable definition of your data.
  * **Safety:** It gives you the confidence to refactor. If your refactored code passes the same data contract, you have high confidence it's correct.

-----

#### 1\. How to Define a Data Contract

You define a `DataFrameSchema` that outlines every column, its data type, and its rules.

Let's imagine our "Bronze" (raw) `orders` data. We need to validate it *before* passing it to our core transformation logic.

```python
# src/workdatahub/core/contracts.py
#
# We define all our contracts in a central file.

import pandera as pa
from pandera.typing import Series

class BronzeOrdersSchema(pa.DataFrameSchema):
    """
    Contract for raw orders data ingested from the source.
    We are strict about what we need for our transformations.
    """
    
    # We expect an 'order_id' column that must be a string 
    # and must be unique.
    order_id: Series[str] = pa.Field(
        nullable=False, 
        unique=True
    )
    
    # 'order_date' must be a datetime. 
    # pandera will attempt to coerce it, and fail if it can't.
    order_date: Series[pa.DateTime] = pa.Field(nullable=False)

    # 'user_id' can be a string, but it *can* have nulls.
    user_id: Series[str] = pa.Field(nullable=True)
    
    # 'amount' must be a float.
    # We add a custom business rule: 'amount' must be positive.
    amount: Series[float] = pa.Field(
        nullable=False,
        check=pa.Check.greater_than_or_equal_to(0, 
            error="Order amount cannot be negative.")
    )

    # 'status' must be a string and must be one of these values.
    # This prevents unexpected values like "pendingg" or "Canceled".
    status: Series[str] = pa.Field(
        nullable=False,
        check=pa.Check.isin(["pending", "shipped", "cancelled"])
    )

    # By default, this schema is "strict". It will fail if
    # extra columns (e.g., "internal_notes") are present.
    # You can set strict=False if you want to allow extra columns.
    class Config:
        strict = True 
        coerce = True # Automatically try to cast types (e.g., "2025-01-01" to datetime)
```

-----

#### 2\. How to Use Data Contracts

You use these contracts at the *boundaries* of your code, especially when data is passed between layers (e.Sg., from `infrastructure` to `core`).

The easiest way is as a decorator on your transformation functions.

```python
# src/workdatahub/core/transformations.py

import pandas as pd
import pandera as pa
from pandera.typing import DataFrame

# Import the contracts we just defined
from .contracts import BronzeOrdersSchema, SilverUserOrdersSchema

# Use the decorator to enforce the contract
# This function will *only* run if 'raw_orders_df'
# successfully passes the BronzeOrdersSchema validation.
@pa.check_schema(BronzeOrdersSchema)
def transform_orders(raw_orders_df: DataFrame) -> DataFrame:
    """
    Applies business logic to raw orders data.
    
    We are 100% confident that 'raw_orders_df' meets our
    contract (e.g., no null amounts, valid statuses).
    """
    
    # Filter for only shipped orders (we know 'status' is clean)
    shipped_orders = raw_orders_df[
        raw_orders_df["status"] == "shipped"
    ].copy()
    
    # Apply some business logic
    shipped_orders["processing_fee"] = shipped_orders["amount"] * 0.05
    
    # ... more logic ...
    
    # Rename and select final columns for the "Silver" layer
    silver_df = shipped_orders[[
        "order_id", 
        "user_id", 
        "processing_fee"
    ]]
    
    return silver_df

# You can also stack decorators to validate the *output*!
# This guarantees your function's output is also correct.
@pa.check_schema(BronzeOrdersSchema, output_schema=SilverUserOrdersSchema)
def transform_orders_with_output_check(
    raw_orders_df: DataFrame
) -> DataFrame:
    """
    This function validates BOTH its input and its output,
    making it exceptionally robust.
    """
    # ... (same logic as above) ...
    silver_df = ...
    return silver_df
```

-----

#### 3\. Where to Use Contracts in Your Pipeline

Use contracts at the "doorways" between your architectural layers:

1.  **Bronze Layer (Ingestion):**

      * **Who:** Your `infrastructure` readers (e.g., `SqlOrderReader`).
      * **What:** After reading from the database/API, validate the raw DataFrame against the `BronzeOrdersSchema`.
      * **Why:** This stops bad source data from *ever* entering your system.

2.  **Core Layer (Transformation):**

      * **Who:** Your `core.transformations` functions.
      * **What:** Use the `@pa.check_schema` decorator to validate the *inputs* and *outputs* of your pure business logic.
      * **Why:** This creates perfect, testable, and robust functions.

3.  **Gold Layer (Loading):**

      * **Who:** Your `infrastructure` writers (e.g., `CsvReportWriter`).
      * **What:** Before writing your final `daily_sales_report.csv`, validate the DataFrame against a `GoldSalesSchema`.
      * **Why:** This is your "promise" to your data consumers. It guarantees the final report is correct, preventing the BI tool from breaking or showing bad data.

This "contract-driven" approach, combined with the Strangler Fig pattern, gives you a robust, testable, and safe migration path that ensures your new system is not just more modular, but more reliable.

-----

We have now covered the "what" (modern architectures), the "how" (refactoring strategies like Strangler Fig), and the "safety" (data contracts with `pandera`).

Are there any other specific areas, such as technology-agnostic patterns (like the Repository pattern), that you would like to drill into with code examples?