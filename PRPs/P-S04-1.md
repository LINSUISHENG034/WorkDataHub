name: "P-S04: MVP End-to-End Company Enrichment Integration"
description: |

## Purpose
Integrate CompanyEnrichmentService into annuity_performance domain processing with configurable enablement, statistics collection, CSV export, and comprehensive end-to-end validation against legacy results to achieve MVP-level production readiness.

## Core Principles
1. **Context is King**: Include ALL necessary documentation, examples, and caveats
2. **Validation Loops**: Provide executable tests/lints the AI can run and fix
3. **Information Dense**: Use keywords and patterns from the codebase
4. **Progressive Success**: Start simple, validate, then enhance
5. **Global rules**: Be sure to follow all rules in CLAUDE.md

---

## Goal
Implement optional company enrichment integration in annuity_performance processing pipeline that can resolve company IDs using internal mappings, external EQC lookups (with budget control), and asynchronous queue processing, while maintaining ≥95% consistency with legacy results and graceful error handling.

## Why
- **Business value**: Unified company ID resolution replacing manual mapping maintenance
- **Integration**: Validates S-001 → S-003 infrastructure with real domain data
- **Problems solved**: Automates company identification for financial reporting with fallback mechanisms
- **Production readiness**: Establishes patterns for enrichment service adoption across other domains

## What
An optional, configurable enhancement to annuity_performance processing that:
- Resolves company IDs using priority-based lookup (internal → EQC → queue → temp ID)
- Collects detailed statistics on resolution sources and performance
- Exports unknown company names to CSV for manual review
- Provides queue consumption job for async processing
- Maintains complete backward compatibility when disabled

### Success Criteria
- [ ] Enrichment integration works with configurable enable/disable
- [ ] Internal mapping hit rate ≥95% consistent with legacy implementation
- [ ] EQC lookups respect budget limits and gracefully degrade
- [ ] Queue processing handles async lookups with proper error recovery
- [ ] CSV export generates actionable unknown names list
- [ ] Performance impact <50% when enrichment enabled
- [ ] Legacy comparison tests validate ≥95% consistency
- [ ] All validation gates pass without manual intervention

## All Needed Context

### Documentation & References
```yaml
# MUST READ - Include these in your context window
- file: src/work_data_hub/domain/company_enrichment/service.py
  why: CompanyEnrichmentService.resolve_company_id() method and patterns

- file: src/work_data_hub/domain/company_enrichment/models.py
  why: CompanyIdResult, ResolutionStatus enum, and validation patterns

- file: src/work_data_hub/domain/annuity_performance/service.py
  why: Current process() function signature and transformation patterns

- file: src/work_data_hub/domain/annuity_performance/models.py
  why: AnnuityPerformanceOut model structure and field mappings

- file: src/work_data_hub/orchestration/ops.py
  why: Config class patterns, dependency injection, and op structure

- file: src/work_data_hub/orchestration/jobs.py
  why: Job composition, CLI argument parsing, and run_config generation

- file: src/work_data_hub/config/settings.py
  why: Existing enrichment configuration fields and WDH_* env pattern

- file: legacy/annuity_hub/data_handler/data_cleaner.py
  why: AnnuityPerformanceCleaner 5-step company_id resolution for comparison

- file: tests/domain/company_enrichment/test_enrichment_service.py
  why: Mock patterns, test structure, and assertion styles

- url: https://docs.pydantic.dev/latest/concepts/models/
  why: Model composition, field validation, and serialization patterns

- url: https://docs.dagster.io/concepts/configuration/config-schema
  why: Config classes, dependency injection, and run_config patterns
```

### Implementation-Facing Research Notes

```yaml
sources:
  - file: src/work_data_hub/domain/company_enrichment/service.py
    section: CompanyEnrichmentService.resolve_company_id()
    why: Core integration API with priority-based resolution flow
    type: codebase

  - file: src/work_data_hub/config/settings.py
    section: company_enrichment_enabled, company_sync_lookup_limit
    why: Existing configuration fields to extend
    type: codebase

  - url: https://docs.pydantic.dev/latest/concepts/models/
    section: Model composition and field validation
    why: EnrichmentStats and ProcessingResultWithEnrichment design
    type: docs

  - url: https://docs.dagster.io/concepts/configuration/config-schema
    section: Config classes and dependency injection
    why: Op configuration patterns for enrichment parameters
    type: docs

tldr:
  - CompanyEnrichmentService already exists with resolve_company_id() method that returns CompanyIdResult
  - Current annuity_performance service has simple process(rows, data_source) → List[AnnuityPerformanceOut] signature
  - Orchestration uses Config classes for parameter injection with run_config pattern
  - Settings already has enrichment config fields (company_enrichment_enabled, company_sync_lookup_limit)
  - Legacy cleaner has 5-step company_id mapping logic to achieve ≥95% consistency with

setup_commands:
  - "psql $WDH_DATABASE__URI -f scripts/create_table/ddl/lookup_requests.sql"
  - "export WDH_COMPANY_ENRICHMENT_ENABLED=1"
  - "export WDH_ENRICHMENT_SYNC_BUDGET=5"

api_decisions:
  - name: process() function signature extension
    choice: Add optional enrichment_service parameter with defaults
    rationale: Maintains backward compatibility while enabling dependency injection

  - name: Return type evolution
    choice: Always return ProcessingResultWithEnrichment, embed List[AnnuityPerformanceOut] as .records field
    rationale: Clean interface, easier statistics access, type-safe

  - name: Configuration injection
    choice: Extend existing LoadConfig pattern with enrichment fields
    rationale: Consistent with existing op configuration patterns

versions:
  - library: pydantic
    constraint: "v2"
    compatibility: Existing models use pydantic v2 patterns

  - library: dagster
    constraint: Current version in pyproject.toml
    compatibility: Op and Config patterns must match existing jobs

pitfalls_and_mitigations:
  - issue: Enrichment service requires DB connection for loader and queue
    mitigation: Follow existing ops.py patterns for database connection management

  - issue: EQC client may fail or timeout during synchronous lookups
    mitigation: Catch exceptions, log warnings, fall back to queueing without failing main flow

  - issue: CSV export needs consistent file path and encoding
    mitigation: Follow existing output patterns, use UTF-8 encoding, unique timestamps

  - issue: Legacy comparison requires identical input preprocessing
    mitigation: Use same Excel reader and raw row format for both legacy and new processing

open_questions:
  - Where do existing jobs output CSV files? (Check data_base_dir usage patterns)
  - Should queue processing job be standalone or integrated into existing job structure?
```

### Current Codebase tree
```bash
src/work_data_hub/
  domain/
    annuity_performance/
      service.py                   # TARGET: Extend process() with enrichment
      models.py                    # TARGET: Add ProcessingResultWithEnrichment
    company_enrichment/
      service.py                   # EXISTS: CompanyEnrichmentService
      models.py                    # EXISTS: CompanyIdResult, ResolutionStatus
      lookup_queue.py              # EXISTS: LookupQueue for async processing
  io/
    loader/
      company_enrichment_loader.py # EXISTS: CompanyEnrichmentLoader
  orchestration/
    ops.py                         # TARGET: Update process_annuity_performance_op
    jobs.py                        # TARGET: Add queue job + CLI args
  config/
    settings.py                    # EXISTS: enrichment config fields
tests/
  domain/
    annuity_performance/           # TARGET: Add enrichment integration tests
    company_enrichment/            # EXISTS: Test patterns to follow
legacy/
  annuity_hub/
    data_handler/
      data_cleaner.py              # REFERENCE: AnnuityPerformanceCleaner logic
```

### Desired Codebase tree with files to be added
```bash
src/work_data_hub/
  domain/
    annuity_performance/
      service.py                   # EXTENDED: process() with enrichment params
      models.py                    # ADDED: EnrichmentStats, ProcessingResultWithEnrichment
      csv_export.py                # ADDED: write_unknowns_csv() utility
  orchestration/
    ops.py                         # EXTENDED: enrichment config in process_annuity_performance_op
    jobs.py                        # ADDED: process_company_lookup_queue job + CLI args
tests/
  domain/
    annuity_performance/
      test_enrichment_integration.py # ADDED: Integration tests with mocks
      test_legacy_comparison.py      # ADDED: E2E legacy vs new comparison
    company_enrichment/
      test_queue_processing_job.py   # ADDED: Queue consumption job tests
  e2e/
    test_enrichment_e2e.py          # ADDED: Full pipeline with enrichment
```

### Known Gotchas & Library Quirks
```python
# CRITICAL: CompanyEnrichmentService requires proper dependency injection
# Pattern: loader = CompanyEnrichmentLoader(connection), queue = LookupQueue(connection)
# service = CompanyEnrichmentService(loader, queue, eqc_client, sync_lookup_budget=budget)

# CRITICAL: resolve_company_id() accepts multiple name fields for lookup priority
# service.resolve_company_id(plan_code=..., customer_name=..., account_name=..., sync_lookup_budget=...)

# CRITICAL: Database connection in ops must be managed like existing load_op pattern
# Use psycopg2.connect(settings.get_database_connection_string()) for plan-only=False

# CRITICAL: CSV export encoding must be UTF-8 for Chinese character support
# Use open(file_path, 'w', encoding='utf-8', newline='') with csv.writer

# CRITICAL: Legacy comparison requires identical column preprocessing
# Both legacy and new must use same Excel reader output and field mappings

# CRITICAL: Enrichment must never fail the main pipeline
# Wrap all enrichment calls in try/except, log errors, continue processing

# CRITICAL: Configuration fields use WDH_ prefix with double underscore for nested
# WDH_COMPANY_ENRICHMENT_ENABLED, WDH_ENRICHMENT_SYNC_BUDGET, WDH_ENRICHMENT_EXPORT_UNKNOWNS
```

## Implementation Blueprint

### Data models and structure

```python
# src/work_data_hub/domain/annuity_performance/models.py - ADD these models

from pydantic import BaseModel, Field
from typing import Optional, List
from datetime import datetime

class EnrichmentStats(BaseModel):
    """Statistics collection for company enrichment operations."""

    total_records: int = 0
    success_internal: int = 0      # Resolved via internal mappings
    success_external: int = 0      # Resolved via EQC lookup + cached
    pending_lookup: int = 0        # Queued for async processing
    temp_assigned: int = 0         # Assigned temporary ID
    failed: int = 0               # Resolution failed completely
    sync_budget_used: int = 0     # EQC lookups consumed from budget
    processing_time_ms: int = 0   # Total enrichment processing time

    def record(self, status: ResolutionStatus, source: Optional[str] = None):
        """Record a resolution result for statistics tracking."""
        self.total_records += 1
        if status == ResolutionStatus.SUCCESS_INTERNAL:
            self.success_internal += 1
        elif status == ResolutionStatus.SUCCESS_EXTERNAL:
            self.success_external += 1
            self.sync_budget_used += 1
        elif status == ResolutionStatus.PENDING_LOOKUP:
            self.pending_lookup += 1
        elif status == ResolutionStatus.TEMP_ASSIGNED:
            self.temp_assigned += 1

class ProcessingResultWithEnrichment(BaseModel):
    """Extended processing result including enrichment statistics and exports."""

    records: List[AnnuityPerformanceOut] = Field(..., description="Processed annuity performance records")
    enrichment_stats: EnrichmentStats = Field(default_factory=EnrichmentStats)
    unknown_names_csv: Optional[str] = Field(None, description="Path to exported unknown names CSV")
    data_source: str = Field("unknown", description="Source file or identifier")
    processing_time_ms: int = Field(0, description="Total processing time")
```

### List of tasks to be completed

```yaml
Task 1: Extend Annuity Performance Models
ADD TO src/work_data_hub/domain/annuity_performance/models.py:
  - ADD EnrichmentStats class with record() method
  - ADD ProcessingResultWithEnrichment class
  - IMPORT ResolutionStatus from company_enrichment.models

CREATE src/work_data_hub/domain/annuity_performance/csv_export.py:
  - ADD write_unknowns_csv() function with UTF-8 encoding
  - FOLLOW existing output directory patterns
  - HANDLE file name uniqueness with timestamps

Task 2: Extend Annuity Performance Service
MODIFY src/work_data_hub/domain/annuity_performance/service.py:
  - EXTEND process() function signature with enrichment parameters
  - ADD enrichment integration loop after row transformation
  - ADD statistics collection and unknown names tracking
  - PRESERVE existing error handling patterns
  - RETURN ProcessingResultWithEnrichment always

Task 3: Extend Orchestration Configuration
MODIFY src/work_data_hub/orchestration/ops.py:
  - ADD enrichment fields to existing config or create new EnrichmentConfig
  - EXTEND process_annuity_performance_op with enrichment setup
  - ADD database connection and service instantiation
  - HANDLE new return type from domain service
  - PRESERVE existing error handling and logging

Task 4: Add Queue Processing Job
ADD TO src/work_data_hub/orchestration/jobs.py:
  - ADD process_company_lookup_queue_job() function
  - CREATE corresponding CLI arguments and handling
  - FOLLOW existing job patterns for database connection
  - ADD comprehensive error handling and statistics reporting

Task 5: Extend CLI Interface
MODIFY src/work_data_hub/orchestration/jobs.py main():
  - ADD --enrichment-enabled, --enrichment-sync-budget, --export-unknown-names arguments
  - EXTEND build_run_config() to include enrichment parameters
  - ADD job selection logic for queue processing
  - PRESERVE existing argument validation patterns

Task 6: Add Integration Tests
CREATE tests/domain/annuity_performance/test_enrichment_integration.py:
  - MOCK CompanyEnrichmentService dependencies
  - TEST enrichment enabled/disabled scenarios
  - TEST error handling and graceful degradation
  - VERIFY statistics collection accuracy

Task 7: Add Legacy Comparison Tests
CREATE tests/domain/annuity_performance/test_legacy_comparison.py:
  - LOAD same test data through legacy and new processing
  - COMPARE company_id resolution results
  - MEASURE and ASSERT ≥95% consistency
  - REPORT detailed discrepancy analysis

Task 8: Add E2E Validation Tests
CREATE tests/e2e/test_enrichment_e2e.py:
  - TEST full pipeline with enrichment enabled
  - VERIFY queue processing job functionality
  - TEST CSV export generation and content
  - MEASURE performance impact (<50% increase)
```

### Per task pseudocode

```python
# Task 2: Extend Annuity Performance Service
def process(
    rows: List[Dict[str, Any]],
    data_source: str = "unknown",
    enrichment_service: Optional[CompanyEnrichmentService] = None,
    sync_lookup_budget: int = 0,
    export_unknown_names: bool = True,
) -> ProcessingResultWithEnrichment:
    """Extended process function with optional enrichment integration."""

    start_time = time.time()
    stats = EnrichmentStats()
    unknown_names: List[str] = []

    # PRESERVE existing transformation logic exactly
    processed_records = []
    for row_index, raw_row in enumerate(rows):
        try:
            # EXISTING: Transform single row to AnnuityPerformanceOut
            record = _transform_single_row(raw_row, data_source, row_index)

            # NEW: Optional enrichment integration
            if enrichment_service and record:
                try:
                    # CRITICAL: Call enrichment service with proper field mapping
                    result = enrichment_service.resolve_company_id(
                        plan_code=raw_row.get("计划代码"),
                        customer_name=raw_row.get("客户名称"),
                        account_name=raw_row.get("年金账户名"),
                        sync_lookup_budget=sync_lookup_budget,
                    )

                    # Update record with resolved company_id
                    if result.company_id:
                        record.company_id = result.company_id

                    # Record statistics
                    stats.record(result.status, result.source)

                    # Collect unknown names for CSV export
                    if not result.company_id and raw_row.get("客户名称"):
                        unknown_names.append(raw_row.get("客户名称"))

                except Exception as e:
                    # CRITICAL: Never fail main pipeline on enrichment errors
                    logger.warning(f"Enrichment failed for row {row_index}: {e}")
                    stats.failed += 1

            processed_records.append(record)

        except Exception as e:
            # PRESERVE existing error handling
            logger.error(f"Row transformation failed: {e}")

    # Generate CSV export if requested and unknowns exist
    csv_path = None
    if export_unknown_names and unknown_names:
        csv_path = write_unknowns_csv(unknown_names, data_source)

    processing_time_ms = int((time.time() - start_time) * 1000)
    stats.processing_time_ms = processing_time_ms

    return ProcessingResultWithEnrichment(
        records=processed_records,
        enrichment_stats=stats,
        unknown_names_csv=csv_path,
        data_source=data_source,
        processing_time_ms=processing_time_ms,
    )

# Task 3: Extend Orchestration Op
@op
def process_annuity_performance_op(
    context: OpExecutionContext,
    config: ProcessingConfig,  # EXTENDED with enrichment fields
    excel_rows: List[Dict[str, Any]],
    file_paths: List[str]
) -> List[Dict[str, Any]]:
    """Extended op with optional enrichment service integration."""

    file_path = file_paths[0] if file_paths else "unknown"

    # NEW: Conditional enrichment service setup
    enrichment_service = None
    if config.enrichment_enabled:
        try:
            # PATTERN: Follow existing database connection pattern
            if not config.plan_only:
                conn = psycopg2.connect(get_settings().get_database_connection_string())
                loader = CompanyEnrichmentLoader(conn)
                queue = LookupQueue(conn)
                eqc_client = EQCClient()  # Uses settings for auth
                enrichment_service = CompanyEnrichmentService(
                    loader=loader,
                    queue=queue,
                    eqc_client=eqc_client,
                    sync_lookup_budget=config.enrichment_sync_budget
                )
        except Exception as e:
            # CRITICAL: Log error but continue without enrichment
            context.log.warning(f"Failed to setup enrichment service: {e}")

    # Call extended domain service
    result = process_annuity(
        excel_rows,
        data_source=file_path,
        enrichment_service=enrichment_service,
        sync_lookup_budget=config.enrichment_sync_budget,
        export_unknown_names=config.export_unknown_names,
    )

    # Log enrichment statistics
    if result.enrichment_stats.total_records > 0:
        context.log.info(
            "Enrichment completed",
            extra={
                "total": result.enrichment_stats.total_records,
                "internal_hits": result.enrichment_stats.success_internal,
                "external_hits": result.enrichment_stats.success_external,
                "pending": result.enrichment_stats.pending_lookup,
                "budget_used": result.enrichment_stats.sync_budget_used,
                "csv_exported": bool(result.unknown_names_csv),
            }
        )

    # Return serialized records for downstream ops
    return [
        record.model_dump(mode="json", by_alias=True, exclude_none=True)
        for record in result.records
    ]
```

### Integration Points
```yaml
DATABASE:
  - prerequisite: "psql $WDH_DATABASE__URI -f scripts/create_table/ddl/lookup_requests.sql"
  - connection: "Follow existing ops.py pattern with psycopg2.connect()"

CONFIG:
  - extend: src/work_data_hub/config/settings.py
  - pattern: "Existing fields: company_enrichment_enabled, company_sync_lookup_limit"
  - new env: "WDH_ENRICHMENT_EXPORT_UNKNOWNS (default True)"

ORCHESTRATION:
  - extend: process_annuity_performance_op config schema
  - add: ProcessingConfig with enrichment_enabled, enrichment_sync_budget, export_unknown_names
  - add: process_company_lookup_queue job for async processing

CLI:
  - extend: jobs.py main() argument parser
  - add: --enrichment-enabled, --enrichment-sync-budget, --export-unknown-names
  - add: --job process_company_lookup_queue for queue consumption
```

## Validation Loop

### Level 1: Syntax & Style
```bash
# Run these FIRST - fix any errors before proceeding
uv run ruff check src/ --fix              # Auto-fix style issues
uv run mypy src/                           # Type checking

# Expected: No errors. If errors, READ and fix.
```

### Level 2: Unit Tests
```python
# tests/domain/annuity_performance/test_enrichment_integration.py
async def test_enrichment_disabled_preserves_baseline():
    """Test that enrichment disabled maintains existing behavior."""
    sample_rows = [{"计划代码": "P0001", "客户名称": "测试公司"}]

    # Without enrichment
    baseline_result = process(sample_rows, "test_source")

    # With enrichment disabled (None service)
    enrichment_result = process(
        sample_rows, "test_source",
        enrichment_service=None,
        sync_lookup_budget=0,
        export_unknown_names=False
    )

    # ASSERT: Identical processed records
    assert enrichment_result.records == baseline_result.records
    assert enrichment_result.enrichment_stats.total_records == 0

async def test_enrichment_enabled_with_budget():
    """Test enrichment with sync budget respects limits."""
    mock_service = Mock(spec=CompanyEnrichmentService)
    mock_service.resolve_company_id.return_value = CompanyIdResult(
        company_id="12345",
        status=ResolutionStatus.SUCCESS_EXTERNAL,
        source="EQC"
    )

    sample_rows = [{"计划代码": "", "客户名称": "测试公司"}]
    result = process(
        sample_rows, "test_source",
        enrichment_service=mock_service,
        sync_lookup_budget=5,
        export_unknown_names=True
    )

    # ASSERT: Service called with correct parameters
    mock_service.resolve_company_id.assert_called_once_with(
        plan_code="",
        customer_name="测试公司",
        account_name=None,
        sync_lookup_budget=5
    )

    # ASSERT: Statistics recorded correctly
    assert result.enrichment_stats.success_external == 1
    assert result.enrichment_stats.sync_budget_used == 1

# tests/domain/annuity_performance/test_legacy_comparison.py
def test_legacy_vs_new_consistency():
    """Compare legacy and new company_id resolution for consistency."""
    # Load same test data
    test_data = load_sample_annuity_data()

    # Process with legacy cleaner
    legacy_cleaner = AnnuityPerformanceCleaner("test_data.xlsx")
    legacy_df = legacy_cleaner._clean_method()
    legacy_company_ids = legacy_df['company_id'].tolist()

    # Process with new enrichment (internal mappings only for fair comparison)
    mock_loader = Mock()
    mock_loader.load_mappings.return_value = load_legacy_mappings()
    mock_service = CompanyEnrichmentService(mock_loader, Mock(), Mock())

    new_result = process(test_data, enrichment_service=mock_service, sync_lookup_budget=0)
    new_company_ids = [record.company_id for record in new_result.records]

    # Calculate consistency
    matches = sum(1 for l, n in zip(legacy_company_ids, new_company_ids) if l == n)
    consistency = matches / len(legacy_company_ids)

    # ASSERT: ≥95% consistency requirement
    assert consistency >= 0.95, f"Consistency {consistency:.2%} below 95% threshold"
```

```bash
# Run tests iteratively until passing:
uv run pytest tests/domain/annuity_performance/ -v
uv run pytest tests/e2e/ -v

# Run legacy comparison specifically:
uv run pytest -v -k "legacy_comparison or enrichment_service or lookup_queue"
```

### Level 3: Integration Test
```bash
# Test enrichment disabled (baseline behavior)
export WDH_COMPANY_ENRICHMENT_ENABLED=0
uv run python -m src.work_data_hub.orchestration.jobs \
  --domain annuity_performance \
  --plan-only --max-files 1 \
  --mode append --debug

# Expected: No enrichment statistics, baseline performance

# Test enrichment enabled with sync budget
export WDH_COMPANY_ENRICHMENT_ENABLED=1
export WDH_ENRICHMENT_SYNC_BUDGET=5
uv run python -m src.work_data_hub.orchestration.jobs \
  --domain annuity_performance \
  --plan-only --max-files 1 \
  --enrichment-enabled \
  --enrichment-sync-budget 5 \
  --export-unknown-names \
  --debug

# Expected: Enrichment statistics logged, CSV export mentioned if unknowns exist

# Test queue processing job
uv run python -m src.work_data_hub.orchestration.jobs \
  --job process_company_lookup_queue \
  --plan-only --debug

# Expected: Queue processing statistics, no errors
```

### Level 4: End-to-End Validation
```bash
# Apply database DDL
psql "$WDH_DATABASE__URI" -f scripts/create_table/ddl/lookup_requests.sql

# Full pipeline with enrichment and execution
export WDH_DATABASE__URI=postgresql://user:pass@host:5432/db
export WDH_COMPANY_ENRICHMENT_ENABLED=1
export WDH_ENRICHMENT_SYNC_BUDGET=5

WDH_DATA_BASE_DIR="tests/fixtures/sample_data/annuity_subsets" \
uv run python -m src.work_data_hub.orchestration.jobs \
  --domain annuity_performance \
  --execute --max-files 1 \
  --enrichment-enabled \
  --enrichment-sync-budget 5 \
  --export-unknown-names \
  --mode append --debug

# Expected:
# - Enrichment statistics show internal/external/pending breakdown
# - CSV export generated if unknown names exist
# - Processing time increase <50% compared to baseline
# - No errors in logs, graceful degradation on service failures

# Queue consumption validation
uv run python -m src.work_data_hub.orchestration.jobs \
  --job process_company_lookup_queue \
  --execute --debug

# Expected: Pending queue requests processed, success/failure counts reported
```

## Final Validation Checklist
- [ ] All tests pass: `uv run pytest tests/ -v`
- [ ] No linting errors: `uv run ruff check src/`
- [ ] No type errors: `uv run mypy src/`
- [ ] Enrichment disabled preserves baseline behavior exactly
- [ ] Enrichment enabled respects budget limits and collects statistics
- [ ] EQC errors gracefully degrade without failing main pipeline
- [ ] Queue processing job handles async lookups correctly
- [ ] CSV export generates UTF-8 encoded file with unknown names
- [ ] Legacy comparison validates ≥95% consistency on test data
- [ ] Performance impact <50% when enrichment enabled
- [ ] CLI arguments correctly flow through run_config to ops
- [ ] Database DDL applied successfully for queue functionality
- [ ] All validation commands from INITIAL.md execute without errors

---

## Anti-Patterns to Avoid
- ❌ Don't modify existing function signatures - extend with optional parameters
- ❌ Don't fail main pipeline on enrichment errors - log and continue
- ❌ Don't skip legacy comparison - it's critical for production confidence
- ❌ Don't hardcode file paths - use existing output directory patterns
- ❌ Don't ignore database connection management - follow existing op patterns
- ❌ Don't bypass budget limits - respect sync_lookup_budget strictly
- ❌ Don't commit sensitive test data - use anonymized samples
- ❌ Don't skip CSV encoding specification - Chinese characters require UTF-8

## Confidence Score: 9/10

High confidence due to:
- Comprehensive codebase analysis and existing pattern identification
- Clear integration points with working CompanyEnrichmentService
- Detailed legacy comparison requirements with specific success criteria
- Well-defined error handling and graceful degradation strategies
- Complete validation pipeline from unit tests to E2E scenarios
- Thorough research of Pydantic v2 and Dagster configuration patterns

Minor uncertainty on CSV output directory conventions, but existing codebase inspection will resolve this quickly.