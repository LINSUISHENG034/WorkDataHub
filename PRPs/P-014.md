name: "Dagster Schedules, Sensors, and Alerting for Trustee Performance Pipeline"
description: |

## Purpose
Operationalize the trustee_performance pipeline by implementing production schedules, file-based trigger sensors, and data quality sensors with alert hooks. Expose these through a Dagster Definitions module for `dagster dev` discovery.

## Core Principles
1. **Context is King**: Leverage existing patterns and configurations
2. **Validation Loops**: Provide executable tests matching existing patterns
3. **Information Dense**: Use established codebase conventions
4. **Progressive Success**: Build incrementally with validation at each step

---

## Goal
Implement M3 Milestone features F-030 (schedules), F-031 (data quality sensors), and C-030 (alerting) by creating production-ready Dagster schedules and sensors that integrate seamlessly with the existing trustee performance pipeline.

## Why
- **Operational Excellence**: Automate daily processing of trustee performance data
- **Data Quality Assurance**: Detect and alert on data processing anomalies
- **Event-Driven Processing**: Trigger jobs when new files arrive
- **Production Readiness**: Enable `dagster dev` discovery and monitoring

## What
A complete orchestration system with:
- Daily schedule running `trustee_performance_multi_file_job` at 02:00 Asia/Shanghai
- File discovery sensor triggering jobs when new trustee files appear
- Data quality sensor alerting when processed data has zero rows
- Definitions module registering all components for Dagster discovery

### Success Criteria
- [ ] Daily schedule executes trustee_performance_multi_file_job with valid run_config
- [ ] File discovery sensor triggers on new files and maintains cursor state
- [ ] Data quality sensor performs lightweight health checks and logs alerts
- [ ] Definitions module enables `dagster dev` discovery without errors
- [ ] All validation gates pass (ruff, mypy, pytest)

## All Needed Context

### Documentation & References
```yaml
# MUST READ - Core Dagster patterns
- url: https://docs.dagster.io/concepts/partitions-schedules-sensors/schedules
  why: Schedule decorator, cron syntax, timezone configuration
  
- url: https://docs.dagster.io/concepts/partitions-schedules-sensors/sensors  
  why: Sensor patterns, cursor management, RunRequest/SkipReason
  
- url: https://docs.dagster.io/deployment/code-locations#definitions
  why: Definitions module pattern for registering components
  
- url: https://docs.dagster.io/concepts/partitions-schedules-sensors/schedules#time-zones
  why: Timezone handling to avoid UTC surprises

# CRITICAL PATTERNS TO FOLLOW
- file: src/work_data_hub/orchestration/jobs.py
  why: build_run_config pattern, job structure, config loading from YAML
  
- file: src/work_data_hub/io/connectors/file_connector.py  
  why: DataSourceConnector.discover() usage, DiscoveredFile metadata patterns
  
- file: src/work_data_hub/io/loader/warehouse_loader.py
  why: build_insert_sql() for DQ sensor plan-only probes
  
- file: tests/orchestration/test_jobs.py
  why: Testing patterns, monkeypatch usage, config validation approaches
  
- file: src/work_data_hub/config/data_sources.yml
  why: Configuration structure for table/pk extraction
```

### Current Codebase Structure
```bash
src/work_data_hub/
├── orchestration/
│   ├── __init__.py
│   ├── jobs.py                   # trustee_performance_multi_file_job, build_run_config
│   └── ops.py                    # discover_files_op, read_and_process_trustee_files_op, load_op
├── io/
│   ├── connectors/
│   │   └── file_connector.py     # DataSourceConnector.discover()
│   └── loader/
│       └── warehouse_loader.py   # build_insert_sql()
├── config/
│   ├── settings.py               # get_settings()
│   └── data_sources.yml          # domains.trustee_performance.table/pk
└── domain/
    └── trustee_performance/
        └── service.py            # process() function
```

### Desired Codebase Structure (Files to Add)
```bash
src/work_data_hub/orchestration/
├── schedules.py                  # Daily schedule for trustee_performance_multi_file_job
├── sensors.py                    # File discovery + data quality sensors  
└── repository.py                 # Definitions module exporting jobs, schedules, sensors

tests/orchestration/
├── test_schedules.py             # Schedule configuration tests
├── test_sensors.py               # Sensor logic and cursor management tests
└── test_repository.py            # Definitions import and registration tests
```

### Known Gotchas & Library Quirks
```python
# CRITICAL: Dagster requires proper timezone specification
# Without execution_timezone, schedules run in UTC causing confusion
@schedule(cron_schedule="0 2 * * *", execution_timezone="Asia/Shanghai")

# CRITICAL: Sensor cursor management for file discovery
# Must use float(context.cursor) and context.update_cursor(str(max_mtime))
# Cursor persistence prevents reprocessing same files

# CRITICAL: Keep Definitions imports lightweight  
# Avoid heavy I/O at import time - defer expensive operations to runtime
# Import pattern: from .jobs import job_list, not individual jobs

# CRITICAL: Data quality sensor should use plan-only probes
# Avoid actual DB operations - use build_insert_sql() with plan_only=True
# This provides health check without side effects

# CRITICAL: Windows path handling
# Use Path objects and os.path.join() for cross-platform compatibility
# DataSourceConnector already handles this correctly

# CRITICAL: Run config structure must match op schemas
# DiscoverFilesConfig, ReadProcessConfig, LoadConfig define expected structure
# Mirror build_run_config() pattern exactly for consistency
```

## Implementation Blueprint

### Data Models and Structure
The existing Pydantic config classes define the expected run_config structure:
```python
# From ops.py - these define the expected config schemas
class DiscoverFilesConfig(BaseModel):
    domain: str  # Must be "trustee_performance"

class ReadProcessConfig(BaseModel):  
    sheet: int = 0        # Excel sheet index
    max_files: int = 5    # Maximum files to process

class LoadConfig(BaseModel):
    table: str            # Database table name
    mode: str            # "delete_insert" for schedules 
    pk: List[str]        # Primary key columns
    plan_only: bool      # False for execute mode, True for DQ probes
```

### Task Implementation Order

```yaml
Task 1: CREATE schedules.py
PATTERN: Mirror build_run_config() from jobs.py exactly
  - Helper function _build_schedule_run_config()
  - Load data_sources.yml using get_settings().data_sources_config
  - Extract domain_config for trustee_performance table/pk
  - Build nested ops config matching existing pattern
  - @schedule decorator with cron_schedule="0 2 * * *"
  - execution_timezone="Asia/Shanghai" 
  - Target trustee_performance_multi_file_job

Task 2: CREATE sensors.py  
PATTERN: Use DataSourceConnector.discover("trustee_performance")
FILE_SENSOR:
  - @sensor decorator with minimum_interval_seconds=300
  - Use DataSourceConnector().discover("trustee_performance") 
  - Cursor management with last processed modified_time
  - Return RunRequest with run_key=str(max_mtime)
  - SkipReason when no new files found
DQ_SENSOR:
  - @sensor with minimum_interval_seconds=600  
  - Lightweight probe using build_insert_sql() plan-only
  - Process first file only to check health
  - SkipReason with informative messages for alerting

Task 3: CREATE repository.py
PATTERN: Follow Definitions module pattern
  - Import trustee_performance_job, trustee_performance_multi_file_job
  - Import schedules and sensors as lists
  - Export single Definitions object as 'defs'
  - Keep imports lightweight (no heavy I/O)

Task 4: CREATE test files
PATTERN: Mirror tests/orchestration/test_jobs.py structure
  - Use pytest and monkeypatch for dependency injection
  - Mock DataSourceConnector.discover() with synthetic files
  - Test run_config structure matches op schemas
  - Test cursor management in sensors
  - Validate schedule configuration

Task 5: VALIDATE implementation
PATTERN: Follow existing validation commands
  - ruff check --fix (style/syntax)
  - mypy src/ (type checking) 
  - pytest tests/ -v (all tests pass)
  - Optional: dagster dev -m src.work_data_hub.orchestration.repository
```

### Critical Code Examples

#### Schedule Implementation Pattern
```python
# schedules.py - EXACT pattern from INITIAL.md
from dagster import schedule
from .jobs import trustee_performance_multi_file_job
from ..config.settings import get_settings
import yaml

def _build_schedule_run_config():
    """Build run_config from data_sources.yml - mirrors build_run_config pattern"""
    settings = get_settings()
    with open(settings.data_sources_config, "r", encoding="utf-8") as f:
        ds = yaml.safe_load(f) or {}
    domain_cfg = ds.get("domains", {}).get("trustee_performance", {})
    return {
        "ops": {
            "discover_files_op": {"config": {"domain": "trustee_performance"}},
            "read_and_process_trustee_files_op": {"config": {"sheet": 0, "max_files": 5}},
            "load_op": {
                "config": {
                    "table": domain_cfg.get("table", "trustee_performance"),
                    "mode": "delete_insert",
                    "pk": domain_cfg.get("pk", ["report_date", "plan_code", "company_code"]),
                    "plan_only": False,
                }
            },
        }
    }

@schedule(cron_schedule="0 2 * * *", job=trustee_performance_multi_file_job, execution_timezone="Asia/Shanghai")
def trustee_daily_schedule(_context):
    return _build_schedule_run_config()
```

#### File Discovery Sensor Pattern  
```python
# sensors.py - File discovery with cursor management
from dagster import sensor, RunRequest, SkipReason
from ..io.connectors.file_connector import DataSourceConnector
from .jobs import trustee_performance_multi_file_job

@sensor(job=trustee_performance_multi_file_job, minimum_interval_seconds=300)
def trustee_new_files_sensor(context):
    connector = DataSourceConnector()
    files = connector.discover("trustee_performance")
    if not files:
        return SkipReason("No trustee_performance files found")

    # Cursor: last processed mtime  
    last_mtime = float(context.cursor) if context.cursor else 0.0
    new_files = [f for f in files if f.metadata.get("modified_time", 0) > last_mtime]
    if not new_files:
        return SkipReason("No new files since last poll")

    max_mtime = max(f.metadata.get("modified_time", 0) for f in new_files)
    context.update_cursor(str(max_mtime))

    # Use same config structure as schedule
    run_config = {
        "ops": {
            "discover_files_op": {"config": {"domain": "trustee_performance"}},
            "read_and_process_trustee_files_op": {"config": {"sheet": 0, "max_files": 5}},
            "load_op": {"config": {"table": "trustee_performance", "mode": "delete_insert", "pk": ["report_date","plan_code","company_code"], "plan_only": False}},
        }
    }
    return RunRequest(run_key=str(max_mtime), run_config=run_config)
```

### Integration Points
```yaml
CONFIG INTEGRATION:
  - Read from: src/work_data_hub/config/data_sources.yml
  - Extract: domains.trustee_performance.table and .pk
  - Pattern: Mirror build_run_config() yaml loading exactly

JOB INTEGRATION:
  - Target: trustee_performance_multi_file_job (existing)
  - Config: Must match DiscoverFilesConfig, ReadProcessConfig, LoadConfig schemas
  - Mode: "delete_insert" for production execution

FILE DISCOVERY:
  - Use: DataSourceConnector().discover("trustee_performance") 
  - Returns: DiscoveredFile objects with .metadata["modified_time"]
  - Pattern: Existing connector handles Unicode filenames and selection strategies

DATABASE:
  - No schema changes required
  - DQ sensor uses plan-only probes via build_insert_sql()
  - Schedule runs in execute mode (plan_only=False)
```

## Validation Loop

### Level 1: Syntax & Style  
```bash
# CRITICAL: Fix all style/syntax issues first
uv run ruff check src/work_data_hub/orchestration/ --fix
uv run mypy src/work_data_hub/orchestration/

# Expected: No errors. If errors exist, read output and fix before proceeding.
```

### Level 2: Unit Tests
```python  
# test_schedules.py - Test schedule configuration
def test_build_schedule_run_config():
    """Ensure schedule generates valid run_config matching op schemas"""
    from src.work_data_hub.orchestration.schedules import _build_schedule_run_config
    config = _build_schedule_run_config()
    
    # Validate structure matches op config schemas
    assert "ops" in config
    assert "discover_files_op" in config["ops"]
    assert config["ops"]["discover_files_op"]["config"]["domain"] == "trustee_performance"
    assert config["ops"]["load_op"]["config"]["plan_only"] is False

# test_sensors.py - Test sensor logic with mocks
def test_file_sensor_cursor_management(monkeypatch):
    """Test file sensor maintains cursor state correctly"""
    # Mock DataSourceConnector.discover to return synthetic files with mtimes
    mock_files = [
        type('DiscoveredFile', (), {
            'metadata': {'modified_time': 1000.0}, 'domain': 'trustee_performance'
        })()
    ]
    
    def mock_discover(domain):
        return mock_files if domain == "trustee_performance" else []
    
    monkeypatch.setattr("src.work_data_hub.io.connectors.file_connector.DataSourceConnector.discover", mock_discover)
    
    # Test sensor with cursor
    from src.work_data_hub.orchestration.sensors import trustee_new_files_sensor
    # ... test cursor logic

def test_dq_sensor_plan_only_probe():
    """Test DQ sensor performs lightweight health check"""
    # Mock components for plan-only operation
    # Verify no actual DB operations occur
```

```bash
# Run tests iteratively until all pass
uv run pytest tests/orchestration/ -v --cov=src.work_data_hub.orchestration --cov-report=term-missing

# Expected: All tests pass with good coverage. If failing, debug and fix.
```

### Level 3: Integration Test
```bash  
# Test Dagster discovery and UI integration
uv run dagster dev -m src.work_data_hub.orchestration.repository

# Expected behavior in Dagster UI:
# - Jobs page shows trustee_performance_job and trustee_performance_multi_file_job  
# - Schedules page shows trustee_daily_schedule (2:00 AM Asia/Shanghai)
# - Sensors page shows trustee_new_files_sensor and trustee_dq_sensor
# - No import errors or missing dependencies

# Manual verification:
# 1. Check schedule shows correct timezone and cron
# 2. Verify sensors show as "Stopped" (not errored)
# 3. Test sensor tick by checking logs for SkipReason messages
```

## Final Validation Checklist
- [ ] All tests pass: `uv run pytest tests/orchestration/ -v`
- [ ] No linting errors: `uv run ruff check src/work_data_hub/orchestration/`
- [ ] No type errors: `uv run mypy src/work_data_hub/orchestration/`
- [ ] Dagster UI loads without import errors
- [ ] Schedule appears with correct timezone (Asia/Shanghai)
- [ ] Sensors appear and can be started/stopped
- [ ] File sensor skips appropriately when no new files
- [ ] DQ sensor performs plan-only health checks
- [ ] All components follow existing codebase patterns

---

## Anti-Patterns to Avoid
- ❌ Don't create new config patterns - use existing data_sources.yml structure
- ❌ Don't skip timezone specification - causes UTC confusion
- ❌ Don't perform expensive I/O in Definitions imports
- ❌ Don't do actual DB operations in DQ sensor - use plan-only probes
- ❌ Don't ignore cursor management - leads to reprocessing same files
- ❌ Don't hardcode config values - extract from data_sources.yml like existing code

## Rollout & Risk Assessment
- **Low Risk**: Changes are purely additive, no modifications to existing jobs/ops
- **Rollback**: Remove new modules from Definitions to disable scheduling/sensors  
- **Dependencies**: Uses existing infrastructure (DataSourceConnector, build_insert_sql)
- **Monitoring**: Dagster UI provides built-in monitoring for schedules/sensors

## Confidence Score: 9/10

**High confidence factors:**
- Complete examples provided in INITIAL.md with exact code patterns
- Well-established codebase patterns to follow (build_run_config, DataSourceConnector)
- Comprehensive Dagster documentation for schedules/sensors
- Existing test patterns provide clear validation approach
- Integration points are clearly defined and tested
- Low risk additive changes with clear rollback path

**Minor uncertainty:**
- Sensor cursor behavior in edge cases (file deletion, clock changes)
- But Dagster's cursor management is well-documented and tested