name: "Shared Cleansing Pipeline Foundation - TransformStep API + Pipeline Builder"
description: |

## Purpose
Build a reusable domain-cleansing pipeline foundation that provides a TransformStep API, pipeline builder, config loader, and diagnostics. This enables domain services to orchestrate standardized data cleansing workflows before enrichment.

## Core Principles
1. **Context is King**: Include ALL necessary documentation, examples, and caveats
2. **Validation Loops**: Provide executable tests/lints the AI can run and fix
3. **Information Dense**: Use keywords and patterns from the codebase
4. **Progressive Success**: Start simple, validate, then enhance

---

## Goal
Create a production-ready pipeline foundation under `src/work_data_hub/domain/pipelines/` that allows domain services to compose cleansing workflows from configuration while maintaining error handling, metrics collection, and integration with existing cleansing rules.

## Why
- **Business value**: Standardizes data cleansing across all domain services
- **Integration**: Leverages existing cleansing rules through adapter pattern
- **Problems solved**: Eliminates monolithic cleansing code, enables config-driven workflows

## What
A library foundation where:
- Domain services build pipelines from config (YAML/JSON/dict)
- TransformStep API provides standardized step interface
- Pipeline runner executes steps in order with error handling
- Adapters integrate existing cleansing rules from registry
- Comprehensive metrics and diagnostics capture execution details

### Success Criteria
- [ ] `build_pipeline` assembles steps from config and executes in declared order
- [ ] Errors propagate according to config (`stop_on_error` vs warning mode)
- [ ] At least one adapter wraps `cleansing.registry` rules into TransformSteps
- [ ] Tests cover execution order, error handling, config validation, registry integration
- [ ] Documentation explains how domain services invoke the pipeline

## All Needed Context

### Documentation & References
```yaml
# MUST READ - Include these in your context window
- url: https://towardsdatascience.com/7-data-pipeline-best-practices-to-follow-5b83e2f1933e
  why: Pipeline best practices including error handling and monitoring

- url: https://refactoring.guru/design-patterns/builder
  why: Builder pattern for assembling pipelines from configuration

- url: https://medium.com/@satyem77/mastering-the-builder-pattern-in-python-55163a608ac4
  why: Python-specific builder pattern implementation

- url: https://lakefs.io/blog/etl-pipeline-architecture-the-ultimate-guide/
  why: Step-based pipeline architecture and modularity principles

- file: src/work_data_hub/cleansing/registry.py
  why: Registry pattern, singleton implementation, @rule decorator pattern, logging style

- file: src/work_data_hub/domain/sample_trustee_performance/service.py
  why: Domain orchestration patterns, error handling, batch processing, logging

- file: tests/domain/company_enrichment/test_lookup_queue.py
  why: Test structuring with mocks, fixtures, context managers, pytest patterns

- file: src/work_data_hub/utils/types.py
  why: Dataclass patterns, type definitions, metadata extraction patterns
```

### Current Codebase Structure
```bash
src/work_data_hub/
  cleansing/
    registry.py           # Singleton registry, @rule decorator, CleansingRule dataclass
    rules/
      numeric_rules.py    # Example cleansing implementations
  domain/
    annuity_performance/
      service.py          # Monolithic cleansing flow (will refactor later)
    sample_trustee_performance/
      service.py          # Pure transformation functions, error handling patterns
  utils/
    types.py              # Dataclass patterns, type definitions
    logging.py            # Logging utilities
```

### Desired Structure - Files to Create
```bash
src/work_data_hub/domain/pipelines/
├── __init__.py           # Public API: build_pipeline, TransformStep, Pipeline
├── types.py              # Row, StepResult, PipelineMetrics (from INITIAL.md)
├── config.py             # StepConfig, PipelineConfig Pydantic models
├── core.py               # TransformStep ABC, Pipeline class, execution logic
├── builder.py            # PipelineBuilder class with fluent API
├── adapters.py           # CleansingRuleStep adapter for registry integration
└── exceptions.py         # PipelineStepError, PipelineAssemblyError hierarchy

tests/domain/pipelines/
├── __init__.py           # Test package init
├── test_core.py          # Pipeline execution, step ordering, metrics
├── test_config_builder.py # Config validation, builder assembly
├── test_adapters.py      # Registry integration, cleansing rule wrapping
└── conftest.py           # Shared fixtures and test utilities
```

### Known Gotchas & Library Quirks
```python
# CRITICAL: Keep pipeline execution synchronous - domain services run in sync context
# CRITICAL: Pydantic v2 - use .model_validate (not .parse_obj) for config ingestion
# CRITICAL: Steps must mutate row copies (not original dict) to avoid side-effects
# CRITICAL: Use time.perf_counter() for metrics, convert to ms (int)
# CRITICAL: Provide clear exception hierarchy for future ops integration
# CRITICAL: Functions <40 lines, prefer dataclasses & helpers for clarity
# CRITICAL: Follow existing logging patterns with logger.getLogger(__name__)
```

## Implementation Blueprint

### Data Models and Structure

```python
# types.py - Core data structures (from INITIAL.md)
from dataclasses import dataclass
from typing import Dict, Any, Optional

Row = Dict[str, Any]

@dataclass
class StepResult:
    row: Row
    warnings: list[str]
    errors: list[str]
    metadata: dict[str, Any]

@dataclass
class PipelineMetrics:
    executed_steps: list[str]
    duration_ms: int

@dataclass
class PipelineResult:
    row: Row
    warnings: list[str]
    errors: list[str]
    metrics: PipelineMetrics
```

```python
# config.py - Pydantic configuration models
from pydantic import BaseModel, Field
from typing import Any, List

class StepConfig(BaseModel):
    name: str
    import_path: str  # e.g. "work_data_hub.domain.pipelines.steps.numeric.clean_decimal"
    options: dict[str, Any] = Field(default_factory=dict)
    requires: list[str] = Field(default_factory=list)

class PipelineConfig(BaseModel):
    name: str
    steps: list[StepConfig]
    stop_on_error: bool = True
```

### List of Tasks to be Completed

```yaml
Task 1: Create Core Types and Exceptions
CREATE src/work_data_hub/domain/pipelines/types.py:
  - PATTERN: Follow dataclass patterns from src/work_data_hub/utils/types.py
  - Define Row, StepResult, PipelineMetrics, PipelineResult from INITIAL.md specs
  - Keep types simple and JSON-serializable

CREATE src/work_data_hub/domain/pipelines/exceptions.py:
  - PATTERN: Follow error patterns from domain/sample_trustee_performance/service.py
  - Define PipelineStepError, PipelineAssemblyError hierarchy
  - Include context information (step name, row index) in error messages

Task 2: Implement TransformStep Interface and Pipeline Core
CREATE src/work_data_hub/domain/pipelines/core.py:
  - PATTERN: Follow ABC patterns and callable interfaces
  - Define TransformStep abstract base class with apply(row, context) -> StepResult
  - Implement Pipeline class with execute() method and metrics collection
  - Use time.perf_counter() for timing, convert to ms as int
  - Follow logging patterns from existing domain services

Task 3: Create Pydantic Configuration Models
CREATE src/work_data_hub/domain/pipelines/config.py:
  - PATTERN: Follow Pydantic v2 patterns from config/settings.py
  - Implement StepConfig and PipelineConfig models from INITIAL.md
  - Use .model_validate for config ingestion (not .parse_obj)
  - Add validation for import_path format and required fields

Task 4: Implement Pipeline Builder
CREATE src/work_data_hub/domain/pipelines/builder.py:
  - PATTERN: Follow builder pattern from external research
  - Implement PipelineBuilder with fluent API (add_step, with_config, etc.)
  - Support building from dict/YAML/JSON configuration
  - Dynamic import of step classes from import_path
  - Validate step dependencies and ordering

Task 5: Create Cleansing Rule Adapters
CREATE src/work_data_hub/domain/pipelines/adapters.py:
  - PATTERN: Follow adapter patterns and registry integration from cleansing/registry.py
  - Implement CleansingRuleStep.from_registry("rule_name") factory method
  - Wrap existing cleansing rules into TransformStep interface
  - Handle rule execution and result mapping to StepResult

Task 6: Implement Public API Module
CREATE src/work_data_hub/domain/pipelines/__init__.py:
  - PATTERN: Follow public API export patterns from existing modules
  - Export build_pipeline function, TransformStep, Pipeline classes
  - Add comprehensive module docstring with usage examples
  - Keep imports clean and well-organized

Task 7: Create Comprehensive Test Suite
CREATE tests/domain/pipelines/:
  - PATTERN: Follow test patterns from tests/domain/company_enrichment/test_lookup_queue.py
  - test_core.py: Pipeline execution, step ordering, error propagation, metrics
  - test_config_builder.py: Config validation, builder assembly, import resolution
  - test_adapters.py: Registry integration, cleansing rule wrapping
  - conftest.py: Shared fixtures for sample rows, configs, mock steps

Task 8: Add Integration Documentation
UPDATE src/work_data_hub/domain/pipelines/__init__.py:
  - PATTERN: Follow documentation patterns from existing domain services
  - Add module-level docstring explaining integration with domain services
  - Include example usage snippet from INITIAL.md
  - Document error handling and metrics collection approach
```

### Per Task Pseudocode

```python
# Task 2: Pipeline Core Implementation
class Pipeline:
    def __init__(self, steps: List[TransformStep], config: PipelineConfig):
        self.steps = steps
        self.config = config
        self.logger = logging.getLogger(__name__)

    def execute(self, row: Row, context: Optional[Dict] = None) -> PipelineResult:
        # PATTERN: Follow error handling from sample_trustee_performance/service.py
        start_time = time.perf_counter()
        executed_steps = []
        all_warnings = []
        all_errors = []
        current_row = {**row}  # CRITICAL: Work with copy to avoid side effects

        for step in self.steps:
            try:
                self.logger.debug(f"Executing step: {step.name}")
                result = step.apply(current_row, context or {})

                current_row = {**result.row}  # CRITICAL: Always copy
                all_warnings.extend(result.warnings)
                all_errors.extend(result.errors)
                executed_steps.append(step.name)

                # Handle stop_on_error configuration
                if result.errors and self.config.stop_on_error:
                    raise PipelineStepError(f"Step {step.name} failed: {result.errors[0]}")

            except Exception as e:
                error_msg = f"Step {step.name} execution failed: {e}"
                self.logger.error(error_msg)
                if self.config.stop_on_error:
                    raise PipelineStepError(error_msg)
                all_errors.append(error_msg)

        duration_ms = int((time.perf_counter() - start_time) * 1000)
        metrics = PipelineMetrics(executed_steps=executed_steps, duration_ms=duration_ms)

        return PipelineResult(
            row=current_row,
            warnings=all_warnings,
            errors=all_errors,
            metrics=metrics
        )

# Task 4: Builder Implementation with Config Support
def build_pipeline(config: Union[Dict, PipelineConfig], steps: Optional[List[TransformStep]] = None) -> Pipeline:
    # PATTERN: Follow builder pattern from external research
    if isinstance(config, dict):
        pipeline_config = PipelineConfig.model_validate(config)  # CRITICAL: Pydantic v2
    else:
        pipeline_config = config

    if steps is None:
        # Build steps from configuration
        steps = []
        for step_config in pipeline_config.steps:
            step_class = _import_step_class(step_config.import_path)
            step_instance = step_class(**step_config.options)
            steps.append(step_instance)

    return Pipeline(steps=steps, config=pipeline_config)

# Task 5: Cleansing Rule Adapter
class CleansingRuleStep(TransformStep):
    @classmethod
    def from_registry(cls, rule_name: str, **options) -> 'CleansingRuleStep':
        # PATTERN: Follow registry patterns from cleansing/registry.py
        from work_data_hub.cleansing.registry import registry

        rule = registry.get_rule(rule_name)
        if not rule:
            raise PipelineAssemblyError(f"Cleansing rule '{rule_name}' not found in registry")

        return cls(rule=rule, options=options)

    def apply(self, row: Row, context: Dict) -> StepResult:
        # Apply cleansing rule and wrap result in StepResult
        try:
            # Execute the cleansing rule function
            processed_row = {**row}  # CRITICAL: Work with copy
            for key, value in row.items():
                if key in self.options.get('target_fields', row.keys()):
                    processed_value = self.rule.func(value, **self.options)
                    processed_row[key] = processed_value

            return StepResult(
                row=processed_row,
                warnings=[],
                errors=[],
                metadata={"rule_name": self.rule.name, "category": self.rule.category.value}
            )
        except Exception as e:
            return StepResult(
                row=row,  # Return original on error
                warnings=[],
                errors=[f"Cleansing rule {self.rule.name} failed: {e}"],
                metadata={"rule_name": self.rule.name, "error": str(e)}
            )
```

### Integration Points
```yaml
EXISTING_INTEGRATIONS:
  - Registry: Import and wrap cleansing rules via adapters
  - Domain Services: Will integrate via future refactoring (Task 3 - out of scope)
  - Config: Support dict/YAML/JSON configuration formats
  - Logging: Use existing logger.getLogger(__name__) patterns

FUTURE_INTEGRATIONS:
  - Ops: Exception hierarchy designed for Dagster integration
  - Enrichment: Pipeline prepares clean data for company enrichment
  - Validation: Steps can include data quality checks
```

## Implementation-Facing Research Notes

### Source URLs and Key Insights
- **Pipeline Best Practices** (https://towardsdatascience.com/7-data-pipeline-best-practices-to-follow-5b83e2f1933e):
  - TL;DR: Modular steps, graceful error handling, comprehensive logging, retry mechanisms
  - Key insight: Validate data early and throughout pipeline, use dead-letter queues for problematic records
  - Required setup: None beyond existing uv environment

- **Builder Pattern** (https://refactoring.guru/design-patterns/builder):
  - TL;DR: Separate construction logic from representation, fluent API for readability
  - Key insight: Builder handles "how" to create pipeline, Pipeline handles "what" to execute
  - API decision: `build_pipeline(config, steps=None)` supports both config-driven and manual assembly

- **Python Builder Implementation** (https://medium.com/@satyem77/mastering-the-builder-pattern-in-python-55163a608ac4):
  - TL;DR: Chain methods for fluent API, return builder instance for chaining
  - Setup command: No additional dependencies required
  - Pitfall: Avoid mutable default arguments in builder methods

### Version Constraints and Compatibility
- **Pydantic v2**: Use `.model_validate` instead of `.parse_obj` for config loading
- **Python 3.8+**: Required for typing annotations and dataclass features
- **Existing Dependencies**: No new dependencies required, leverages existing uv environment

### Known Pitfalls and Mitigations
- **Row Mutation**: Always create copies `{**row}` to avoid side effects between steps
- **Sync Execution**: Keep all operations synchronous - domain services don't support async
- **Import Resolution**: Use `importlib.import_module` with proper error handling for dynamic imports
- **Memory Usage**: For large datasets, consider chunking in future iterations (out of scope)

### Open Questions for Clarification
- None significant - scope is well-defined and requirements are clear
- Future enhancement: Consider async support when domain services evolve

## Validation Loop

### Level 1: Syntax & Style
```bash
# Run these FIRST - fix any errors before proceeding
uv run ruff check src/ --fix         # Auto-fix style issues
uv run mypy src/                     # Type checking

# Expected: No errors. If errors, READ and fix.
```

### Level 2: Unit Tests
```python
# test_core.py - Pipeline execution and step ordering
def test_pipeline_executes_steps_in_order():
    """Test pipeline executes steps in declared order"""
    step1 = UpperCaseStep()
    step2 = TrimStep()
    config = PipelineConfig(name="test", steps=[], stop_on_error=True)
    pipeline = Pipeline(steps=[step1, step2], config=config)

    result = pipeline.execute({"name": "  test  "})
    assert result.row["name"] == "TEST"
    assert result.metrics.executed_steps == ["uppercase", "trim"]

def test_pipeline_handles_errors_according_to_config():
    """Test error propagation with stop_on_error configuration"""
    failing_step = FailingStep()
    config = PipelineConfig(name="test", steps=[], stop_on_error=False)
    pipeline = Pipeline(steps=[failing_step], config=config)

    result = pipeline.execute({"data": "test"})
    assert len(result.errors) > 0
    assert result.row["data"] == "test"  # Original data preserved

# test_config_builder.py - Configuration and builder
def test_build_pipeline_from_dict_config():
    """Test building pipeline from dictionary configuration"""
    config = {
        "name": "test_pipeline",
        "steps": [
            {"name": "uppercase", "import_path": "tests.fixtures.UpperCaseStep", "options": {}}
        ],
        "stop_on_error": True
    }

    pipeline = build_pipeline(config)
    assert pipeline.config.name == "test_pipeline"
    assert len(pipeline.steps) == 1

# test_adapters.py - Registry integration
def test_cleansing_rule_adapter_from_registry():
    """Test adapter wraps registry rules correctly"""
    step = CleansingRuleStep.from_registry("decimal_quantization", precision=2)

    result = step.apply({"amount": 123.456}, {})
    assert result.row["amount"] == 123.46
    assert result.metadata["rule_name"] == "decimal_quantization"
```

```bash
# Run tests iteratively until passing:
uv run pytest tests/domain/pipelines/ -v --cov=src/work_data_hub/domain/pipelines --cov-report=term-missing

# Focus on specific areas:
uv run pytest -v -k "pipelines"
uv run pytest -v tests/domain/pipelines/test_core.py
```

### Level 3: Integration Tests
```python
# Integration test with actual cleansing rules
def test_pipeline_with_real_cleansing_rules():
    """Test pipeline integrates with actual cleansing registry"""
    config = {
        "name": "numeric_cleaning",
        "steps": [
            {
                "name": "decimal_clean",
                "import_path": "work_data_hub.domain.pipelines.adapters.CleansingRuleStep.from_registry",
                "options": {"rule_name": "decimal_quantization", "precision": 4}
            }
        ],
        "stop_on_error": True
    }

    pipeline = build_pipeline(config)
    result = pipeline.execute({"计划代码": "AN001", "规模": 123.456789})

    assert result.row["规模"] == 123.4568
    assert len(result.errors) == 0
    assert "decimal_clean" in result.metrics.executed_steps

# Performance test with realistic data
def test_pipeline_performance_with_batch():
    """Test pipeline handles realistic data volumes"""
    rows = [{"amount": i * 1.23456} for i in range(1000)]
    config = PipelineConfig(name="perf_test", steps=[], stop_on_error=True)
    step = CleansingRuleStep.from_registry("decimal_quantization", precision=2)
    pipeline = Pipeline(steps=[step], config=config)

    start_time = time.perf_counter()
    for row in rows:
        result = pipeline.execute(row)
    duration = time.perf_counter() - start_time

    assert duration < 1.0  # Should process 1000 rows in under 1 second
```

## Final Validation Checklist
- [ ] All tests pass: `uv run pytest tests/domain/pipelines/ -v`
- [ ] No linting errors: `uv run ruff check src/ --fix`
- [ ] No type errors: `uv run mypy src/`
- [ ] Config-driven pipeline assembly works
- [ ] Registry adapter integrates existing cleansing rules
- [ ] Error handling respects stop_on_error configuration
- [ ] Metrics collection captures execution details
- [ ] Row immutability maintained (no side effects)
- [ ] Public API exports work correctly
- [ ] Documentation includes usage examples
- [ ] Integration patterns documented for domain services

---

## Anti-Patterns to Avoid
- ❌ Don't mutate original row dictionaries - always work with copies
- ❌ Don't use async functions - domain services require sync execution
- ❌ Don't use Pydantic v1 patterns - use .model_validate not .parse_obj
- ❌ Don't ignore error handling configuration - respect stop_on_error
- ❌ Don't skip metrics collection - timing and step tracking are required
- ❌ Don't hardcode step imports - use dynamic import from config
- ❌ Don't create monolithic step functions - keep steps focused and small

## Confidence Score: 8/10

High confidence due to:
- Clear requirements and scope definition in INITIAL.md
- Existing codebase patterns to follow (registry, service, test structures)
- Well-researched external patterns and best practices
- Comprehensive validation gates at multiple levels
- No new external dependencies required

Minor uncertainty on dynamic import error handling edge cases, but comprehensive test coverage should catch any issues.