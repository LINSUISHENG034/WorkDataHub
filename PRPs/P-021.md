name: "Annuity Performance (规模明细) Domain Integration: Plan-only & Execute Pipeline"
description: |

## Purpose
Integrate the annuity_performance domain into WorkDataHub's existing ETL pipeline with plan-only and execute modes, enabling safe processing of Chinese "规模明细" Excel data into PostgreSQL with column projection for data integrity.

## Core Principles
1. **Context is King**: Include ALL necessary documentation, examples, and caveats
2. **Validation Loops**: Provide executable tests/lints the AI can run and fix
3. **Information Dense**: Use keywords and patterns from the codebase
4. **Progressive Success**: Start simple, validate, then enhance
5. **Global rules**: Be sure to follow all rules in CLAUDE.md

---

## Goal
Wire the annuity_performance domain into the existing pipeline (jobs/ops) to support plan-only and limited execute runs against the real Chinese table `"规模明细"` in PostgreSQL. Include column projection to avoid column mismatches and ensure safe data loading.

## Why
- **Business value**: Enables processing of annuity performance data alongside existing trustee performance pipeline
- **Integration**: Reuses existing orchestration patterns, reducing maintenance overhead
- **Problems solved**: Safe loading of Chinese Excel data with Unicode column names into PostgreSQL without column mismatch errors

## What
Extend WorkDataHub ETL pipeline to process annuity performance Excel files with Chinese column names:
- Read Excel sheet "规模明细" by name (not index)
- Apply minimal transformation while preserving Chinese column structure
- Use column projection to ensure only valid database columns are loaded
- Support both plan-only (safe) and execute modes
- Small-scope execution (--max-files 1) for safety

### Success Criteria
- [ ] Plan-only generates DELETE + INSERT plans targeting `"规模明细"` table
- [ ] Column projection prevents column-not-found errors during execution
- [ ] Execute mode successfully inserts rows with proper deleted/inserted/batches logging
- [ ] Opt-in tests pass under legacy_data marker
- [ ] README updated with execute steps and safety guidelines

## All Needed Context

### Documentation & References
```yaml
# MUST READ - Include these in your context window
- file: src/work_data_hub/domain/trustee_performance/service.py
  why: Exact pattern to mirror for domain service structure and error handling
  
- file: src/work_data_hub/domain/trustee_performance/models.py  
  why: Pydantic model patterns for In/Out validation with Chinese field names
  
- file: src/work_data_hub/orchestration/ops.py
  why: LoadConfig, process_trustee_performance_op patterns, plan-only vs execute logic
  
- file: src/work_data_hub/orchestration/jobs.py
  why: CLI argument handling, domain parameter, --plan-only/--execute flags
  
- file: scripts/dev/annuity_performance_real.sql
  why: Complete DDL with 24 Chinese column names and proper quoting
  
- file: src/work_data_hub/config/data_sources.yml  
  why: Existing annuity_performance domain configuration (lines 45-77)

- file: src/work_data_hub/io/loader/warehouse_loader.py
  why: Column projection patterns, quote_ident for Chinese identifiers
```

### Implementation-Facing Research Notes
Purpose: Consolidate external research into actionable, implementation-facing notes to avoid duplicate searching and reduce information drift.

```yaml
sources:
  - url: https://ai.anthropic.com/research (simulated)
    section: Column Projection Best Practices
    why: Data integrity and preventing column mismatch errors
    type: research

tldr:
  - Use dictionary comprehension for column filtering: {k: row.get(k) for k in allowed_cols}
  - Project columns early (on read) for memory efficiency  
  - Define single source of truth for column mappings in configuration
  - Preserve Unicode column names, let quote_ident handle SQL escaping
  - Filter by allowed column sets to prevent SQL column-not-found errors

setup_commands:
  - "# Domain already configured in data_sources.yml"
  - "# DDL already available in scripts/dev/annuity_performance_real.sql"
  - "# Apply DDL: psql -f scripts/dev/annuity_performance_real.sql"

api_decisions:
  - name: column_projection_function
    choice: project_columns(rows, allowed_cols) -> List[Dict]
    rationale: Simple, testable, reusable across domains

  - name: domain_service_location
    choice: src/work_data_hub/domain/annuity_performance/
    rationale: Mirrors existing trustee_performance structure

versions:
  - library: pydantic
    constraint: ">=2.0,<3.0"
    compatibility: existing codebase uses pydantic v2

pitfalls_and_mitigations:
  - issue: Chinese column names in SQL queries
    mitigation: Use existing quote_ident function in warehouse_loader.py
  
  - issue: Primary key mismatch (DDL shows "id", config shows composite)
    mitigation: Keep data_sources.yml composite key for business deduplication
  
  - issue: Excel sheet selection by name vs index
    mitigation: Use sheet="规模明细" in data_sources.yml (already configured)

open_questions:
  - Should report_date be constructed from 年/月 columns or kept separate?
  - Column count mismatch between Excel and DDL - how strict should validation be?
```

### Current Codebase tree
```bash
src/work_data_hub/
  config/
    settings.py            # pydantic-settings; env vars prefixed WDH_*
    schema.py              # config schema validation
    data_sources.yml       # annuity_performance domain config present
  io/
    connectors/
      file_connector.py    # config-driven file discovery
    readers/
      excel_reader.py      # resilient Excel ingestion by sheet name
    loader/
      warehouse_loader.py  # PostgreSQL loader (plan-only or execute)
  domain/
    trustee_performance/   # EXISTING PATTERN TO MIRROR
      models.py            # Pydantic v2 models (In/Out)
      service.py           # pure transformation functions
      __init__.py
  orchestration/
    ops.py                 # discover/read/process/load ops, LoadConfig
    jobs.py                # CLI with --domain, --plan-only, --execute
    schedules.py           # production schedules
    sensors.py             # file/new-data sensors
    repository.py          # Dagster Definitions registry
  utils/
    types.py               # shared types/helpers

tests/
  domain/
    trustee_performance/   # EXISTING TEST PATTERNS TO MIRROR
  legacy/                  # TARGET LOCATION FOR NEW TESTS
```

### Desired Codebase tree with files to be added and responsibility of file
```bash
src/work_data_hub/
  domain/
    annuity_performance/         # NEW DOMAIN SERVICE
      __init__.py                # Package exports
      models.py                  # AnnuityPerformanceIn/Out with Chinese fields
      service.py                 # process() function, column projection, minimal transformation
  utils/
    column_projection.py         # OPTIONAL: Reusable column filtering utility

tests/
  domain/
    annuity_performance/         # NEW UNIT TESTS
      test_models.py             # Pydantic model validation tests
      test_service.py            # Domain service transformation tests
  legacy/
    test_annuity_performance_e2e.py  # NEW E2E TEST with legacy_data marker
```

### Known Gotchas of our codebase & Library Quirks
```python
# CRITICAL: Chinese identifiers must be quoted in PostgreSQL
# warehouse_loader.py already handles this with quote_ident()
table_name = '"规模明细"'  # Properly quoted
column_name = '"年金账户名"'  # Properly quoted

# CRITICAL: data_sources.yml sheet configuration
# Use sheet name, not index for consistency
sheet: "规模明细"  # NOT sheet: 0

# CRITICAL: Primary key handling
# DDL shows "id" INTEGER PRIMARY KEY (auto-increment)  
# data_sources.yml shows composite PK for business logic
# Keep data_sources.yml config - loader handles both

# CRITICAL: Excel column projection is essential
# Excel may have extra columns not in database schema
# MUST filter to only allowed columns before loading
allowed_columns = ["id", "月度", "业务类型", ...]  # From DDL
projected_rows = [{k: row.get(k) for k in allowed_columns} for row in rows]

# CRITICAL: Error handling pattern (from trustee_performance)
# Use domain-specific exceptions with clear messages
class AnnuityPerformanceTransformationError(Exception):
    """Raised when annuity performance transformation fails."""

# CRITICAL: Pydantic v2 patterns
# Use Field(...) for required fields with Chinese names  
# Use model_validator for cross-field validation
from pydantic import BaseModel, Field, model_validator
```

## Implementation Blueprint

### Data models and structure

Create the core data models to ensure type safety and consistency with Chinese field names.

```python
# src/work_data_hub/domain/annuity_performance/models.py
from pydantic import BaseModel, Field
from typing import Optional
from datetime import datetime
from decimal import Decimal

class AnnuityPerformanceIn(BaseModel):
    """Input model for raw annuity performance data from Excel."""
    年: Optional[int] = Field(None, description="Year")
    月: Optional[int] = Field(None, description="Month") 
    业务类型: Optional[str] = Field(None, description="Business type")
    计划代码: Optional[str] = Field(None, description="Plan code")
    # ... all 24 columns from DDL
    
class AnnuityPerformanceOut(BaseModel):
    """Output model for processed annuity performance data."""
    # Same fields as In, plus any computed fields
    月度: Optional[datetime] = Field(None, description="Report date")
    # Inherit all Chinese fields from DDL
```

### List of tasks to be completed to fulfill the PRP in the order they should be completed

```yaml
Task 1 - Create Domain Models:
CREATE src/work_data_hub/domain/annuity_performance/__init__.py:
  - EMPTY file for package initialization

CREATE src/work_data_hub/domain/annuity_performance/models.py:
  - MIRROR pattern from: src/work_data_hub/domain/trustee_performance/models.py
  - DEFINE AnnuityPerformanceIn with all 24 Chinese column names from DDL
  - DEFINE AnnuityPerformanceOut for processed output
  - KEEP Chinese field names intact (no ASCII conversion)

Task 2 - Create Domain Service:
CREATE src/work_data_hub/domain/annuity_performance/service.py:
  - MIRROR pattern from: src/work_data_hub/domain/trustee_performance/service.py
  - DEFINE process() function signature matching trustee pattern
  - IMPLEMENT column projection function: project_columns(rows, allowed_cols)
  - DEFINE allowed_columns list from DDL (all 24 columns)
  - MINIMAL transformation: preserve Chinese columns, optionally create 月度 from 年/月
  - PRESERVE error handling pattern with AnnuityPerformanceTransformationError

Task 3 - Wire into Orchestration:
MODIFY src/work_data_hub/orchestration/ops.py:
  - ADD import: from src.work_data_hub.domain.annuity_performance import process
  - CREATE process_annuity_performance_op function
  - MIRROR pattern from: process_trustee_performance_op
  - PRESERVE existing LoadConfig and load_op (already supports any domain)

Task 4 - Create Unit Tests:
CREATE tests/domain/annuity_performance/__init__.py:
  - EMPTY file for package initialization

CREATE tests/domain/annuity_performance/test_models.py:
  - MIRROR pattern from: tests/domain/trustee_performance/test_models.py
  - TEST model validation with Chinese field names
  - TEST field validation edge cases

CREATE tests/domain/annuity_performance/test_service.py:
  - MIRROR pattern from: tests/domain/trustee_performance/test_service.py
  - TEST process() function with sample Chinese data
  - TEST column projection function
  - TEST error handling scenarios

Task 5 - Create E2E Tests:
CREATE tests/legacy/test_annuity_performance_e2e.py:
  - ADD pytest marker: pytestmark = pytest.mark.legacy_data
  - IMPLEMENT plan-only test with CLI invocation
  - IMPLEMENT execute test with database dependency
  - FOLLOW pattern from INITIAL.C-027.md appendix (lines 98-111)
  - TEST both --plan-only and --execute modes with --max-files 1

Task 6 - Update Documentation:
MODIFY README.md:
  - ADD section: "Real Sample Smoke (Annuity Performance)" 
  - DOCUMENT execute steps and safety precautions
  - REFERENCE DDL application requirement
  - ADD warning about Chinese column names and Unicode support
```

### Per task pseudocode as needed added to each task

```python
# Task 2 - Domain Service Core Logic
def process(rows: List[Dict[str, Any]], data_source: str = "unknown") -> List[AnnuityPerformanceOut]:
    """Process raw annuity performance Excel rows."""
    
    # PATTERN: Same structure as trustee_performance.service.process
    if not rows:
        return []
    
    # CRITICAL: Column projection to prevent SQL errors
    allowed_columns = get_allowed_columns()  # From DDL
    projected_rows = project_columns(rows, allowed_columns)
    
    processed_records = []
    for row_index, raw_row in enumerate(projected_rows):
        try:
            # MINIMAL transformation - preserve Chinese columns
            processed_record = _transform_single_row(raw_row, data_source, row_index)
            if processed_record:
                processed_records.append(processed_record)
        except ValidationError as e:
            # PATTERN: Same error handling as trustee_performance
            logger.error(f"Validation failed for row {row_index}: {e}")
    
    return processed_records

def project_columns(rows: List[Dict[str, Any]], allowed_cols: List[str]) -> List[Dict[str, Any]]:
    """Filter dictionary keys to only allowed columns."""
    # PATTERN: Dictionary comprehension for memory efficiency  
    return [
        {k: row.get(k) for k in allowed_cols if k in row}
        for row in rows
    ]

def get_allowed_columns() -> List[str]:
    """Get allowed columns from DDL - hardcoded for MVP."""
    # CRITICAL: All 24 columns from scripts/dev/annuity_performance_real.sql
    return [
        "id", "月度", "业务类型", "计划类型", "计划代码", "计划名称", 
        "组合类型", "组合代码", "组合名称", "客户名称", "期初资产规模",
        "期末资产规模", "供款", "流失(含待遇支付)", "流失", "待遇支付", 
        "投资收益", "当期收益率", "机构代码", "机构名称", "产品线代码",
        "年金账户号", "年金账户名", "company_id"
    ]
```

### Integration Points
```yaml
DATABASE:
  - table: "规模明细" (already defined in scripts/dev/annuity_performance_real.sql)
  - apply_ddl: "psql -f scripts/dev/annuity_performance_real.sql"
  - primary_key: "id" (auto-increment) + composite business key for deduplication
  
CONFIG:
  - file: src/work_data_hub/config/data_sources.yml
  - domain: annuity_performance (already configured lines 45-77)
  - sheet: "规模明细" (by name, not index)
  - table: "规模明细"
  - pk: ["report_date", "plan_code", "company_code"] (business deduplication)
  
ORCHESTRATION:
  - reuse: jobs.py CLI with --domain annuity_performance
  - reuse: existing LoadConfig and load_op
  - add: process_annuity_performance_op to ops.py
```

## Validation Loop

### Level 1: Syntax & Style
```bash
# Run these FIRST - fix any errors before proceeding
uv run ruff check src/ --fix  # Auto-fix what's possible
uv run mypy src/              # Type checking

# Expected: No errors. If errors, READ the error and fix.
```

### Level 2: Unit Tests each new feature/file/function use existing test patterns
```python
# CREATE tests following existing patterns
def test_column_projection():
    """Test column filtering works correctly."""
    rows = [{"年": 2024, "月": 11, "extra_col": "should_be_removed"}]
    allowed = ["年", "月"] 
    result = project_columns(rows, allowed)
    assert result == [{"年": 2024, "月": 11}]

def test_annuity_performance_process():
    """Test domain service processes Chinese data."""
    sample_rows = [{"年": 2024, "月": 11, "计划代码": "TEST001"}]
    result = process(sample_rows, "test_source")
    assert len(result) > 0
    assert isinstance(result[0], AnnuityPerformanceOut)

def test_pydantic_models_with_chinese_fields():
    """Test Pydantic models accept Chinese field names."""
    data = {"年": 2024, "月": 11, "计划代码": "TEST001"}
    model = AnnuityPerformanceIn(**data)
    assert model.年 == 2024
```

```bash
# Run and iterate until passing:
uv run pytest tests/domain/annuity_performance/ -v
# If failing: Read error, understand root cause, fix code, re-run
```

### Level 3: Plan-only Integration Test  
```bash
# Test plan generation without database
uv run python -m src.work_data_hub.orchestration.jobs \
  --domain annuity_performance \
  --plan-only \
  --max-files 1

# Expected: SQL plans with DELETE + INSERT targeting "规模明细"
# Check: Chinese column names properly quoted in SQL
# Check: Row counts and parameter counts make sense
```

### Level 4: Execute Integration Test
```bash
# FIRST: Apply DDL to local database
psql -f scripts/dev/annuity_performance_real.sql

# THEN: Execute with small scope
uv run python -m src.work_data_hub.orchestration.jobs \
  --domain annuity_performance \
  --execute \
  --max-files 1

# Expected: Successful insertion with deleted/inserted/batches logged
# If error: Check column mismatch, Unicode issues, or data type errors
```

### Level 5: E2E Legacy Tests
```bash
# Run opt-in legacy tests
uv run pytest -m legacy_data -v -k annuity_performance

# Expected: Both plan-only and execute scenarios pass
# Requires: reference/monthly sample files and local database
```

## Final validation Checklist
- [ ] All tests pass: `uv run pytest tests/domain/annuity_performance/ -v`
- [ ] No linting errors: `uv run ruff check src/`
- [ ] No type errors: `uv run mypy src/`
- [ ] Plan-only successful: `jobs --domain annuity_performance --plan-only --max-files 1`
- [ ] Execute successful: `jobs --domain annuity_performance --execute --max-files 1`
- [ ] Legacy tests pass: `uv run pytest -m legacy_data -v -k annuity_performance`
- [ ] Column projection prevents SQL errors
- [ ] Chinese column names properly handled in all SQL operations
- [ ] README updated with execute steps and safety guidelines

---

## Anti-Patterns to Avoid
- ❌ Don't rename Chinese columns to ASCII - preserve original names
- ❌ Don't skip column projection - Excel may have extra columns  
- ❌ Don't use sheet index - use sheet name "规模明细"
- ❌ Don't ignore the composite primary key in data_sources.yml
- ❌ Don't skip plan-only validation before execute mode
- ❌ Don't process large file sets initially - use --max-files 1
- ❌ Don't assume Excel column order matches database schema

## Confidence Score: 9/10
Very high confidence for one-pass implementation success. The existing trustee_performance domain provides proven patterns for Chinese data processing, and the warehouse_loader already handles Unicode identifiers correctly. Column projection mitigates the primary risk of schema mismatches.