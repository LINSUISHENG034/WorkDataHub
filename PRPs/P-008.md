name: "Convert Legacy Handler/Mapping DB to YAML Seeds - C-012"
description: |

## Purpose
Replace legacy DB-resident handler/mapping lookups with configuration-driven YAML seeds and a typed loader. This removes DB coupling, aligns with declarative config, and prepares the ground for migrating additional domains beyond trustee_performance.

## Core Principles
1. **Context is King**: Include ALL necessary documentation, examples, and caveats
2. **Validation Loops**: Provide executable tests/lints the AI can run and fix
3. **Information Dense**: Use keywords and patterns from the codebase
4. **Progressive Success**: Start simple, validate, then enhance
5. **Global rules**: Follow all rules in CLAUDE.md

---

## Goal
Create YAML seed files and a typed loader that model key legacy mappings and handler configuration in code, without any database dependency. Deliver a foundational configuration system that can be extended for future domain migrations.

## Why
- **Business value**: Eliminates DB coupling for static mapping data, improving startup time and reducing complexity
- **Integration**: Prepares foundation for migrating additional domains beyond trustee_performance
- **Problems solved**: Removes dependency on MySQL lookups for small constant maps, enables version-controlled configuration

## What
A configuration system that provides:
- Four YAML seed files for key legacy mappings (company_branch, portfolio codes, overrides, business types)
- A typed mapping loader module with validation
- Optional Pydantic schema for data_sources.yml validation
- Comprehensive tests ensuring reliability and error handling

### Success Criteria
- [ ] YAML seeds created with UTF-8 support and sample content
- [ ] `mapping_loader.py` loads all mappings with proper validation
- [ ] Optional `schema.py` validates existing `data_sources.yml` structure
- [ ] All tests pass with comprehensive coverage
- [ ] No EAST-related code added anywhere
- [ ] No DB/network dependencies introduced

## All Needed Context

### Documentation & References
```yaml
# MUST READ - Include these in your context window
- url: https://docs.pydantic.dev/latest/concepts/models/
  why: BaseModel patterns for schema validation, field validation

- url: https://docs.pydantic.dev/latest/concepts/pydantic_settings/
  why: Settings management patterns currently used in codebase

- url: https://docs.pydantic.dev/latest/api/config/
  why: Model configuration options for validation behavior

- file: src/work_data_hub/config/settings.py
  why: Existing Pydantic configuration patterns, field validation approach

- file: src/work_data_hub/io/connectors/file_connector.py  
  why: EXACT YAML loading pattern with UTF-8, error handling, validation structure

- file: src/work_data_hub/config/data_sources.yml
  why: Current config structure to understand and optionally validate with schema

- file: tests/io/test_file_connector.py
  why: Comprehensive test patterns, fixtures, mocking, error case testing

- file: INITIAL.md
  why: Complete requirements, sample YAML content, validation gates, constraints
```

### Current Codebase tree
```bash
src/work_data_hub/
├── config/
│   ├── data_sources.yml          # Current config - validate with optional schema
│   ├── settings.py               # Pydantic patterns to follow
│   └── __init__.py
├── io/connectors/
│   └── file_connector.py         # EXACT YAML loading pattern to mirror
├── domain/trustee_performance/   # Existing domain structure
└── utils/types.py               # Type definitions for reference

tests/
├── io/
│   └── test_file_connector.py    # Test patterns to follow
└── config/                       # Directory to create
```

### Desired Codebase tree with files to be added
```bash
src/work_data_hub/
├── config/
│   ├── mappings/                 # NEW: YAML seed files directory
│   │   ├── company_branch.yml    # 机构名称 -> 机构代码
│   │   ├── default_portfolio_code.yml  # 集合计划 -> QTAN codes
│   │   ├── company_id_overrides_plan.yml  # Plan ID overrides
│   │   └── business_type_code.yml  # 产品线 -> 代码 (samples)
│   ├── mapping_loader.py         # NEW: Core loader with typed functions
│   ├── schema.py                 # NEW: Optional data_sources.yml schema
│   ├── data_sources.yml          # EXISTING: Validate with new schema
│   └── settings.py               # EXISTING: Reference for patterns

tests/
├── config/                       # NEW: Test directory
│   ├── test_mapping_loader.py    # NEW: Comprehensive loader tests
│   └── test_data_sources_schema.py  # NEW: Optional schema validation tests
└── fixtures/                     # EXISTING: Use for test data
```

### Known Gotchas & Library Quirks
```python
# CRITICAL: EAST 相关实现（停用，不做）- Never implement EAST anywhere
# CRITICAL: Use yaml.safe_load() with encoding="utf-8" for security and Chinese support  
# CRITICAL: Follow file_connector.py pattern exactly - it's the proven YAML loading approach
# CRITICAL: Keep YAML keys in natural (Chinese) names - loader should not force ASCII
# CRITICAL: Use allow_unicode=True when writing YAML files for Chinese character support
# CRITICAL: PyYAML is already in dependencies - don't add new YAML libraries
# CRITICAL: Follow test_file_connector.py patterns for comprehensive test coverage
# CRITICAL: Validation gates are specific: pytest -k "(mapping_loader or data_sources_schema) and not postgres"
```

## Implementation Blueprint

### Data models and structure

```python
# models.py - Core data structures following settings.py patterns
from pydantic import BaseModel, Field
from typing import Dict, List, Optional, Any, Literal

# Optional schema for data_sources.yml validation
class DomainConfig(BaseModel):
    """Schema for individual domain configuration."""
    description: Optional[str] = None
    pattern: str = Field(..., description="Regex pattern for file matching")
    select: Literal["latest_by_year_month", "latest_by_mtime"] = Field(
        ..., description="Selection strategy for multiple files"
    )
    sheet: int | str = Field(0, description="Excel sheet to process")
    table: str = Field(..., description="Target database table")
    pk: List[str] = Field(..., min_items=1, description="Primary key columns")
    required_columns: Optional[List[str]] = None
    validation: Optional[Dict[str, Any]] = None

class DataSourcesConfig(BaseModel):
    """Schema for complete data_sources.yml structure."""
    domains: Dict[str, DomainConfig] = Field(..., min_items=1)
    discovery: Optional[Dict[str, Any]] = None

# Mapping validation helper
class MappingValidationError(ValueError):
    """Raised when YAML mapping structure is invalid."""
    pass
```

### List of tasks to be completed

```yaml
Task 1: Create YAML Seed Files
CREATE src/work_data_hub/config/mappings/ directory:
  - PATTERN: Mirror existing config/ structure
  - Use exact content from INITIAL.md examples
  - Ensure UTF-8 encoding with Chinese character support

CREATE src/work_data_hub/config/mappings/company_branch.yml:
  - CONTENT: Exact sample from INITIAL.md lines 55-64
  - ENCODING: UTF-8 with allow_unicode=True
  - PATTERN: Key-value mapping structure

CREATE src/work_data_hub/config/mappings/default_portfolio_code.yml:
  - CONTENT: Exact sample from INITIAL.md lines 66-71
  - PATTERN: Plan type -> QTAN code mapping

CREATE src/work_data_hub/config/mappings/company_id_overrides_plan.yml:
  - CONTENT: Exact sample from INITIAL.md lines 73-84
  - PATTERN: Plan code -> company ID overrides

CREATE src/work_data_hub/config/mappings/business_type_code.yml:
  - CONTENT: Exact sample from INITIAL.md lines 86-91
  - PATTERN: Business type -> code (sample entries)

Task 2: Implement Core Mapping Loader
CREATE src/work_data_hub/config/mapping_loader.py:
  - PATTERN: Follow file_connector.py YAML loading structure exactly
  - Use yaml.safe_load with encoding="utf-8"
  - Implement shared load_yaml_mapping helper
  - Add specific loader functions for each mapping type
  - Include comprehensive error handling and validation

Task 3: Optional Schema Implementation  
CREATE src/work_data_hub/config/schema.py:
  - PATTERN: Follow settings.py Pydantic model structure
  - Validate existing data_sources.yml structure
  - Use BaseModel with field validation
  - Include clear error messages for validation failures

Task 4: Comprehensive Test Coverage
CREATE tests/config/test_mapping_loader.py:
  - PATTERN: Follow test_file_connector.py structure exactly
  - Test happy path for each mapping loader
  - Test invalid YAML raises clear exceptions
  - Test file not found scenarios
  - Test invalid mapping structure validation
  - Use fixtures for test data setup

CREATE tests/config/test_data_sources_schema.py:
  - PATTERN: Follow existing test patterns
  - Test schema validation passes for current data_sources.yml
  - Test schema validation fails for missing required fields
  - Test comprehensive validation scenarios
```

### Per task pseudocode

```python
# Task 2: mapping_loader.py - Mirror file_connector.py exactly
from pathlib import Path
from typing import Dict
import yaml

class MappingLoaderError(Exception):
    """Raised when mapping loader encounters an error."""
    pass

def load_yaml_mapping(path: str) -> Dict[str, str]:
    """
    Load and validate YAML mapping file - EXACT pattern from file_connector.py
    
    Args:
        path: Path to YAML mapping file
        
    Returns:
        Dictionary with string keys and string values
        
    Raises:
        MappingLoaderError: If file cannot be loaded or structure invalid
    """
    config_path = Path(path)
    
    if not config_path.exists():
        raise MappingLoaderError(f"Mapping file not found: {path}")
    
    try:
        # PATTERN: Exact same structure as file_connector._load_config
        with open(config_path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f) or {}
    except yaml.YAMLError as e:
        raise MappingLoaderError(f"Invalid YAML mapping: {e}")
    except Exception as e:
        raise MappingLoaderError(f"Failed to load mapping: {e}")
    
    # PATTERN: Validate structure like file_connector domain validation
    if not isinstance(data, dict):
        raise MappingLoaderError("Mapping must be a dictionary")
    
    # Convert all values to strings for consistency
    validated_mapping = {}
    for key, value in data.items():
        if not isinstance(key, str):
            raise MappingLoaderError(f"Mapping key must be string, got {type(key)}")
        if not isinstance(value, (str, int)):
            raise MappingLoaderError(f"Mapping value must be string or int, got {type(value)}")
        validated_mapping[key] = str(value)
    
    return validated_mapping

# Specific loader functions
def load_company_branch() -> Dict[str, str]:
    """Load company branch name to code mapping."""
    return load_yaml_mapping("src/work_data_hub/config/mappings/company_branch.yml")

def load_default_portfolio_code() -> Dict[str, str]:
    """Load default portfolio code mapping."""
    return load_yaml_mapping("src/work_data_hub/config/mappings/default_portfolio_code.yml")

def load_company_id_overrides_plan() -> Dict[str, str]:
    """Load company ID override mapping for plans."""
    return load_yaml_mapping("src/work_data_hub/config/mappings/company_id_overrides_plan.yml")

def load_business_type_code() -> Dict[str, str]:
    """Load business type to code mapping."""
    return load_yaml_mapping("src/work_data_hub/config/mappings/business_type_code.yml")

# Task 3: schema.py - Follow settings.py patterns
from pydantic import BaseModel, Field, validator
from pathlib import Path
import yaml

def validate_data_sources_config(config_path: str = "src/work_data_hub/config/data_sources.yml") -> bool:
    """
    Validate data_sources.yml against schema.
    
    Returns:
        True if valid, raises ValidationError if invalid
    """
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
        
        # PATTERN: Use Pydantic validation like settings.py
        config = DataSourcesConfig(**data)
        return True
    except Exception as e:
        raise ValidationError(f"data_sources.yml validation failed: {e}")
```

### Integration Points
```yaml
FILE_STRUCTURE:
  - mappings/: New directory under src/work_data_hub/config/
  - Four YAML files: company_branch, default_portfolio_code, company_id_overrides_plan, business_type_code
  - mapping_loader.py: Core module with typed functions
  - schema.py: Optional validation for data_sources.yml

DEPENDENCIES:
  - No new dependencies required
  - PyYAML: Already in pyproject.toml
  - Pydantic: Already available for validation

TESTING:
  - tests/config/: New test directory
  - Follow test_file_connector.py patterns exactly
  - Use existing fixture patterns from tests/fixtures/
```

## Validation Loop

### Level 1: Syntax & Style
```bash
# Run these FIRST - fix any errors before proceeding
uv run ruff check src/ --fix        # Auto-fix style issues - EXACT command from INITIAL.md
uv run mypy src/                    # Type checking - EXACT command from INITIAL.md

# Expected: No errors. If errors, READ and fix following CLAUDE.md patterns.
```

### Level 2: Unit Tests
```python
# test_mapping_loader.py - Following test_file_connector.py patterns
import pytest
import yaml
from pathlib import Path

def test_load_company_branch_happy_path():
    """Test successful loading of company branch mapping."""
    mapping = load_company_branch()
    
    # Assert known sample entries from YAML
    assert "内蒙" in mapping
    assert mapping["内蒙"] == "G31"
    assert "济南" in mapping
    assert mapping["济南"] == "G21"

def test_load_mapping_file_not_found():
    """Test error when mapping file doesn't exist."""
    with pytest.raises(MappingLoaderError, match="not found"):
        load_yaml_mapping("/nonexistent/path.yml")

def test_load_invalid_yaml_structure():
    """Test error with invalid YAML mapping structure."""
    # Create temp invalid YAML file
    invalid_content = "not_a_dict_but_a_string"
    
    with pytest.raises(MappingLoaderError, match="must be a dictionary"):
        # Test with invalid structure
        pass

def test_load_invalid_value_types():
    """Test error with invalid value types in mapping."""
    # Test with complex objects as values
    with pytest.raises(MappingLoaderError, match="must be string or int"):
        # Test validation
        pass

# test_data_sources_schema.py 
def test_current_data_sources_validates():
    """Test that current data_sources.yml passes schema validation."""
    result = validate_data_sources_config()
    assert result is True

def test_invalid_data_sources_fails():
    """Test that invalid structure fails validation."""
    with pytest.raises(ValidationError):
        # Test with missing required fields
        pass
```

```bash
# Run exact command from INITIAL.md validation gates:
uv run pytest -v -k "(mapping_loader or data_sources_schema) and not postgres"

# If failing: Debug specific test, understand error, fix code, re-run
# Never mock to pass - fix the actual implementation
```

### Level 3: Integration Test
```python
# Manual verification script
def test_all_mappings_load_successfully():
    """Integration test - all mappings load without errors."""
    mappings = {
        "company_branch": load_company_branch(),
        "portfolio_code": load_default_portfolio_code(), 
        "id_overrides": load_company_id_overrides_plan(),
        "business_type": load_business_type_code()
    }
    
    # Verify each mapping has expected structure
    for name, mapping in mappings.items():
        assert isinstance(mapping, dict)
        assert len(mapping) > 0
        # All keys and values should be strings
        for key, value in mapping.items():
            assert isinstance(key, str)
            assert isinstance(value, str)
    
    print("✓ All mappings loaded successfully")

# Test UTF-8 Chinese character support
def test_chinese_characters_preserved():
    """Verify Chinese characters are preserved in mappings."""
    branch_mapping = load_company_branch()
    
    # Check Chinese keys are preserved
    assert "内蒙" in branch_mapping
    assert "济南" in branch_mapping
    
    print("✓ Chinese characters preserved correctly")
```

## Final Validation Checklist
- [ ] All YAML files created with UTF-8 encoding and Chinese character support
- [ ] mapping_loader.py implements all 4 specific loader functions
- [ ] Optional schema.py validates current data_sources.yml successfully  
- [ ] All tests pass: `uv run pytest -v -k "(mapping_loader or data_sources_schema) and not postgres"`
- [ ] No linting errors: `uv run ruff check src/ --fix`
- [ ] No type errors: `uv run mypy src/`
- [ ] No EAST-related code added anywhere in implementation
- [ ] No DB/network dependencies introduced
- [ ] Chinese characters load correctly from YAML files
- [ ] Error handling provides clear, actionable messages

---

## Anti-Patterns to Avoid
- ❌ Don't implement or reference EAST anywhere - it's deprecated
- ❌ Don't introduce DB connections or network calls - pure config only
- ❌ Don't use sync/async patterns - this is pure synchronous config loading
- ❌ Don't add new dependencies - use existing PyYAML and Pydantic
- ❌ Don't skip UTF-8 encoding - critical for Chinese character support
- ❌ Don't hardcode file paths - use relative paths from config directory
- ❌ Don't ignore validation errors - provide clear, actionable messages

## Confidence Score: 9/10

High confidence due to:
- Clear, specific requirements from INITIAL.md
- Existing proven patterns to follow in file_connector.py
- Comprehensive external research on Pydantic + YAML best practices  
- Well-defined validation gates with exact commands
- Sample YAML content provided in requirements
- Thorough test patterns available in test_file_connector.py

Minor uncertainty only on optional schema implementation complexity, but core mapping loader is straightforward following existing patterns.