name: "Legacy Company ID Mapping Migration to Simplified Schema - P-030"
description: |

## Purpose
Migrate existing legacy system's 5-layer COMPANY_ID mapping logic (COMPANY_ID1-5_MAPPING) to a new simplified 2-table structure in the enterprise schema, ensuring 100% compatibility and maintaining all mapping relationships and priority logic.

## Core Principles
1. **Context is King**: Include ALL necessary documentation, examples, and caveats
2. **Validation Loops**: Provide executable tests/lints the AI can run and fix
3. **Information Dense**: Use keywords and patterns from the codebase
4. **Progressive Success**: Start simple, validate, then enhance
5. **Global rules**: Be sure to follow all rules in CLAUDE.md

---

## Goal
Create a unified company mapping system that converts the existing 5-layer legacy mapping structure (COMPANY_ID1_MAPPING through COMPANY_ID5_MAPPING) into a single PostgreSQL table (`enterprise.company_mapping`) while preserving all existing mapping relationships, priority logic, and ensuring 100% backward compatibility.

## Why
- **Business value**: Simplifies complex 5-layer mapping logic into a single, maintainable table structure
- **Integration**: Enables centralized company ID resolution with consistent priority-based lookup
- **Problems solved**: Eliminates scattered hardcoded mappings, reduces maintenance burden, and provides audit trail for company ID associations

## What
A complete migration system that:
- Extracts data from 5 legacy mapping sources (MySQL tables + hardcoded dictionaries)
- Transforms into unified schema with priority-based lookup
- Loads into PostgreSQL with proper indexing and constraints
- Provides CLI tools for import/export operations
- Maintains exact compatibility with existing `_update_company_id` logic

### Success Criteria
- [ ] All 5 legacy mapping layers migrate to `enterprise.company_mapping` table with 100% data fidelity
- [ ] Priority-based lookup logic matches legacy behavior exactly: plan → account → hardcode → customer name → account name
- [ ] CLI import tool supports both plan-only and execute modes
- [ ] Performance: Company ID resolution queries complete in <100ms
- [ ] Full unit test coverage for all mapping scenarios and edge cases
- [ ] Import statistics reporting (counts per mapping type, conflicts handled)

## All Needed Context

### Documentation & References
```yaml
# MUST READ - Include these in your context window
- file: legacy/annuity_hub/data_handler/mappings.py
  why: Complete legacy mapping implementation with MySQL queries and hardcoded dictionaries

- file: legacy/annuity_hub/data_handler/data_cleaner.py
  why: _update_company_id method shows exact priority logic to replicate (lines 123-156, 204-227)

- file: src/work_data_hub/domain/annuity_performance/models.py
  why: Pydantic v2 model patterns, validation, and field conventions to follow

- file: src/work_data_hub/io/loader/warehouse_loader.py
  why: PostgreSQL loading patterns, transaction handling, and error management

- file: src/work_data_hub/orchestration/ops.py
  why: CLI integration patterns, configuration models, and job structure

- file: CLAUDE.md
  why: Project architecture conventions, testing requirements, and code style rules

# Key legacy mapping structure reference
- pattern: COMPANY_ID1_MAPPING through COMPANY_ID5_MAPPING
  location: legacy/annuity_hub/data_handler/mappings.py:128-194
  logic: Lines 139-227 in data_cleaner.py show exact precedence order
```

### Implementation-Facing Research Notes
```yaml
sources:
  - file: legacy/annuity_hub/data_handler/data_cleaner.py#_update_company_id
    section: Lines 123-156 (generic method) and 204-227 (annuity specific)
    why: Exact priority logic implementation to replicate
    type: legacy_reference

tldr:
  - Legacy uses 5-layer fallback: 计划代码 → 集团企业客户号 → hardcoded → 客户名称 → 年金账户名
  - Each layer only applies if previous layers return null/empty
  - COMPANY_ID3_MAPPING is hardcoded dictionary, others from MySQL
  - Default fallback value is '600866980' for empty customer names
  - Priority determines search order, lower number = higher priority

setup_commands:
  - "# PostgreSQL schema creation"
  - "CREATE SCHEMA IF NOT EXISTS enterprise;"
  - "# Legacy MySQL connection for data extraction"
  - "# Will use existing MySqlDBManager from legacy code"

api_decisions:
  - name: resolve_company_id method signature
    choice: "resolve_company_id(plan_code: str, account_number: str, customer_name: str, account_name: str) -> Optional[str]"
    rationale: Matches legacy _update_company_id parameters and return type

  - name: table_structure
    choice: "compound_primary_key(alias_name, match_type)"
    rationale: Allows same alias across different match types while preventing duplicates

  - name: priority_field
    choice: "integer_priority_1_to_5"
    rationale: Explicit priority for ORDER BY queries, matches legacy layer numbering

versions:
  - library: pydantic
    constraint: ">=2.0.0,<3.0.0"
    compatibility: "matches existing domain models in codebase"
  - library: psycopg2-binary
    constraint: ">=2.9.0"
    compatibility: "existing warehouse_loader.py dependency"

pitfalls_and_mitigations:
  - issue: "MySQL to PostgreSQL type differences (especially VARCHAR lengths)"
    mitigation: "Use explicit length limits in Pydantic models matching DDL"
  - issue: "Legacy MySQL connection instability mentioned in feature spec"
    mitigation: "Implement retry logic with exponential backoff in data extraction"
  - issue: "Chinese characters in customer names need proper encoding"
    mitigation: "Use UTF-8 encoding throughout, test with sample Chinese company names"
  - issue: "Empty vs NULL handling differs between MySQL and PostgreSQL"
    mitigation: "Normalize empty strings to None in Pydantic models before validation"

open_questions:
  - "Should we maintain audit trail of mapping changes or just current state?"
  - "How to handle mapping conflicts if same alias maps to different company_ids?"
```

### Current Codebase Tree
```bash
src/work_data_hub/
  domain/
    annuity_performance/
      models.py              # Pydantic v2 patterns to follow
      service.py             # Pure transformation functions
  io/
    loader/
      warehouse_loader.py    # PostgreSQL loading with transaction handling
  orchestration/
    ops.py                   # CLI integration and configuration models
    jobs.py                  # Job composition and CLI entry points
  config/
    settings.py              # Environment configuration
    data_sources.yml         # Table/PK binding configuration

legacy/annuity_hub/
  data_handler/
    mappings.py              # Source mapping data and MySQL queries
    data_cleaner.py          # _update_company_id priority logic
```

### Desired Codebase Tree with Files to be Added
```bash
src/work_data_hub/
  domain/
    company_enrichment/                 # NEW: Company ID mapping domain
      __init__.py                      # Package init
      models.py                        # CompanyMappingRecord, CompanyMappingService
      service.py                       # resolve_company_id, data transformation logic
  io/
    loader/
      company_mapping_loader.py        # NEW: Specialized loader for mapping data
  scripts/
    migrate_company_mappings.py        # NEW: Standalone migration script
  sql/
    ddl/
      company_mapping.sql              # NEW: Table creation DDL

tests/
  domain/
    test_company_enrichment.py         # NEW: Full domain testing
  io/
    test_company_mapping_loader.py     # NEW: Loader testing
  fixtures/
    sample_legacy_mappings.json        # NEW: Test data fixtures
```

### Known Gotchas & Library Quirks
```python
# CRITICAL: Legacy MySQL connection handling from mappings.py
# MySqlDBManager context manager must be used properly with try/finally
# Connection may fail - implement retry logic

# CRITICAL: Pydantic v2 model validation differences from v1
# Use ConfigDict instead of Config class, field_validator instead of validator

# CRITICAL: Legacy _update_company_id logic has specific precedence rules
# COMPANY_ID1_MAPPING (plan) -> COMPANY_ID2_MAPPING (account) -> COMPANY_ID3_MAPPING (hardcode)
# -> COMPANY_ID4_MAPPING (customer name) -> COMPANY_ID5_MAPPING (account name)
# Each step only executes if previous steps return null/empty

# CRITICAL: PostgreSQL identifier quoting in warehouse_loader.py
# Use quote_ident() for table/column names, especially with Chinese characters

# CRITICAL: Default fallback value handling
# Legacy uses '600866980' as default when customer_name is null/empty
# This must be preserved in migration logic

# CRITICAL: Chinese character encoding in customer names
# Ensure UTF-8 handling throughout the pipeline
```

## Implementation Blueprint

### Data models and structure

Create domain-specific models ensuring type safety and validation consistency with existing patterns.

```python
# domain/company_enrichment/models.py - Core data structures
from pydantic import BaseModel, Field, ConfigDict
from typing import Literal, Optional
from datetime import datetime, UTC

class CompanyMappingRecord(BaseModel):
    """
    Input/output model for company ID mapping records.

    Maps directly to enterprise.company_mapping table DDL.
    """
    model_config = ConfigDict(
        str_strip_whitespace=True,
        validate_default=True,
        from_attributes=True,
    )

    alias_name: str = Field(..., max_length=255, description="Source identifier (plan code, account, name)")
    canonical_id: str = Field(..., max_length=50, description="Target company_id")
    source: Literal["internal"] = "internal"
    match_type: Literal["plan", "account", "hardcode", "name", "account_name"] = Field(
        ..., description="Mapping type determining priority"
    )
    priority: int = Field(..., ge=1, le=5, description="Search priority (1=highest)")
    updated_at: datetime = Field(default_factory=lambda: datetime.now(UTC))

class CompanyMappingQuery(BaseModel):
    """Input model for company ID resolution queries."""
    plan_code: Optional[str] = None
    account_number: Optional[str] = None
    customer_name: Optional[str] = None
    account_name: Optional[str] = None

class CompanyResolutionResult(BaseModel):
    """Result model for company ID resolution."""
    company_id: Optional[str]
    match_type: Optional[str]  # Which mapping type provided the result
    source_value: Optional[str]  # The alias_name that matched
```

### List of tasks to be completed to fulfill the PRP in the order they should be completed

```yaml
Task 1: Create Core Domain Models
CREATE src/work_data_hub/domain/company_enrichment/__init__.py:
  - Empty package init file

CREATE src/work_data_hub/domain/company_enrichment/models.py:
  - MIRROR pattern from: src/work_data_hub/domain/annuity_performance/models.py
  - CompanyMappingRecord, CompanyMappingQuery, CompanyResolutionResult models
  - Use Pydantic v2 ConfigDict pattern from existing models
  - Field validation for Chinese characters and length limits

Task 2: Create Company Mapping Service
CREATE src/work_data_hub/domain/company_enrichment/service.py:
  - PATTERN: Pure functions like annuity_performance/service.py
  - implement resolve_company_id() matching legacy _update_company_id logic
  - Priority-based search algorithm (1=plan, 2=account, 3=hardcode, 4=name, 5=account_name)
  - Unit-testable transformation functions

Task 3: Create Database DDL
CREATE src/work_data_hub/sql/ddl/company_mapping.sql:
  - PostgreSQL table creation script
  - Compound primary key (alias_name, match_type)
  - Priority-based index for fast lookups
  - Proper VARCHAR lengths and UTF-8 support

Task 4: Create Specialized Data Loader
CREATE src/work_data_hub/io/loader/company_mapping_loader.py:
  - PATTERN: Use existing warehouse_loader.py functions
  - Legacy MySQL extraction from mappings.py
  - Data transformation and validation
  - PostgreSQL loading with transaction safety

Task 5: Create Migration Script
CREATE src/work_data_hub/scripts/migrate_company_mappings.py:
  - Standalone migration script for ops use
  - Extract from all 5 legacy mapping sources
  - Transform using domain models and service
  - Load using specialized loader

Task 6: Integrate with Orchestration System
MODIFY src/work_data_hub/orchestration/jobs.py:
  - Add import_company_mappings job
  - CLI argument parsing for plan-only/execute modes
  - Configuration integration with existing patterns

UPDATE src/work_data_hub/config/data_sources.yml:
  - Add company_mapping table configuration
  - Primary key definition for warehouse operations

Task 7: Create Comprehensive Tests
CREATE tests/domain/test_company_enrichment.py:
  - Unit tests for all service functions
  - Edge cases: null/empty values, Chinese characters, priority logic
  - PATTERN: Follow existing test structure in tests/domain/

CREATE tests/io/test_company_mapping_loader.py:
  - Integration tests for data extraction and loading
  - Mock MySQL connections for test isolation
  - Verify data transformation accuracy

CREATE tests/fixtures/sample_legacy_mappings.json:
  - Representative test data covering all 5 mapping types
  - Chinese character edge cases
  - Null/empty value scenarios

Task 8: Integration and E2E Testing
CREATE tests/e2e/test_company_mapping_migration.py:
  - End-to-end migration testing
  - Performance validation (<100ms queries)
  - Data consistency verification
```

### Per Task Pseudocode

```python
# Task 2: Company Mapping Service Implementation
def resolve_company_id(
    mappings: List[CompanyMappingRecord],
    query: CompanyMappingQuery
) -> CompanyResolutionResult:
    """
    Resolve company ID using priority-based lookup exactly matching legacy logic.

    Priority order (matches legacy _update_company_id):
    1. plan_code -> COMPANY_ID1_MAPPING
    2. account_number -> COMPANY_ID2_MAPPING
    3. hardcoded mappings -> COMPANY_ID3_MAPPING
    4. customer_name -> COMPANY_ID4_MAPPING
    5. account_name -> COMPANY_ID5_MAPPING
    """
    # PATTERN: Sort by priority for deterministic search
    sorted_mappings = sorted(mappings, key=lambda m: m.priority)

    # Build lookup by match_type for O(1) access
    lookup = {}
    for mapping in sorted_mappings:
        if mapping.match_type not in lookup:
            lookup[mapping.match_type] = {}
        lookup[mapping.match_type][mapping.alias_name] = mapping

    # Priority-based search (matches legacy precedence)
    search_order = [
        ("plan", query.plan_code),
        ("account", query.account_number),
        ("hardcode", query.plan_code),  # Hardcode uses plan_code as key
        ("name", query.customer_name),
        ("account_name", query.account_name)
    ]

    for match_type, search_value in search_order:
        if search_value and match_type in lookup:
            if search_value in lookup[match_type]:
                mapping = lookup[match_type][search_value]
                return CompanyResolutionResult(
                    company_id=mapping.canonical_id,
                    match_type=match_type,
                    source_value=search_value
                )

    # CRITICAL: Default fallback matching legacy behavior
    # Legacy returns '600866980' when customer_name is empty
    if not query.customer_name:
        return CompanyResolutionResult(
            company_id="600866980",
            match_type="default",
            source_value=None
        )

    return CompanyResolutionResult(company_id=None, match_type=None, source_value=None)

# Task 4: Legacy Data Extraction
def extract_legacy_mappings() -> List[CompanyMappingRecord]:
    """Extract from all 5 legacy sources with retry logic."""
    mappings = []

    # PATTERN: Retry logic for unstable MySQL connections
    @retry(attempts=3, backoff=exponential)
    def safe_extract(query_func, mapping_type, priority):
        try:
            raw_data = query_func()
            for alias, company_id in raw_data.items():
                # CRITICAL: Handle null/empty values like legacy
                if alias and company_id:
                    mappings.append(CompanyMappingRecord(
                        alias_name=alias,
                        canonical_id=company_id,
                        match_type=mapping_type,
                        priority=priority,
                        source="internal"
                    ))
        except Exception as e:
            logger.warning(f"MySQL extraction failed for {mapping_type}: {e}")
            raise

    # Extract from all 5 sources (matches mappings.py structure)
    safe_extract(get_company_id1_mapping, "plan", 1)
    safe_extract(get_company_id2_mapping, "account", 2)
    safe_extract(get_company_id4_mapping, "name", 4)
    safe_extract(get_company_id5_mapping, "account_name", 5)

    # CRITICAL: Hardcoded COMPANY_ID3_MAPPING (from mappings.py:148-158)
    hardcoded = {
        'FP0001': '614810477', 'FP0002': '614810477', 'FP0003': '610081428',
        'P0809': '608349737', 'SC002': '604809109', 'SC007': '602790403',
        'XNP466': '603968573', 'XNP467': '603968573', 'XNP596': '601038164'
    }
    for alias, company_id in hardcoded.items():
        mappings.append(CompanyMappingRecord(
            alias_name=alias,
            canonical_id=company_id,
            match_type="hardcode",
            priority=3,
            source="internal"
        ))

    return mappings
```

### Integration Points
```yaml
DATABASE:
  - table: "enterprise.company_mapping"
  - schema: |
      CREATE TABLE enterprise.company_mapping (
          alias_name VARCHAR(255) NOT NULL,
          canonical_id VARCHAR(50) NOT NULL,
          source VARCHAR(20) DEFAULT 'internal',
          match_type VARCHAR(20) NOT NULL,
          priority INTEGER NOT NULL,
          updated_at TIMESTAMPTZ DEFAULT now(),
          PRIMARY KEY (alias_name, match_type)
      );
      CREATE INDEX idx_company_mapping_priority ON enterprise.company_mapping (priority, alias_name);

CONFIG:
  - add to: src/work_data_hub/config/data_sources.yml
  - section: |
      company_mapping:
        table: company_mapping
        pk: ["alias_name", "match_type"]
        schema: enterprise

ENV_VARS:
  - WDH_COMPANY_MAPPING_ENABLED: "Feature flag for gradual rollout"
  - WDH_DATABASE__URI: "Reuse existing PostgreSQL connection"

CLI_COMMANDS:
  - command: |
      uv run python -m src.work_data_hub.orchestration.jobs --job import_company_mappings --plan-only
      uv run python -m src.work_data_hub.orchestration.jobs --job import_company_mappings --execute
```

## Validation Loop

### Level 1: Syntax & Style
```bash
# Run these FIRST - fix any errors before proceeding
uv run ruff check src/work_data_hub/domain/company_enrichment/ --fix
uv run mypy src/work_data_hub/domain/company_enrichment/
uv run ruff check src/work_data_hub/io/loader/company_mapping_loader.py --fix
uv run mypy src/work_data_hub/io/loader/company_mapping_loader.py

# Expected: No errors. If errors, READ the error and fix.
```

### Level 2: Unit Tests
```python
# tests/domain/test_company_enrichment.py
def test_priority_based_resolution():
    """Test that priority order matches legacy _update_company_id exactly."""
    mappings = [
        CompanyMappingRecord(alias_name="AN001", canonical_id="614810477", match_type="plan", priority=1),
        CompanyMappingRecord(alias_name="GM123456", canonical_id="608349737", match_type="account", priority=2),
        CompanyMappingRecord(alias_name="测试企业A", canonical_id="614810477", match_type="name", priority=4),
    ]

    # Plan code should take priority over customer name
    query = CompanyMappingQuery(plan_code="AN001", customer_name="测试企业A")
    result = resolve_company_id(mappings, query)

    assert result.company_id == "614810477"
    assert result.match_type == "plan"
    assert result.source_value == "AN001"

def test_chinese_character_handling():
    """Test proper handling of Chinese company names."""
    mappings = [CompanyMappingRecord(
        alias_name="中国平安保险股份有限公司",
        canonical_id="614810477",
        match_type="name",
        priority=4
    )]

    query = CompanyMappingQuery(customer_name="中国平安保险股份有限公司")
    result = resolve_company_id(mappings, query)

    assert result.company_id == "614810477"

def test_default_fallback_logic():
    """Test default fallback when customer_name is empty."""
    mappings = []
    query = CompanyMappingQuery(customer_name="")
    result = resolve_company_id(mappings, query)

    # Should match legacy behavior
    assert result.company_id == "600866980"
    assert result.match_type == "default"

def test_null_vs_empty_handling():
    """Test proper handling of null vs empty string values."""
    mappings = []

    # None should trigger default
    query = CompanyMappingQuery(customer_name=None)
    result = resolve_company_id(mappings, query)
    assert result.company_id == "600866980"

    # Empty string should also trigger default
    query = CompanyMappingQuery(customer_name="")
    result = resolve_company_id(mappings, query)
    assert result.company_id == "600866980"
```

```bash
# Run tests iteratively until passing:
uv run pytest tests/domain/test_company_enrichment.py -v
uv run pytest tests/io/test_company_mapping_loader.py -v

# If failing: Read error, understand root cause, fix code, re-run
```

### Level 3: Integration Test
```bash
# Test end-to-end migration
uv run python -m src.work_data_hub.scripts.migrate_company_mappings --plan-only

# Expected output:
# Migration Plan:
# - Extract from 5 legacy sources: COMPANY_ID1-5_MAPPING
# - Transform: 1,234 total mappings
#   - plan: 45 mappings (priority 1)
#   - account: 156 mappings (priority 2)
#   - hardcode: 9 mappings (priority 3)
#   - name: 890 mappings (priority 4)
#   - account_name: 134 mappings (priority 5)
# - Load to enterprise.company_mapping

# Execute migration
uv run python -m src.work_data_hub.scripts.migrate_company_mappings --execute

# Test CLI integration
uv run python -m src.work_data_hub.orchestration.jobs --job import_company_mappings --plan-only

# Test resolution performance
uv run python -c "
from src.work_data_hub.domain.company_enrichment.service import resolve_company_id
import time
# Performance test should complete in <100ms
"
```

## Final Validation Checklist
- [ ] All tests pass: `uv run pytest tests/ -k company_mapping -v`
- [ ] No linting errors: `uv run ruff check src/work_data_hub/domain/company_enrichment/`
- [ ] No type errors: `uv run mypy src/work_data_hub/domain/company_enrichment/`
- [ ] Migration plan-only mode works: CLI shows extraction counts
- [ ] Migration execute mode works: Data loads to PostgreSQL
- [ ] Priority logic matches legacy exactly: All unit tests pass
- [ ] Chinese character handling: UTF-8 encoding throughout
- [ ] Performance meets requirements: <100ms query resolution
- [ ] CLI integration works: Jobs system recognizes new command
- [ ] Data consistency: 100% of legacy mappings migrate successfully

---

## Anti-Patterns to Avoid
- ❌ Don't modify legacy code - create new domain parallel to existing
- ❌ Don't change priority order - must match legacy exactly
- ❌ Don't skip transaction handling - use warehouse_loader patterns
- ❌ Don't ignore Chinese character encoding - test with real names
- ❌ Don't hardcode connection strings - use existing config patterns
- ❌ Don't skip the plan-only mode - ops team needs preview capability
- ❌ Don't forget default fallback logic - '600866980' must be preserved

## Confidence Score: 9/10

High confidence due to:
- Clear legacy reference implementation to replicate exactly
- Well-established patterns in codebase to follow
- Comprehensive validation gates covering all scenarios
- Detailed task breakdown with specific implementation guidance
- Strong focus on data consistency and backward compatibility

Minor uncertainty around MySQL connection stability, but retry patterns mitigate this risk.