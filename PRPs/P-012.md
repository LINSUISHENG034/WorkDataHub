name: "Fix Trustee Performance E2E Execute Mode - DB Context, JSONB, Decimal Precision"
description: |

## Goal
Ensure the trustee_performance Dagster job runs end-to-end against PostgreSQL with `--execute` mode, fixing three critical defects: psycopg2 connection context recursion, JSONB parameter adaptation, and Decimal precision validation causing row drops.

## Why
- **Business value**: Enables production ETL execution with database persistence instead of plan-only mode
- **Integration**: Completes the trustee performance data pipeline for operational use
- **Problems solved**: Eliminates blocking errors preventing database loading of processed trustee performance data

## What
Fix the execution path in `uv run python -m src.work_data_hub.orchestration.jobs --execute --max-files 2` by resolving:
- Connection lifecycle management preventing recursive psycopg2 context manager usage
- JSONB parameter adaptation for PostgreSQL compatibility 
- Decimal input quantization to match schema precision requirements

### Success Criteria
- [ ] Execute mode completes without `ProgrammingError: connection cannot be re-entered recursively`
- [ ] JSONB parameters (validation_warnings) are correctly adapted without "can't adapt type" errors
- [ ] Decimal inputs are quantized to schema precision preventing `decimal_max_places` validation failures
- [ ] All validation gates pass (ruff, mypy, pytest) and E2E execute succeeds

## All Needed Context

### Documentation & References
```yaml
# MUST READ - Include these in your context window
- url: https://docs.pydantic.dev/2.11/usage/validators/#field-validators
  why: Pydantic v2 field validators and FieldValidationInfo for quantization by field name
  
- url: https://docs.python.org/3/library/decimal.html#decimal.Decimal.quantize
  why: Decimal quantization patterns and rounding modes to avoid float precision issues
  
- url: https://www.psycopg.org/docs/extras.html#json-adaptation
  why: psycopg2.extras.Json adapter for JSONB parameter conversion
  
- url: https://www.psycopg.org/docs/connection.html#connection
  why: Connection context manager behavior and transaction management patterns

- file: src/work_data_hub/orchestration/ops.py:342
  why: Current connection lifecycle creating recursion - uses `with psycopg2.connect(dsn) as conn:`
  
- file: src/work_data_hub/io/loader/warehouse_loader.py:283
  why: Transaction management layer - uses `with conn:` causing recursion
  
- file: src/work_data_hub/domain/trustee_performance/models.py:159-194
  why: Current clean_decimal_fields validator - needs quantization extension
  
- file: scripts/create_table/trustee_performance.sql:12-14,18
  why: Schema precision requirements - NUMERIC(8,6), NUMERIC(18,4), NUMERIC(18,2), JSONB column
```

### Current Codebase Structure
```bash
src/work_data_hub/
  orchestration/
    ops.py               # load_op opens psycopg2 connection (ISSUE: nested context)
    jobs.py              # CLI and run_config (works fine)
  io/
    loader/warehouse_loader.py  # uses `with conn:` and execute_values (ISSUE: recursion + JSONB)
    readers/excel_reader.py      # working correctly
  domain/trustee_performance/
    models.py            # Pydantic v2; decimal_places set (ISSUE: no quantization)
    service.py           # transformation; model_dump() yields dict for warnings
config/
  data_sources.yml       # table/pk = trustee_performance (working)
scripts/create_table/
  trustee_performance.sql   # schema with NUMERIC(8,6), NUMERIC(18,4), NUMERIC(18,2), JSONB
tests/
  io/test_warehouse_loader.py    # existing patterns for testing SQL builders
  domain/trustee_performance/test_service.py  # existing patterns for model testing
```

### Known Gotchas & Library Quirks
```python
# CRITICAL: psycopg2 context managers cannot be nested on same connection
# ops.py: `with psycopg2.connect(dsn) as conn:` + warehouse_loader: `with conn:` = recursion error

# CRITICAL: psycopg2.extras.execute_values cannot adapt dict/list to JSONB automatically  
# Must wrap with psycopg2.extras.Json() before execute_values call

# CRITICAL: Pydantic v2 decimal_places validation fails on float precision tail
# 0.048799999999999996 (float) fails decimal_places=6 - need quantization in field_validator

# CRITICAL: Use Decimal(str(value)) not Decimal(float_value) to avoid precision issues
# Always convert float to string first before Decimal conversion

# CRITICAL: FieldValidationInfo.field_name in Pydantic v2 provides field context for quantization map
# Different fields need different decimal places: return_rate=6, net_asset_value=4, fund_scale=2

# CRITICAL: Follow CLAUDE.md - KISS principle, use existing test patterns, validate with ruff/mypy
```

## Implementation Blueprint

### Data models and structure

The core issue is in the validation and parameter adaptation chain:
```python
# Current flow causing issues:
# 1. Excel data -> TrusteePerformanceIn (flexible input)
# 2. Domain transformation -> TrusteePerformanceOut (strict validation)  
# 3. model.model_dump() -> dict (contains list for validation_warnings)
# 4. execute_values with dict/list params -> JSONB adaptation failure
# 5. float precision tail -> decimal_max_places validation failure

# Target flow after fixes:
# 1. Excel data -> TrusteePerformanceIn (flexible input) 
# 2. Domain transformation -> TrusteePerformanceOut (quantized decimals)
# 3. model.model_dump() -> dict (contains list for validation_warnings)
# 4. JSONB parameter adaptation -> execute_values success
# 5. Clean connection lifecycle -> no recursion errors
```

### List of tasks to be completed

```yaml
Task 1: Fix Connection Lifecycle in ops.py
MODIFY src/work_data_hub/orchestration/ops.py:
  - FIND pattern: "with psycopg2.connect(dsn) as conn:" (line ~342)
  - REPLACE with bare connection: "conn = psycopg2.connect(dsn)"
  - ADD finally block with "conn.close()" for cleanup
  - PRESERVE plan_only path unchanged (no connection created)
  - CRITICAL: Let warehouse_loader.py manage transaction with "with conn:"

Task 2: Add JSONB Parameter Adaptation in warehouse_loader.py
MODIFY src/work_data_hub/io/loader/warehouse_loader.py:
  - FIND pattern: execute_values call with row_data (line ~309-314)
  - ADD helper function "_adapt_param(v)" before execute_values
  - INJECT Json wrapper: "from psycopg2.extras import Json, execute_values"
  - PATTERN: wrap dict/list values with Json() adapter
  - PRESERVE plan_only sql_plans unchanged (readability)

Task 3: Extend Decimal Quantization in models.py  
MODIFY src/work_data_hub/domain/trustee_performance/models.py:
  - FIND pattern: "clean_decimal_fields" validator (line ~159-194)
  - ADD quantization using field_name mapping with FieldValidationInfo
  - INJECT precision map: {"return_rate": 6, "net_asset_value": 4, "fund_scale": 2}
  - PRESERVE existing cleaning logic (%, currency removal)
  - CRITICAL: Use Decimal(str(value)) before quantize() to avoid float precision

Task 4: Add Comprehensive Tests
CREATE test cases following existing patterns:
  - EXTEND tests/io/test_warehouse_loader.py with JSONB adaptation tests
  - EXTEND tests/domain/trustee_performance/test_service.py with quantization tests
  - ADD optional @pytest.mark.postgres tests for live DB validation
  - PATTERN: Mock external dependencies, test error cases, ensure 80%+ coverage
```

### Per task pseudocode

```python
# Task 1: Connection Lifecycle Fix
@op
def load_op(context, config, processed_rows):
    # CRITICAL: Replace nested context managers
    try:
        conn = None
        if not config.plan_only:
            import psycopg2
            settings = get_settings()
            dsn = settings.get_database_connection_string()
            
            # PATTERN: Bare connection, let warehouse_loader manage transaction
            conn = psycopg2.connect(dsn)
            
            result = load(
                table=config.table,
                rows=processed_rows, 
                mode=config.mode,
                pk=config.pk,
                conn=conn,  # Pass connection, don't manage transaction here
            )
        else:
            # Plan-only path unchanged
            result = load(..., conn=None)
    finally:
        # CRITICAL: Always clean up connection
        if conn is not None:
            conn.close()

# Task 2: JSONB Parameter Adaptation
def _adapt_param(v):
    """Adapt dict/list parameters for JSONB columns."""
    # PATTERN: Wrap non-scalar types for psycopg2 JSONB adaptation
    if isinstance(v, (dict, list)):
        return Json(v)  # psycopg2.extras.Json
    return v

def load(table, rows, mode, pk, conn):
    # ... existing logic ...
    
    # CRITICAL: Before execute_values, adapt parameters
    from psycopg2.extras import Json, execute_values
    
    # Convert flattened params back to rows and adapt
    row_data = [
        [_adapt_param(val) for val in params[i:i+cols_per_row]]
        for i in range(0, len(params), cols_per_row)
    ]
    
    execute_values(cursor, sql, row_data, page_size=chunk_size)

# Task 3: Decimal Quantization  
from decimal import Decimal, ROUND_HALF_UP
from pydantic import FieldValidationInfo

# CRITICAL: Field-specific precision map matching schema
DECIMAL_PLACES = {
    "return_rate": 6,        # NUMERIC(8,6)
    "net_asset_value": 4,    # NUMERIC(18,4)  
    "fund_scale": 2          # NUMERIC(18,2)
}

@field_validator("return_rate", "net_asset_value", "fund_scale", mode="before")
@classmethod
def clean_decimal_fields(cls, v, info: FieldValidationInfo):
    if v is None or v == "":
        return None
        
    # PRESERVE existing cleaning (%, currency removal)
    # ... existing cleaning logic ...
    
    # CRITICAL: Convert to Decimal via string to avoid float precision
    if isinstance(v, float):
        d = Decimal(str(v))  # Avoid float precision tail
    elif isinstance(v, str):
        d = Decimal(v.strip())
    else:
        d = Decimal(str(v))
    
    # PATTERN: Quantize based on field name using FieldValidationInfo
    places = DECIMAL_PLACES.get(info.field_name)
    if places is not None:
        quantizer = Decimal("1." + ("0" * places))
        d = d.quantize(quantizer, rounding=ROUND_HALF_UP)
    
    return d
```

### Integration Points
```yaml
DATABASE:
  - schema: Uses existing trustee_performance table (no DDL changes)
  - precision: NUMERIC(8,6), NUMERIC(18,4), NUMERIC(18,2) match quantization
  - jsonb: validation_warnings JSONB column with Json() adapter
  
CONFIG:
  - env: Uses existing .env -> Settings.get_database_connection_string()
  - no new vars needed, connection management improvement only
  
TESTING:
  - unit: Extend existing test patterns in tests/io/ and tests/domain/
  - integration: Optional @pytest.mark.postgres for live DB testing
  - e2e: Both --plan-only and --execute modes must succeed
```

## Validation Loop

### Level 1: Syntax & Style
```bash
# Run these FIRST - fix any errors before proceeding
uv run ruff check src/ --fix              # Auto-fix style issues  
uv run mypy src/                          # Type checking

# Expected: No errors. If errors, READ and understand, then fix.
```

### Level 2: Unit Tests
```python
# test_warehouse_loader.py - Add JSONB adaptation tests
def test_adapt_param_wraps_dict_and_list():
    """Test parameter adaptation for JSONB types."""
    from psycopg2.extras import Json
    
    dict_param = {"key": "value"}
    list_param = ["item1", "item2"]
    scalar_param = "scalar"
    
    assert isinstance(_adapt_param(dict_param), Json)
    assert isinstance(_adapt_param(list_param), Json)  
    assert _adapt_param(scalar_param) == "scalar"

def test_load_adapts_jsonb_parameters():
    """Test that load function properly adapts JSONB parameters."""
    rows = [{"validation_warnings": ["warning1", "warning2"]}]
    result = load("test_table", rows, mode="append", conn=None)
    # Verify sql_plans contain adapted parameters
    
# test_service.py - Add decimal quantization tests  
def test_decimal_quantization_by_field_name():
    """Test field-specific decimal quantization."""
    # Test return_rate quantization (6 places)
    model = TrusteePerformanceOut(
        report_date="2024-01-01",
        plan_code="P001", 
        company_code="C001",
        return_rate=0.048799999999999996,  # Float precision tail
        data_source="test"
    )
    assert model.return_rate == Decimal("0.048800")  # Quantized to 6 places
    
    # Test net_asset_value quantization (4 places)  
    model.net_asset_value = 1.23456789
    assert model.net_asset_value == Decimal("1.2346")  # Quantized to 4 places
```

```bash
# Run tests iteratively until passing:
uv run pytest -v

# If failing: Debug specific test, understand root cause, fix code, re-run
```

### Level 3: Integration Test - E2E Validation
```bash
# Test plan-only mode (should work without DB)
uv run python -m src.work_data_hub.orchestration.jobs --plan-only --max-files 2

# Expected: Success with sql_plans generated, no connection errors

# Test execute mode (requires reachable PostgreSQL)
uv run python -m src.work_data_hub.orchestration.jobs --execute --max-files 2

# Expected: 
# - Success with database insertion
# - No "connection cannot be re-entered recursively" errors
# - No "can't adapt type list/dict" errors  
# - No "decimal_max_places" validation errors on valid precision inputs
# - Processed rows match inserted count

# Check database state
psql -c "SELECT COUNT(*), MAX(processed_at) FROM trustee_performance;"
```

## Final Validation Checklist
- [ ] All tests pass: `uv run pytest -v`
- [ ] No linting errors: `uv run ruff check src/`
- [ ] No type errors: `uv run mypy src/` 
- [ ] Plan-only mode succeeds: `uv run python -m src.work_data_hub.orchestration.jobs --plan-only --max-files 2`
- [ ] Execute mode succeeds: `uv run python -m src.work_data_hub.orchestration.jobs --execute --max-files 2`
- [ ] Database contains expected records with correct precision
- [ ] JSONB validation_warnings field populated correctly
- [ ] Connection cleanup verified (no connection leaks)

---

## Anti-Patterns to Avoid
- ❌ Don't nest psycopg2 connection context managers - causes recursion errors
- ❌ Don't pass dict/list directly to execute_values for JSONB - wrap with Json()  
- ❌ Don't construct Decimal from float directly - use Decimal(str(float)) 
- ❌ Don't quantize all decimal fields the same - use field-specific precision
- ❌ Don't skip connection cleanup in finally blocks - causes connection leaks
- ❌ Don't modify plan_only code paths - preserve existing behavior

## Confidence Score: 9/10

High confidence due to:
- Clear error reproduction and root cause analysis from VALIDATION.md
- Well-established psycopg2 and Pydantic v2 patterns from research
- Existing test infrastructure to validate changes
- Comprehensive context including schema, current code, and external documentation
- Specific error messages and line numbers for targeted fixes

Minor uncertainty only on Windows-specific psycopg2 behavior, but patterns are standard across platforms.