name: "DDL Generator: MySQL JSON to PostgreSQL DDL for Chinese Tables"
description: |

## Purpose
Create a robust DDL generator that converts MySQL table definitions from JSON to PostgreSQL DDL, preserving Chinese identifiers and providing staged FK application for the 规模明细 (Annuity Performance) table.

## Core Principles
1. **Context is King**: Include ALL necessary documentation, examples, and caveats
2. **Validation Loops**: Provide executable tests/lints the AI can run and fix
3. **Information Dense**: Use keywords and patterns from the codebase
4. **Progressive Success**: Start simple, validate, then enhance
5. **Global rules**: Be sure to follow all rules in CLAUDE.md

---

## Goal
Generate faithful PostgreSQL DDL from `reference/db_migration/db_structure.json` for the "规模明细" table, preserving Chinese identifiers with proper quoting, accurate type mapping, and optional foreign key staging.

## Why
- **Business value**: Enables local development and testing with real Chinese table structure
- **Integration**: Supports database migration and schema validation workflows  
- **Problems solved**: Eliminates manual DDL conversion and ensures consistency with source schema

## What
A command-line generator script that:
- Parses MySQL table definitions from JSON
- Converts types to PostgreSQL equivalents
- Quotes Chinese table/column names properly
- Generates indexes and foreign keys
- Provides staging options for missing referenced tables

### Success Criteria
- [ ] Generator script creates valid PostgreSQL DDL for 规模明细 table
- [ ] Generated DDL applies successfully to local PostgreSQL instance
- [ ] Chinese identifiers properly quoted and preserved
- [ ] Foreign keys optionally staged when referenced tables missing
- [ ] Type mapping handles all MySQL types in source JSON correctly

## All Needed Context

### Documentation & References
```yaml
# MUST READ - Include these in your context window
- file: scripts/create_table/apply_sql.py
  why: Pattern for PostgreSQL connection, DSN handling, UTF-8 file reading
  
- file: scripts/create_table/trustee_performance.sql  
  why: Example of Chinese comments, quoted identifiers, and DDL structure
  
- file: reference/db_migration/db_structure.json (lines 1444-1715)
  why: Source JSON structure for 规模明细 table definition
  
- file: src/work_data_hub/config/settings.py
  why: Database connection configuration pattern

- url: https://docs.sqlalchemy.org/en/20/dialects/postgresql.html#column-types
  why: PostgreSQL type system and best practices for type mapping
```

### Implementation-Facing Research Notes
```yaml
sources:
  - file: reference/db_migration/db_structure.json
    section: lines 1444-1715 (规模明细 table)
    why: Complete table structure with columns, indexes, foreign keys
    type: source_data
  
  - pattern: scripts/create_table/apply_sql.py  
    section: UTF-8 handling, DSN sanitization, psycopg2 usage
    why: Established patterns for SQL file handling and DB connection
    type: codebase_pattern

tldr:
  - MySQL COLLATE clauses must be stripped for PostgreSQL compatibility
  - Chinese identifiers require double quotes in PostgreSQL DDL
  - DOUBLE type maps to 'double precision' in PostgreSQL
  - Foreign keys may reference non-existent tables, need staging option
  - UTF-8 encoding critical throughout for Chinese character handling

type_mappings:
  - mysql: "INTEGER" 
    postgres: "INTEGER"
    rationale: Direct equivalent
  - mysql: "VARCHAR(n) COLLATE \"utf8_general_ci\""
    postgres: "VARCHAR(n)"
    rationale: Strip COLLATE clause, preserve length
  - mysql: "DATE"
    postgres: "DATE" 
    rationale: Direct equivalent
  - mysql: "DOUBLE"
    postgres: "double precision"
    rationale: PostgreSQL equivalent for MySQL DOUBLE

quoting_rules:
  - table_names: 'Always quote Chinese table names: "规模明细"'
  - column_names: 'Always quote Chinese column names: "月度", "业务类型"'
  - index_names: 'Quote Chinese index names for consistency'

json_structure:
  - path: '["business"]["规模明细"]'
    contains: '["columns", "indexes", "foreign_keys"]'
    primary_key: 'Inferred from unique index on "id" column'

pitfalls_and_mitigations:
  - issue: Chinese characters in identifiers
    mitigation: Always use double quotes and UTF-8 encoding
  - issue: Foreign keys to non-existent tables
    mitigation: Provide --include-fk flag for optional FK generation
  - issue: MySQL COLLATE clauses invalid in PostgreSQL
    mitigation: Strip all COLLATE clauses during type conversion
  - issue: Primary key not explicitly marked in JSON
    mitigation: Infer from unique indexes or use id column by convention

open_questions:
  - Should id column be auto-incrementing (SERIAL) or plain INTEGER?
  - How to handle composite primary keys if present?
  - Should we include table comments explaining the source JSON?
```

### Current Codebase tree
```bash
scripts/
├── create_table/
│   ├── apply_sql.py              # SQL application utility
│   └── trustee_performance.sql   # Example DDL with Chinese identifiers
└── dev/
    └── setup_test_schema.sql     # Development schema setup

reference/
└── db_migration/
    └── db_structure.json         # Source MySQL table definitions

src/work_data_hub/config/
└── settings.py                   # Database configuration
```

### Desired Codebase tree with files to be added
```bash
scripts/
├── create_table/
│   ├── apply_sql.py              # Existing
│   └── trustee_performance.sql   # Existing
└── dev/
    ├── gen_postgres_ddl_from_json.py  # NEW: DDL generator script
    ├── annuity_performance_real.sql   # GENERATED: Output DDL file
    └── setup_test_schema.sql          # Existing
```

### Known Gotchas & Library Quirks
```python
# CRITICAL: Chinese identifiers must be quoted in PostgreSQL DDL
table_name = f'"{table_name}"'  # "规模明细"
column_name = f'"{column_name}"'  # "月度"

# CRITICAL: MySQL COLLATE clauses are incompatible with PostgreSQL
mysql_type = 'VARCHAR(255) COLLATE "utf8_general_ci"'
postgres_type = 'VARCHAR(255)'  # Strip COLLATE

# CRITICAL: UTF-8 encoding must be explicit for JSON reading
with open(json_path, 'r', encoding='utf-8') as f:
    data = json.load(f)

# CRITICAL: Foreign key references may not exist yet
# Use --include-fk flag to make FK generation optional

# CRITICAL: Primary key inference from unique indexes
# Look for unique=true indexes to identify PK candidates
```

## Implementation Blueprint

### Data models and structure

Create type mapping and DDL generation utilities ensuring proper Chinese character handling and PostgreSQL compatibility.

```python
# Type mapping utilities
TYPE_MAPPINGS = {
    'INTEGER': 'INTEGER',
    'DATE': 'DATE', 
    'DOUBLE': 'double precision',
    # VARCHAR with COLLATE pattern handled separately
}

# DDL generation models
from typing import Dict, List, Optional
from dataclasses import dataclass

@dataclass
class ColumnDef:
    name: str
    type: str  
    nullable: bool
    default: Optional[str]
    primary_key: bool

@dataclass  
class IndexDef:
    name: str
    columns: List[str]
    unique: bool

@dataclass
class ForeignKeyDef:
    name: str
    constrained_columns: List[str] 
    referred_table: str
    referred_columns: List[str]
```

### List of tasks to be completed

```yaml
Task 1: Create DDL Generator Script Foundation
CREATE scripts/dev/gen_postgres_ddl_from_json.py:
  - PATTERN: Follow scripts/create_table/apply_sql.py for CLI arg parsing
  - Use argparse with --table, --json, --out, --include-fk arguments
  - Handle UTF-8 encoding explicitly for JSON reading
  - Include comprehensive docstring with usage examples

Task 2: Implement Type Mapping System  
ADD to gen_postgres_ddl_from_json.py:
  - FUNCTION: mysql_to_postgres_type(mysql_type: str) -> str
  - PATTERN: Handle VARCHAR with COLLATE regex matching
  - MAP all types found in 规模明细 JSON structure
  - PRESERVE column constraints (nullable, default values)

Task 3: Implement Table DDL Generation
ADD to gen_postgres_ddl_from_json.py:
  - FUNCTION: generate_table_ddl(table_name: str, table_def: dict) -> str
  - PATTERN: Quote Chinese identifiers like trustee_performance.sql
  - INFER primary key from unique indexes if not explicit
  - GENERATE column definitions with proper type mapping

Task 4: Implement Index Generation
ADD to gen_postgres_ddl_from_json.py:
  - FUNCTION: generate_indexes(table_name: str, indexes: list) -> str  
  - PATTERN: CREATE INDEX statements with quoted Chinese names
  - HANDLE unique and non-unique indexes separately
  - QUOTE column names in index definitions

Task 5: Implement Foreign Key Generation
ADD to gen_postgres_ddl_from_json.py:
  - FUNCTION: generate_foreign_keys(table_name: str, fks: list) -> str
  - PATTERN: ALTER TABLE ADD CONSTRAINT format
  - CONDITIONAL generation based on --include-fk flag
  - HANDLE referenced table name quoting

Task 6: Implement Main CLI Orchestration  
ADD to gen_postgres_ddl_from_json.py:
  - FUNCTION: main() -> int
  - PATTERN: Error handling like apply_sql.py  
  - LOAD JSON and navigate to specified table
  - COMBINE all DDL components into output file
  - INCLUDE header comments explaining source and usage

Task 7: Generate 规模明细 DDL
EXECUTE generator to create actual DDL:
  - RUN: python scripts/dev/gen_postgres_ddl_from_json.py --table 规模明细 --json reference/db_migration/db_structure.json --out scripts/dev/annuity_performance_real.sql --include-fk
  - VERIFY: Generated DDL contains all expected columns, indexes, FKs
  - CHECK: Chinese identifiers properly quoted throughout

Task 8: Test DDL Application
VALIDATE generated DDL applies correctly:
  - TEST: Apply DDL with apply_sql.py --dry-run first
  - EXECUTE: Apply DDL to local PostgreSQL instance  
  - VERIFY: Table created successfully with proper structure
  - TEST: Both with and without --include-fk flag
```

### Per task pseudocode

```python
# Task 2: Type Mapping Implementation
def mysql_to_postgres_type(mysql_type: str) -> str:
    """Convert MySQL type to PostgreSQL equivalent."""
    # PATTERN: Strip COLLATE clauses with regex
    import re
    cleaned_type = re.sub(r'\s+COLLATE\s+"[^"]*"', '', mysql_type)
    
    # GOTCHA: Handle VARCHAR length preservation
    if cleaned_type.startswith('VARCHAR'):
        return cleaned_type  # Keep as-is after COLLATE removal
    
    # PATTERN: Direct mapping for common types
    type_mappings = {
        'INTEGER': 'INTEGER',
        'DATE': 'DATE',
        'DOUBLE': 'double precision',
        'TEXT': 'TEXT'
    }
    
    return type_mappings.get(cleaned_type, cleaned_type)

# Task 3: Table DDL Generation  
def generate_table_ddl(table_name: str, table_def: dict) -> str:
    """Generate CREATE TABLE statement with Chinese identifiers."""
    # CRITICAL: Quote Chinese table name
    quoted_table = f'"{table_name}"'
    
    lines = [f'CREATE TABLE IF NOT EXISTS {quoted_table} (']
    
    # PATTERN: Generate column definitions
    for col in table_def['columns']:
        col_name = f'"{col["name"]}"'  # Quote Chinese column names
        col_type = mysql_to_postgres_type(col['type'])
        nullable = '' if col['nullable'] else ' NOT NULL'
        
        lines.append(f'  {col_name:<20} {col_type}{nullable},')
    
    # GOTCHA: Infer primary key from unique indexes
    pk_columns = infer_primary_key(table_def.get('indexes', []))
    if pk_columns:
        pk_def = ', '.join(f'"{col}"' for col in pk_columns)
        lines.append(f'  CONSTRAINT pk_{table_name} PRIMARY KEY ({pk_def})')
    
    lines.append(');')
    return '\n'.join(lines)

# Task 5: Foreign Key Generation
def generate_foreign_keys(table_name: str, fks: list) -> str:
    """Generate ALTER TABLE statements for foreign keys.""" 
    if not fks:
        return ''
    
    lines = ['-- Foreign Key Constraints']
    quoted_table = f'"{table_name}"'
    
    for fk in fks:
        # CRITICAL: Quote Chinese table and column names
        constraint_name = fk['name']
        local_cols = ', '.join(f'"{col}"' for col in fk['constrained_columns'])
        ref_table = f'"{fk["referred_table"]}"'
        ref_cols = ', '.join(f'"{col}"' for col in fk['referred_columns'])
        
        fk_sql = f'''ALTER TABLE {quoted_table} 
    ADD CONSTRAINT {constraint_name} 
    FOREIGN KEY ({local_cols}) 
    REFERENCES {ref_table}({ref_cols});'''
        
        lines.append(fk_sql)
    
    return '\n'.join(lines)
```

### Integration Points
```yaml
DATABASE:
  - connection: Uses existing settings.py configuration
  - application: Via scripts/create_table/apply_sql.py utility
  
FILES:
  - input: reference/db_migration/db_structure.json
  - output: scripts/dev/annuity_performance_real.sql
  - pattern: Follow scripts/create_table/trustee_performance.sql format
  
COMMANDS:
  - generation: python scripts/dev/gen_postgres_ddl_from_json.py
  - application: python -m scripts.create_table.apply_sql --sql <file>
  - validation: uv run ruff check src/ && uv run mypy src/
```

## Validation Loop

### Level 1: Syntax & Style
```bash
# Run these FIRST - fix any errors before proceeding  
uv run ruff check scripts/dev/gen_postgres_ddl_from_json.py --fix
uv run mypy scripts/dev/gen_postgres_ddl_from_json.py

# Expected: No errors. If errors, READ and fix.
```

### Level 2: Functional Testing
```bash
# Test DDL generation
uv run python scripts/dev/gen_postgres_ddl_from_json.py \
  --table 规模明细 \
  --json reference/db_migration/db_structure.json \
  --out scripts/dev/annuity_performance_real.sql \
  --include-fk

# Verify output file contains expected DDL
cat scripts/dev/annuity_performance_real.sql

# Test without FKs
uv run python scripts/dev/gen_postgres_ddl_from_json.py \
  --table 规模明细 \
  --json reference/db_migration/db_structure.json \
  --out scripts/dev/annuity_performance_real_no_fk.sql

# Expected: Valid DDL files generated with proper Chinese quoting
```

### Level 3: Integration Test  
```bash
# Test DDL application (dry run first)
uv run python -m scripts.create_table.apply_sql \
  --sql scripts/dev/annuity_performance_real_no_fk.sql \
  --dry-run

# Apply DDL to local PostgreSQL
uv run python -m scripts.create_table.apply_sql \
  --sql scripts/dev/annuity_performance_real_no_fk.sql

# Verify table creation
# Connect to PostgreSQL and run: \d "规模明细"
# Expected: Table exists with proper structure and Chinese column names
```

## Final Validation Checklist
- [ ] All syntax/style checks pass: `uv run ruff check scripts/dev/ && uv run mypy scripts/dev/`
- [ ] DDL generator script runs without errors
- [ ] Generated DDL file contains CREATE TABLE with quoted Chinese identifiers  
- [ ] All column types properly mapped from MySQL to PostgreSQL
- [ ] Indexes generated with proper Chinese name quoting
- [ ] Foreign keys conditionally generated based on --include-fk flag
- [ ] DDL applies successfully to local PostgreSQL instance
- [ ] Table structure matches source JSON definition
- [ ] Script includes comprehensive usage documentation
- [ ] Error handling graceful for malformed JSON or missing tables

---

## Anti-Patterns to Avoid
- ❌ Don't hardcode table names - use CLI arguments  
- ❌ Don't forget UTF-8 encoding for Chinese characters
- ❌ Don't skip quoting Chinese identifiers in DDL
- ❌ Don't ignore COLLATE clauses - strip them for PostgreSQL
- ❌ Don't assume primary keys exist - infer from unique indexes
- ❌ Don't force foreign key creation - make it optional via flag
- ❌ Don't generate unreadable DDL - include comments and proper formatting

## Confidence Score: 9/10

High confidence due to:
- Clear source JSON structure already analyzed
- Existing patterns to follow from apply_sql.py and trustee_performance.sql
- Well-defined MySQL to PostgreSQL type mappings
- Straightforward DDL generation requirements  
- Established database connection and application patterns
- Comprehensive validation approach with executable commands

Minor uncertainty around edge cases in JSON structure navigation and primary key inference, but clear fallback strategies provided.