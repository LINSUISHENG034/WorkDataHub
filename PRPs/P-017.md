name: "DB Connection Semantics Alignment (ops ↔ loader)"
description: |

## Purpose
Align database connection and transaction semantics between Dagster ops and warehouse loader to ensure consistent behavior across connection lifecycle scenarios, resolve failing connection-related tests, and maintain session consistency for E2E scenarios.

## Core Principles
1. **Context is King**: Include ALL necessary documentation, examples, and caveats
2. **Validation Loops**: Provide executable tests/lints the AI can run and fix
3. **Information Dense**: Use keywords and patterns from the codebase
4. **Progressive Success**: Start simple, validate, then enhance
5. **Global rules**: Be sure to follow all rules in CLAUDE.md

---

## Goal
Unify database connection/transaction/session semantics between `ops.load_op` and `loader.load` to ensure all connection lifecycle test scenarios pass, eliminate connection-related failures, and maintain TEMP table session visibility for E2E tests.

## Why
- **Business value**: Eliminates flaky database connection failures that block production deployments
- **Integration**: Ensures ops layer and loader layer have consistent connection handling
- **Problems solved**: Resolves connection cleanup, error classification, and session management issues

## What
Surgical alignment of connection handling in `src/work_data_hub/orchestration/ops.py` to work seamlessly with existing transaction management in `src/work_data_hub/io/loader/warehouse_loader.py`.

### Success Criteria
- [ ] All TestLoadOpConnectionLifecycle tests pass
- [ ] E2E integration tests pass (TEMP table visibility maintained)
- [ ] Connection failures produce correct error messages
- [ ] plan_only mode never establishes connections
- [ ] Resource cleanup works in both success and failure scenarios
- [ ] No mypy/ruff errors introduced

## All Needed Context

### Documentation & References
```yaml
# MUST READ - Include these in your context window
- url: https://www.psycopg.org/docs/connection.html
  why: Official psycopg2 connection object documentation and lifecycle
  
- url: https://www.psycopg.org/docs/usage.html#with-statement
  why: Context manager behavior for transactions vs connections
  
- file: src/work_data_hub/orchestration/ops.py
  why: Current implementation with complex connection handling logic
  
- file: src/work_data_hub/io/loader/warehouse_loader.py  
  why: Correct transaction handling pattern with 'with conn:' blocks
  
- file: tests/orchestration/test_ops.py
  why: TestLoadOpConnectionLifecycle contains all expected behaviors
  
- file: tests/e2e/test_trustee_performance_e2e.py
  why: Integration tests requiring session consistency for TEMP tables

- file: src/work_data_hub/config/settings.py
  why: DSN resolution patterns and fallback mechanisms
```

### Current Codebase tree (relevant sections)
```bash
src/work_data_hub/
├── orchestration/
│   └── ops.py                    # MODIFY: load_op connection handling
├── io/
│   └── loader/
│       └── warehouse_loader.py   # VERIFY: transaction semantics correct
└── config/
    └── settings.py              # REFERENCE: DSN building patterns

tests/
├── orchestration/
│   └── test_ops.py              # TARGET: TestLoadOpConnectionLifecycle
└── e2e/
    └── test_trustee_performance_e2e.py  # TARGET: Integration tests
```

### Current Problem Analysis
The current `ops.py` load_op function attempts to support both context manager and bare connection patterns simultaneously, leading to:

1. **Inconsistent behavior**: Different code paths for connections with/without `__enter__`/`__exit__`
2. **Resource leaks**: Complex cleanup logic that fails in edge cases
3. **Test conflicts**: Some tests expect bare connections, others expect context managers
4. **Session breaks**: TEMP tables become invisible due to connection management issues

### Known Gotchas & Library Quirks
```python
# CRITICAL: psycopg2 connection vs transaction context managers
# Connection context manager (with psycopg2.connect(...) as conn:) handles connection.close()
# Transaction context manager (with conn:) handles commit/rollback only
# warehouse_loader.py already uses 'with conn:' for transaction management

# CRITICAL: Mock compatibility for tests
# Some tests use builtins.__import__ mocks, others use direct patching
# Dynamic import at function level required: import psycopg2

# CRITICAL: DSN resolution fallback
# Primary: settings.get_database_connection_string()
# Fallback: settings.database.get_connection_string() for test compatibility
# Must validate result is string and non-empty

# CRITICAL: Error message consistency
# Connection failures: DataWarehouseLoaderError("Database connection failed: <original>")
# Load failures: Preserve original exception (e.g., "Load operation failed")
# Missing module: DataWarehouseLoaderError("psycopg2 not available...")
```

## Implementation Blueprint

### Data models and structure
No new models required. Work with existing:
```python
# Existing models used:
- LoadConfig (from ops.py)
- DataWarehouseLoaderError (from warehouse_loader.py)
- Settings (from config/settings.py)
```

### List of tasks to be completed in order

```yaml
Task 1: Analyze current ops.load_op implementation
EXAMINE src/work_data_hub/orchestration/ops.py:
  - IDENTIFY the dual connection handling paths (lines ~350-380)
  - NOTE the existing DSN resolution logic
  - UNDERSTAND the current error handling approach

Task 2: Study warehouse_loader.load transaction semantics  
EXAMINE src/work_data_hub/io/loader/warehouse_loader.py:
  - VERIFY the 'with conn:' transaction pattern (lines ~290-335)
  - CONFIRM it expects caller to manage connection lifecycle
  - UNDERSTAND why it doesn't close connections

Task 3: Analyze failing test expectations
EXAMINE tests/orchestration/test_ops.py TestLoadOpConnectionLifecycle:
  - IDENTIFY expected connection cleanup patterns
  - NOTE error message format expectations  
  - UNDERSTAND context manager vs bare connection test scenarios

Task 4: Simplify ops.load_op connection handling
MODIFY src/work_data_hub/orchestration/ops.py load_op function:
  - REMOVE dual connection path complexity
  - STANDARDIZE on bare connection approach (no context manager in ops)
  - PRESERVE DSN resolution fallback logic
  - ENSURE proper finally block cleanup

Task 5: Align error handling and messages
MODIFY src/work_data_hub/orchestration/ops.py load_op function:
  - MAP only psycopg2.connect failures to DataWarehouseLoaderError("Database connection failed: ...")
  - PRESERVE original exceptions from load() calls
  - HANDLE ImportError for missing psycopg2 module

Task 6: Test connection lifecycle scenarios
RUN tests/orchestration/test_ops.py:
  - VERIFY TestLoadOpConnectionLifecycle passes
  - CHECK connection cleanup in success/failure cases
  - VALIDATE error message formats

Task 7: Test E2E integration
RUN tests/e2e/test_trustee_performance_e2e.py:
  - VERIFY test_database_connection_lifecycle_integration passes
  - VERIFY test_database_transaction_rollback_on_error passes  
  - CONFIRM TEMP table session visibility maintained
```

### Per task pseudocode

```python
# Task 4: Simplified load_op connection handling
@op
def load_op(context, config, processed_rows):
    conn = None
    try:
        if not config.plan_only:
            # Dynamic import for test mock compatibility
            import psycopg2
            
            # DSN resolution with fallback (existing pattern)
            dsn = _get_dsn_with_fallback()  # Extract existing logic
            
            # CRITICAL: Only catch psycopg2.connect failures
            try:
                conn = psycopg2.connect(dsn)  # Bare connection, no context manager
            except Exception as e:
                raise DataWarehouseLoaderError(f"Database connection failed: {e}") from e
            
            # Call loader - it handles transactions with 'with conn:'
            result = load(table=config.table, rows=processed_rows, 
                         mode=config.mode, pk=config.pk, conn=conn)
        else:
            # Plan-only: no connection created
            result = load(table=config.table, rows=processed_rows,
                         mode=config.mode, pk=config.pk, conn=None)
        
        # Log and return - DON'T catch load() exceptions
        return result
        
    finally:
        # CRITICAL: Clean up bare connection in finally
        if conn is not None:
            conn.close()

# Task 5: Error handling alignment
def _get_dsn_with_fallback():
    """Extract existing DSN resolution logic"""
    settings = get_settings()
    
    # Primary method
    if hasattr(settings, "get_database_connection_string"):
        try:
            dsn = settings.get_database_connection_string()
            if isinstance(dsn, str) and dsn:
                return dsn
        except Exception:
            pass
    
    # Fallback for test compatibility
    if hasattr(settings, "database"):
        try:
            dsn = settings.database.get_connection_string()
            if isinstance(dsn, str) and dsn:
                return dsn
        except Exception:
            pass
    
    raise DataWarehouseLoaderError(
        "Database connection failed: invalid DSN resolved from settings"
    )
```

### Integration Points
```yaml
NO NEW INTEGRATIONS:
  - Keep existing settings.py DSN patterns
  - Preserve loader.load transaction semantics
  - Maintain test mock compatibility patterns
  
PRESERVE EXISTING:
  - All other ops functionality unchanged
  - All loader functionality unchanged  
  - All config/settings patterns unchanged
```

## Validation Loop

### Level 1: Syntax & Style
```bash
# Run these FIRST - fix any errors before proceeding
uv run ruff check src/work_data_hub/orchestration/ops.py --fix
uv run mypy src/work_data_hub/orchestration/ops.py

# Expected: No new errors. If errors, READ the error and fix.
```

### Level 2: Unit Tests - Connection Lifecycle
```bash
# Target the specific failing test class
uv run pytest tests/orchestration/test_ops.py::TestLoadOpConnectionLifecycle -v

# Expected: All connection lifecycle tests pass
# Key tests that MUST pass:
# - test_load_op_uses_bare_connection_not_context_manager
# - test_load_op_connection_cleanup_on_success  
# - test_load_op_connection_cleanup_on_load_failure
# - test_load_op_connection_cleanup_on_connection_failure
# - test_load_op_plan_only_mode_no_connection_created

# If failing: Read the specific test failure, understand expected vs actual behavior
```

### Level 3: E2E Integration Tests
```bash
# Test session consistency and transaction handling
uv run pytest tests/e2e/test_trustee_performance_e2e.py::test_database_connection_lifecycle_integration -v
uv run pytest tests/e2e/test_trustee_performance_e2e.py::test_database_transaction_rollback_on_error -v

# Expected: Both integration tests pass
# These validate TEMP table visibility (same session) and proper rollback
```

### Level 4: Full Regression  
```bash
# Ensure no new failures introduced
uv run pytest -v

# Expected: No new test failures, existing pass rate maintained or improved
```

## Final validation Checklist
- [ ] All TestLoadOpConnectionLifecycle tests pass
- [ ] E2E integration tests pass (session consistency maintained)  
- [ ] Connection error messages match expected format
- [ ] plan_only mode creates no connections
- [ ] Connection cleanup works in finally blocks
- [ ] No new mypy/ruff errors: `uv run mypy src/ && uv run ruff check src/`
- [ ] DSN resolution fallback still works
- [ ] Load operation exceptions preserved (not wrapped)

---

## Anti-Patterns to Avoid
- ❌ Don't create new connection patterns when warehouse_loader.py already has correct transaction handling
- ❌ Don't wrap load() exceptions in DataWarehouseLoaderError - preserve original messages
- ❌ Don't use context managers in ops.load_op when loader already handles transactions  
- ❌ Don't change warehouse_loader.py - it's working correctly
- ❌ Don't break existing DSN fallback mechanisms for test compatibility
- ❌ Don't ignore connection cleanup in finally blocks - resource leaks are critical

## Confidence Score: 9/10

This PRP provides comprehensive context for database connection lifecycle management, specific error handling requirements, and detailed validation steps that match the acceptance criteria in C-023. The implementation focuses on surgical changes to align semantics without breaking existing functionality.