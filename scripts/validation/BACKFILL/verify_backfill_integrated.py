"""
Generic Backfill Framework - Integrated Verification Script (Enhanced)

éªŒè¯ç›®æ ‡ï¼š
1. é…ç½®Schemaå…¼å®¹æ€§ - Pydanticæ¨¡å‹è§£æ
2. çœŸæ­£çš„æ‹“æ‰‘æ’åº - åŸºäºdepends_onçš„DAGæ’åº
3. å¾ªç¯ä¾èµ–æ£€æµ‹ - æ£€æµ‹å¹¶æŠ¥é”™
4. å…¨éƒ¨4ä¸ªFKè¦†ç›– - å¹´é‡‘è®¡åˆ’ã€ç»„åˆè®¡åˆ’ã€äº§å“çº¿ã€ç»„ç»‡æ¶æ„
5. æ•°æ®æ¥æºè¿½è¸ªå­—æ®µ - _source, _needs_review, _derived_from_domain, _derived_at
6. å¤§æ•°æ®é›†æ€§èƒ½åŸºå‡† - 10000è¡Œæµ‹è¯•

Created: 2025-12-12
Enhanced: 2025-12-12 (PM Review)
"""

import logging
import time
from datetime import datetime
from graphlib import CycleError, TopologicalSorter
from typing import List, Literal

import pandas as pd
from pydantic import BaseModel, Field, ValidationError
from sqlalchemy import Boolean, Column, DateTime, MetaData, String, Table, create_engine

# è®¾ç½®æ—¥å¿—æ ¼å¼
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - [BackfillVerify] - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# ==========================================
# PART 1: é…ç½®æ¨¡å‹ (Pydantic)
# ==========================================


class BackfillColumnMapping(BaseModel):
    source: str = Field(..., description="Fact data column")
    target: str = Field(..., description="Reference table column")
    optional: bool = Field(default=False)


class ForeignKeyConfig(BaseModel):
    name: str
    source_column: str
    target_table: str
    target_key: str
    backfill_columns: List[BackfillColumnMapping]
    mode: Literal["insert_missing", "fill_null_only"] = "insert_missing"
    priority: int = 1  # Deprecated: use depends_on for ordering
    depends_on: List[str] = Field(default_factory=list)


class DomainForeignKeysConfig(BaseModel):
    foreign_keys: List[ForeignKeyConfig] = Field(default_factory=list)


# ==========================================
# PART 2: é€šç”¨å›å¡«æœåŠ¡åŸå‹ (Enhanced)
# ==========================================


class GenericBackfillServicePrototype:
    def __init__(self, engine, domain: str = "unknown"):
        self.engine = engine
        self.domain = domain

    def _topological_sort(
        self, configs: List[ForeignKeyConfig]
    ) -> List[ForeignKeyConfig]:
        """
        çœŸæ­£çš„æ‹“æ‰‘æ’åº - åŸºäº depends_on å­—æ®µ
        ä½¿ç”¨ Python 3.9+ graphlib.TopologicalSorter
        """
        name_map = {c.name: c for c in configs}

        # æ„å»ºä¾èµ–å›¾
        graph = {}
        for c in configs:
            graph[c.name] = set(c.depends_on)

        # éªŒè¯ä¾èµ–å­˜åœ¨æ€§
        for name, deps in graph.items():
            for dep in deps:
                if dep not in name_map:
                    raise ValueError(
                        f"Foreign key '{name}' depends on unknown key '{dep}'"
                    )

        # æ‹“æ‰‘æ’åº (ä¼šè‡ªåŠ¨æ£€æµ‹å¾ªç¯ä¾èµ–)
        sorter = TopologicalSorter(graph)
        try:
            sorted_names = list(sorter.static_order())
        except CycleError as e:
            raise ValueError(f"Circular dependency detected: {e}")

        return [name_map[name] for name in sorted_names]

    def derive_candidates(
        self, df: pd.DataFrame, config: ForeignKeyConfig
    ) -> pd.DataFrame:
        """ä»äº‹å®è¡¨ä¸­æå–å€™é€‰æ•°æ®"""
        mapping = {m.source: m.target for m in config.backfill_columns}
        available_sources = [s for s in mapping.keys() if s in df.columns]

        if config.source_column not in df.columns:
            logger.warning(
                f"  [Skip] FK column '{config.source_column}' not found in data"
            )
            return pd.DataFrame()

        # æå–å¹¶å»é‡
        candidates = (
            df[available_sources]
            .drop_duplicates()
            .dropna(subset=[config.source_column])
        )
        candidates = candidates.rename(columns=mapping)

        return candidates

    def backfill_table(
        self,
        candidates: pd.DataFrame,
        config: ForeignKeyConfig,
        conn,
        add_tracking_fields: bool = True,
    ) -> int:
        """
        æ‰§è¡Œå›å¡«æ“ä½œ (å¸¦æ•°æ®æ¥æºè¿½è¸ª)
        Returns: æ’å…¥çš„è®°å½•æ•°
        """
        if candidates.empty:
            return 0

        table_name = config.target_table
        key_col = config.target_key

        # 1. æŸ¥å‡ºç°æœ‰ Key
        existing = pd.read_sql(f'SELECT "{key_col}" FROM "{table_name}"', conn)
        existing_keys = set(existing[key_col])

        # 2. è¿‡æ»¤å‡ºæ–° Key
        to_insert = candidates[~candidates[key_col].isin(existing_keys)].copy()

        if to_insert.empty:
            logger.info(f"  [No-Op] All keys for {table_name} already exist")
            return 0

        # 3. æ·»åŠ æ•°æ®æ¥æºè¿½è¸ªå­—æ®µ
        if add_tracking_fields:
            to_insert["_source"] = "auto_derived"
            to_insert["_needs_review"] = True
            to_insert["_derived_from_domain"] = self.domain
            to_insert["_derived_at"] = datetime.now()

        # 4. æ’å…¥
        logger.info(
            f"  [Insert] Inserting {len(to_insert)} new records into {table_name}"
        )
        to_insert.to_sql(table_name, conn, if_exists="append", index=False)

        return len(to_insert)

    def run(
        self,
        df: pd.DataFrame,
        configs: List[ForeignKeyConfig],
        conn,
        add_tracking_fields: bool = True,
    ) -> dict:
        """
        æ‰§è¡Œå®Œæ•´çš„å›å¡«æµç¨‹
        Returns: ç»Ÿè®¡ä¿¡æ¯
        """
        sorted_configs = self._topological_sort(configs)

        stats = {
            "total_inserted": 0,
            "tables_processed": [],
            "processing_order": [c.name for c in sorted_configs],
        }

        for config in sorted_configs:
            logger.info(f"Processing FK config: {config.name} -> {config.target_table}")
            candidates = self.derive_candidates(df, config)
            inserted = self.backfill_table(
                candidates, config, conn, add_tracking_fields
            )
            stats["total_inserted"] += inserted
            stats["tables_processed"].append(
                {"table": config.target_table, "inserted": inserted}
            )

        return stats


# ==========================================
# PART 3: æµ‹è¯•ç”¨ä¾‹
# ==========================================


class VerificationResult:
    def __init__(self, name: str):
        self.name = name
        self.passed = False
        self.message = ""
        self.details = {}

    def __str__(self):
        status = "âœ… PASSED" if self.passed else "âŒ FAILED"
        return f"{status} - {self.name}: {self.message}"


def test_config_schema_validation() -> VerificationResult:
    """æµ‹è¯•1: é…ç½®Schemaå…¼å®¹æ€§"""
    result = VerificationResult("Configuration Schema Validation")

    # å®Œæ•´çš„4ä¸ªFKé…ç½®
    full_fk_config = [
        {
            "name": "fk_plan",
            "source_column": "è®¡åˆ’ä»£ç ",
            "target_table": "å¹´é‡‘è®¡åˆ’",
            "target_key": "å¹´é‡‘è®¡åˆ’å·",
            "backfill_columns": [
                {"source": "è®¡åˆ’ä»£ç ", "target": "å¹´é‡‘è®¡åˆ’å·"},
                {"source": "è®¡åˆ’åç§°", "target": "è®¡åˆ’åç§°", "optional": True},
            ],
            "priority": 1,
        },
        {
            "name": "fk_portfolio",
            "source_column": "ç»„åˆä»£ç ",
            "target_table": "ç»„åˆè®¡åˆ’",
            "target_key": "ç»„åˆä»£ç ",
            "backfill_columns": [
                {"source": "ç»„åˆä»£ç ", "target": "ç»„åˆä»£ç "},
                {"source": "è®¡åˆ’ä»£ç ", "target": "å¹´é‡‘è®¡åˆ’å·"},
            ],
            "priority": 2,
            "depends_on": ["fk_plan"],
        },
        {
            "name": "fk_product_line",
            "source_column": "äº§å“çº¿ä»£ç ",
            "target_table": "äº§å“çº¿",
            "target_key": "äº§å“çº¿ä»£ç ",
            "backfill_columns": [
                {"source": "äº§å“çº¿ä»£ç ", "target": "äº§å“çº¿ä»£ç "},
                {"source": "äº§å“çº¿åç§°", "target": "äº§å“çº¿åç§°", "optional": True},
            ],
            "priority": 1,
        },
        {
            "name": "fk_organization",
            "source_column": "ç»„ç»‡ä»£ç ",
            "target_table": "ç»„ç»‡æ¶æ„",
            "target_key": "ç»„ç»‡ä»£ç ",
            "backfill_columns": [
                {"source": "ç»„ç»‡ä»£ç ", "target": "ç»„ç»‡ä»£ç "},
                {"source": "ç»„ç»‡åç§°", "target": "ç»„ç»‡åç§°", "optional": True},
            ],
            "priority": 1,
        },
    ]

    try:
        validated = [ForeignKeyConfig(**cfg) for cfg in full_fk_config]
        result.passed = True
        result.message = f"All {len(validated)} FK configs parsed successfully"
        result.details = {"config_count": len(validated)}
    except ValidationError as e:
        result.message = f"Validation failed: {e}"

    return result


def test_topological_sort() -> VerificationResult:
    """æµ‹è¯•2: çœŸæ­£çš„æ‹“æ‰‘æ’åº"""
    result = VerificationResult("Topological Sort (depends_on)")

    # åˆ›å»ºæœ‰ä¾èµ–å…³ç³»çš„é…ç½®
    configs = [
        ForeignKeyConfig(
            name="fk_child",
            source_column="child_id",
            target_table="child",
            target_key="id",
            backfill_columns=[BackfillColumnMapping(source="child_id", target="id")],
            depends_on=["fk_parent"],
        ),
        ForeignKeyConfig(
            name="fk_parent",
            source_column="parent_id",
            target_table="parent",
            target_key="id",
            backfill_columns=[BackfillColumnMapping(source="parent_id", target="id")],
            depends_on=["fk_grandparent"],
        ),
        ForeignKeyConfig(
            name="fk_grandparent",
            source_column="gp_id",
            target_table="grandparent",
            target_key="id",
            backfill_columns=[BackfillColumnMapping(source="gp_id", target="id")],
        ),
    ]

    engine = create_engine("sqlite:///:memory:")
    service = GenericBackfillServicePrototype(engine)

    try:
        sorted_configs = service._topological_sort(configs)
        sorted_names = [c.name for c in sorted_configs]

        # éªŒè¯é¡ºåº: grandparent -> parent -> child
        expected_order = ["fk_grandparent", "fk_parent", "fk_child"]

        if sorted_names == expected_order:
            result.passed = True
            result.message = f"Correct order: {' -> '.join(sorted_names)}"
        else:
            result.message = f"Wrong order: {sorted_names}, expected: {expected_order}"

        result.details = {"sorted_order": sorted_names}
    except Exception as e:
        result.message = f"Sort failed: {e}"

    return result


def test_circular_dependency_detection() -> VerificationResult:
    """æµ‹è¯•3: å¾ªç¯ä¾èµ–æ£€æµ‹"""
    result = VerificationResult("Circular Dependency Detection")

    # åˆ›å»ºå¾ªç¯ä¾èµ–é…ç½®
    configs = [
        ForeignKeyConfig(
            name="fk_a",
            source_column="a_id",
            target_table="table_a",
            target_key="id",
            backfill_columns=[BackfillColumnMapping(source="a_id", target="id")],
            depends_on=["fk_b"],
        ),
        ForeignKeyConfig(
            name="fk_b",
            source_column="b_id",
            target_table="table_b",
            target_key="id",
            backfill_columns=[BackfillColumnMapping(source="b_id", target="id")],
            depends_on=["fk_a"],  # å¾ªç¯!
        ),
    ]

    engine = create_engine("sqlite:///:memory:")
    service = GenericBackfillServicePrototype(engine)

    try:
        service._topological_sort(configs)
        result.message = "Failed to detect circular dependency!"
    except ValueError as e:
        if "Circular dependency" in str(e) or "cycle" in str(e).lower():
            result.passed = True
            result.message = "Correctly detected and raised error"
            result.details = {"error": str(e)}
        else:
            result.message = f"Wrong error type: {e}"
    except Exception as e:
        result.message = f"Unexpected error: {e}"

    return result


def test_all_four_fks() -> VerificationResult:
    """æµ‹è¯•4: å…¨éƒ¨4ä¸ªFKè¦†ç›–"""
    result = VerificationResult("All 4 FK Coverage")

    # åˆ›å»ºå†…å­˜æ•°æ®åº“
    engine = create_engine("sqlite:///:memory:")
    metadata = MetaData()

    # åˆ›å»º4ä¸ªå¼•ç”¨è¡¨ (å¸¦è¿½è¸ªå­—æ®µ)
    for table_name, key_col in [
        ("å¹´é‡‘è®¡åˆ’", "å¹´é‡‘è®¡åˆ’å·"),
        ("ç»„åˆè®¡åˆ’", "ç»„åˆä»£ç "),
        ("äº§å“çº¿", "äº§å“çº¿ä»£ç "),
        ("ç»„ç»‡æ¶æ„", "ç»„ç»‡ä»£ç "),
    ]:
        Table(
            table_name,
            metadata,
            Column(key_col, String, primary_key=True),
            Column("åç§°", String),
            Column("_source", String),
            Column("_needs_review", Boolean),
            Column("_derived_from_domain", String),
            Column("_derived_at", DateTime),
        )

    metadata.create_all(engine)

    # 4ä¸ªFKé…ç½®
    configs = [
        ForeignKeyConfig(
            name="fk_plan",
            source_column="è®¡åˆ’ä»£ç ",
            target_table="å¹´é‡‘è®¡åˆ’",
            target_key="å¹´é‡‘è®¡åˆ’å·",
            backfill_columns=[
                BackfillColumnMapping(source="è®¡åˆ’ä»£ç ", target="å¹´é‡‘è®¡åˆ’å·"),
                BackfillColumnMapping(source="è®¡åˆ’åç§°", target="åç§°", optional=True),
            ],
        ),
        ForeignKeyConfig(
            name="fk_portfolio",
            source_column="ç»„åˆä»£ç ",
            target_table="ç»„åˆè®¡åˆ’",
            target_key="ç»„åˆä»£ç ",
            backfill_columns=[
                BackfillColumnMapping(source="ç»„åˆä»£ç ", target="ç»„åˆä»£ç "),
                BackfillColumnMapping(source="ç»„åˆåç§°", target="åç§°", optional=True),
            ],
            depends_on=["fk_plan"],
        ),
        ForeignKeyConfig(
            name="fk_product_line",
            source_column="äº§å“çº¿ä»£ç ",
            target_table="äº§å“çº¿",
            target_key="äº§å“çº¿ä»£ç ",
            backfill_columns=[
                BackfillColumnMapping(source="äº§å“çº¿ä»£ç ", target="äº§å“çº¿ä»£ç "),
                BackfillColumnMapping(
                    source="äº§å“çº¿åç§°", target="åç§°", optional=True
                ),
            ],
        ),
        ForeignKeyConfig(
            name="fk_organization",
            source_column="ç»„ç»‡ä»£ç ",
            target_table="ç»„ç»‡æ¶æ„",
            target_key="ç»„ç»‡ä»£ç ",
            backfill_columns=[
                BackfillColumnMapping(source="ç»„ç»‡ä»£ç ", target="ç»„ç»‡ä»£ç "),
                BackfillColumnMapping(source="ç»„ç»‡åç§°", target="åç§°", optional=True),
            ],
        ),
    ]

    # æµ‹è¯•æ•°æ® (åŒ…å«æ‰€æœ‰4ä¸ªFKçš„å€¼ï¼Œæ¨¡æ‹ŸçœŸå®äº‹å®è¡¨)
    fact_data = pd.DataFrame(
        {
            "è®¡åˆ’ä»£ç ": ["PLAN_001", "PLAN_002", "PLAN_001"],
            "è®¡åˆ’åç§°": ["Plan A", "Plan B", "Plan A"],
            "ç»„åˆä»£ç ": ["PORT_001", "PORT_002", "PORT_001"],
            "ç»„åˆåç§°": ["Portfolio A", "Portfolio B", "Portfolio A"],
            "äº§å“çº¿ä»£ç ": ["PL_001", "PL_001", "PL_002"],
            "äº§å“çº¿åç§°": ["Product Line A", "Product Line A", "Product Line B"],
            "ç»„ç»‡ä»£ç ": ["ORG_001", "ORG_002", "ORG_003"],
            "ç»„ç»‡åç§°": ["Org A", "Org B", "Org C"],
        }
    )

    service = GenericBackfillServicePrototype(engine, domain="annuity_performance")

    with engine.connect() as conn:
        try:
            stats = service.run(fact_data, configs, conn)

            # éªŒè¯æ¯ä¸ªè¡¨éƒ½æœ‰æ•°æ®
            tables_with_data = 0
            for table_name in ["å¹´é‡‘è®¡åˆ’", "ç»„åˆè®¡åˆ’", "äº§å“çº¿", "ç»„ç»‡æ¶æ„"]:
                count = pd.read_sql(
                    f'SELECT COUNT(*) as cnt FROM "{table_name}"', conn
                ).iloc[0]["cnt"]
                if count > 0:
                    tables_with_data += 1

            if tables_with_data == 4:
                result.passed = True
                result.message = (
                    f"All 4 tables populated, {stats['total_inserted']} total records"
                )
                result.details = stats
            else:
                result.message = f"Only {tables_with_data}/4 tables have data"

        except Exception as e:
            result.message = f"Execution failed: {e}"
            import traceback

            traceback.print_exc()

    return result


def test_tracking_fields() -> VerificationResult:
    """æµ‹è¯•5: æ•°æ®æ¥æºè¿½è¸ªå­—æ®µ"""
    result = VerificationResult("Data Source Tracking Fields")

    engine = create_engine("sqlite:///:memory:")
    metadata = MetaData()

    Table(
        "test_table",
        metadata,
        Column("id", String, primary_key=True),
        Column("_source", String),
        Column("_needs_review", Boolean),
        Column("_derived_from_domain", String),
        Column("_derived_at", DateTime),
    )
    metadata.create_all(engine)

    config = ForeignKeyConfig(
        name="fk_test",
        source_column="test_id",
        target_table="test_table",
        target_key="id",
        backfill_columns=[BackfillColumnMapping(source="test_id", target="id")],
    )

    fact_data = pd.DataFrame({"test_id": ["TEST_001", "TEST_002"]})

    service = GenericBackfillServicePrototype(engine, domain="test_domain")

    with engine.connect() as conn:
        try:
            service.run(fact_data, [config], conn, add_tracking_fields=True)

            # éªŒè¯è¿½è¸ªå­—æ®µ
            records = pd.read_sql('SELECT * FROM "test_table"', conn)

            checks = {
                "_source": all(records["_source"] == "auto_derived"),
                "_needs_review": all(records["_needs_review"] == True),
                "_derived_from_domain": all(
                    records["_derived_from_domain"] == "test_domain"
                ),
                "_derived_at": all(records["_derived_at"].notna()),
            }

            if all(checks.values()):
                result.passed = True
                result.message = "All tracking fields correctly populated"
                result.details = {"record_count": len(records), "checks": checks}
            else:
                failed = [k for k, v in checks.items() if not v]
                result.message = f"Failed checks: {failed}"
                result.details = {"checks": checks}

        except Exception as e:
            result.message = f"Execution failed: {e}"
            import traceback

            traceback.print_exc()

    return result


def test_large_dataset_performance() -> VerificationResult:
    """æµ‹è¯•6: å¤§æ•°æ®é›†æ€§èƒ½åŸºå‡†"""
    result = VerificationResult("Large Dataset Performance (10K rows)")

    engine = create_engine("sqlite:///:memory:")
    metadata = MetaData()

    Table(
        "perf_table",
        metadata,
        Column("id", String, primary_key=True),
        Column("_source", String),
        Column("_needs_review", Boolean),
        Column("_derived_from_domain", String),
        Column("_derived_at", DateTime),
    )
    metadata.create_all(engine)

    config = ForeignKeyConfig(
        name="fk_perf",
        source_column="perf_id",
        target_table="perf_table",
        target_key="id",
        backfill_columns=[BackfillColumnMapping(source="perf_id", target="id")],
    )

    # ç”Ÿæˆ10000è¡Œæµ‹è¯•æ•°æ®
    row_count = 10000
    fact_data = pd.DataFrame({"perf_id": [f"PERF_{i:06d}" for i in range(row_count)]})

    service = GenericBackfillServicePrototype(engine, domain="perf_test")

    with engine.connect() as conn:
        try:
            start_time = time.time()
            stats = service.run(fact_data, [config], conn, add_tracking_fields=True)
            elapsed = time.time() - start_time

            # æ€§èƒ½åŸºå‡†: 10000è¡Œåº”åœ¨5ç§’å†…å®Œæˆ
            threshold_seconds = 5.0

            if elapsed < threshold_seconds and stats["total_inserted"] == row_count:
                result.passed = True
                result.message = f"{row_count} rows in {elapsed:.2f}s ({row_count / elapsed:.0f} rows/sec)"
            else:
                result.message = (
                    f"Too slow: {elapsed:.2f}s (threshold: {threshold_seconds}s)"
                )

            result.details = {
                "row_count": row_count,
                "elapsed_seconds": round(elapsed, 3),
                "rows_per_second": round(row_count / elapsed, 1),
                "inserted": stats["total_inserted"],
            }

        except Exception as e:
            result.message = f"Execution failed: {e}"
            import traceback

            traceback.print_exc()

    return result


# ==========================================
# PART 4: ä¸»éªŒè¯æµç¨‹
# ==========================================


def verify_integration():
    logger.info("=" * 60)
    logger.info("Generic Backfill Framework - Enhanced Verification")
    logger.info("=" * 60)

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        test_config_schema_validation,
        test_topological_sort,
        test_circular_dependency_detection,
        test_all_four_fks,
        test_tracking_fields,
        test_large_dataset_performance,
    ]

    results = []
    for test_func in tests:
        logger.info(f"\n>>> Running: {test_func.__doc__.strip()}")
        result = test_func()
        results.append(result)
        logger.info(str(result))
        if result.details:
            logger.info(f"    Details: {result.details}")

    # æ±‡æ€»
    logger.info("\n" + "=" * 60)
    logger.info("VERIFICATION SUMMARY")
    logger.info("=" * 60)

    passed = sum(1 for r in results if r.passed)
    total = len(results)

    for r in results:
        status = "âœ…" if r.passed else "âŒ"
        logger.info(f"  {status} {r.name}")

    logger.info("-" * 60)
    logger.info(f"  Total: {passed}/{total} tests passed")

    if passed == total:
        logger.info("\nğŸ‰ ALL VERIFICATIONS PASSED - Technical feasibility confirmed!")
        return True
    else:
        logger.info(f"\nâš ï¸  {total - passed} test(s) failed - Review required")
        return False


if __name__ == "__main__":
    success = verify_integration()
    exit(0 if success else 1)
