"""
Generate annuity_income seed data from Legacy MySQL for New Pipeline.

================================================================================
BACKGROUND & PURPOSE
================================================================================

This script generates seed data for the annuity_income domain by:
1. Exporting raw data from Legacy MySQL (business.æ”¶å…¥æ˜ç»†)
2. Cleaning the data to prepare for New Pipeline processing

The goal is to build seed data that goes through the SAME New Pipeline processing
as any new data, ensuring consistent data processing standards (å£å¾„) across
all data, whether legacy or new.

================================================================================
WHY CLEAN DATA BEFORE NEW PIPELINE?
================================================================================

The New Pipeline has upgraded data processing modules (e.g., customer_name_normalize)
that provide better data quality than Legacy processing. To ensure seed data
matches the same standards as new data, we need to:

1. Remove Legacy-processed fields (company_id, äº§å“çº¿ä»£ç )
2. Revert to original raw data (å¹´é‡‘è´¦æˆ·å vs å®¢æˆ·åç§°)
3. Reset default values (G00) to let New Pipeline re-determine

This ensures seed data and future data are processed with IDENTICAL logic,
eliminating discrepancies due to different processing standards.

================================================================================
DATA FLOW
================================================================================

Legacy MySQL (business.æ”¶å…¥æ˜ç»†)
    â†“ [Step 1: Export]
Raw CSV - preserves all original Legacy data
    â†“ [Step 2: Clean]
Cleaned CSV - Bronze layer input for New Pipeline
    â†“ [Step 3: New Pipeline ETL]
Standardized Silver/Gold data - seed data ready for use

================================================================================
TRANSFORMATION STEPS (Step 2: Clean)
================================================================================

Step 1: Handle Missing å¹´é‡‘è´¦æˆ·å
---------------------------------
Problem: Some records have missing/empty å¹´é‡‘è´¦æˆ·å due to data quality issues.
Solution: Fill missing å¹´é‡‘è´¦æˆ·å with å®¢æˆ·åç§° as fallback.
Rationale:
- å®¢æˆ·åç§° contains the same customer information but may have been cleaned
- This ensures we don't lose customer name data
- New Pipeline will re-normalize anyway, so we just need the raw value

Step 2: Replace å®¢æˆ·åç§° with å¹´é‡‘è´¦æˆ·å
-----------------------------------------
Problem: å®¢æˆ·åç§° in Legacy was already processed by Legacy cleaning logic.
Solution: Replace å®¢æˆ·åç§° with å¹´é‡‘è´¦æˆ·å (most original data).
Rationale:
- å¹´é‡‘è´¦æˆ·å contains the most original, unprocessed customer name
- New Pipeline's customer_name_normalize module will re-clean this data
- This ensures seed data uses the same cleaning standards as new data

Step 3: Remove å¹´é‡‘è´¦æˆ·å Column
----------------------------------
After Step 2, we have:
- å®¢æˆ·åç§°: now contains the original å¹´é‡‘è´¦æˆ·å data
- å¹´é‡‘è´¦æˆ·å: duplicate, can be removed

Step 4: Remove company_id and äº§å“çº¿ä»£ç 
-----------------------------------------
Problem: These fields were determined by Legacy logic.
Solution: Delete these columns entirely.
Rationale:
- New Pipeline will re-determine company_id via 5-step resolution strategy
- New Pipeline will re-map äº§å“çº¿ä»£ç  from ä¸šåŠ¡ç±»å‹
- Removing them ensures New Pipeline doesn't use Legacy values

Step 5: Reset G00 Institution Codes
------------------------------------
Problem: Legacy system defaults empty æœºæ„ä»£ç  to "G00" (headquarters).
Solution: Change "G00" values to NULL.
Rationale:
- "G00" is a default, not necessarily accurate
- New Pipeline can re-determine the correct institution code
- NULL allows New Pipeline to apply its own logic

================================================================================
USAGE
================================================================================

Full Pipeline (Export + Clean):
    PYTHONPATH=src uv run python scripts/seed_data/generate_annuity_income_original_data.py

Export Only:
    PYTHONPATH=src uv run python scripts/seed_data/generate_annuity_income_original_data.py --export-only

Clean Only:
    PYTHONPATH=src uv run python scripts/seed_data/generate_annuity_income_original_data.py --clean-only --input data/seed_data/annuity_income_raw.csv

With Limit (for testing):
    PYTHONPATH=src uv run python scripts/seed_data/generate_annuity_income_original_data.py --limit 10000

Custom Output Path:
    PYTHONPATH=src uv run python scripts/seed_data/generate_annuity_income_original_data.py -o data/my_output.csv
    # Creates: data/my_output.csv (cleaned) and data/my_output_raw.csv (raw)

Author: Seed Data Generator
Date: 2026-01-08
"""

import argparse
import os
from datetime import datetime
from pathlib import Path

import pandas as pd
from sqlalchemy import create_engine, text

# ========================================================================
# Configuration
# ========================================================================

DEFAULT_OUTPUT_DIR = Path("data/seed_data")
DEFAULT_BATCH_SIZE = 50000
LEGACY_TABLE = 'business."æ”¶å…¥æ˜ç»†"'


# ========================================================================
# Database Connection
# ========================================================================


def load_env_file(env_path: Path) -> dict[str, str]:
    """Load environment variables from a file."""
    env_vars = {}
    if env_path.exists():
        with open(env_path, encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#") and "=" in line:
                    key, _, value = line.partition("=")
                    env_vars[key.strip()] = value.strip()
    return env_vars


def get_legacy_connection_string() -> str:
    """Get Legacy database connection string from environment or .wdh_env file."""
    # Priority 1: Environment variable
    db_url = os.getenv("LEGACY_DATABASE__URI") or os.getenv("LEGACY_MYSQL_URL")

    # Priority 2: Load from .wdh_env file
    if not db_url:
        project_root = Path(__file__).parent.parent.parent
        env_file = project_root / ".wdh_env"
        env_vars = load_env_file(env_file)
        db_url = env_vars.get("LEGACY_DATABASE__URI")

    # Priority 3: Try settings module
    if not db_url:
        try:
            from work_data_hub.config.settings import get_settings

            settings = get_settings()
            db_url = getattr(settings, "legacy_database_url", None)
        except ImportError:
            pass

    if not db_url:
        raise ValueError(
            "Legacy database connection string not found. "
            "Please set LEGACY_DATABASE__URI in .wdh_env or environment."
        )

    # SQLAlchemy requires 'postgresql://' not 'postgres://'
    if db_url.startswith("postgres://"):
        db_url = db_url.replace("postgres://", "postgresql://", 1)

    return db_url


# ========================================================================
# Step 1: Export Data from Legacy MySQL
# ========================================================================


def export_data(
    output_path: Path,
    batch_size: int = DEFAULT_BATCH_SIZE,
    limit: int | None = None,
) -> Path:
    """
    Export annuity_income data from Legacy MySQL to CSV.

    Args:
        output_path: Path to output CSV file
        batch_size: Number of rows to fetch per batch
        limit: Maximum number of rows to export (None for all)

    Returns:
        Path to the exported file
    """
    print(f"\n{'=' * 70}")
    print("STEP 1: EXPORT DATA FROM LEGACY MYSQL")
    print(f"{'=' * 70}")
    print("ğŸš€ Starting export from Legacy MySQL...")
    print(f"ğŸ“ Output file: {output_path}")

    # Create output directory
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # Create database connection
    db_url = get_legacy_connection_string()
    engine = create_engine(db_url)

    try:
        with engine.connect() as conn:
            # Get total row count
            count_query = text(f"SELECT COUNT(*) as total FROM {LEGACY_TABLE}")
            total_rows = conn.execute(count_query).scalar()
            print(f"ğŸ“Š Total rows in {LEGACY_TABLE}: {total_rows:,}")

            if limit:
                total_rows = min(total_rows, limit)
                print(f"âš ï¸ Export limited to: {total_rows:,} rows")

            print(f"ğŸ“¥ Exporting data in batches of {batch_size:,}...")

            # Export in batches to manage memory
            chunks = []
            for offset in range(0, total_rows, batch_size):
                remaining = min(batch_size, total_rows - offset)
                batch_query = (
                    f"SELECT * FROM {LEGACY_TABLE} ORDER BY id LIMIT {remaining} OFFSET {offset}"
                )
                batch_df = pd.read_sql(text(batch_query), conn)

                chunks.append(batch_df)
                exported = min(offset + batch_size, total_rows)
                print(
                    f"   âœ“ Exported: {exported:,}/{total_rows:,} rows "
                    f"({exported / total_rows * 100:.1f}%)"
                )

            # Concatenate all batches
            print(f"ğŸ”„ Merging {len(chunks)} batches...")
            df = pd.concat(chunks, ignore_index=True)

            # Sort by id to ensure consistent ordering
            df = df.sort_values("id").reset_index(drop=True)

            # Export to CSV
            print(f"ğŸ’¾ Writing to CSV: {output_path}")
            df.to_csv(output_path, index=False, encoding="utf-8-sig")

            # Export metadata
            metadata_path = output_path.parent / f"{output_path.stem}_metadata.txt"
            with open(metadata_path, "w", encoding="utf-8") as f:
                f.write("# Annuity Income Export Metadata\n")
                f.write(f"# Export Date: {datetime.now().isoformat()}\n")
                f.write(f"# Source: Legacy MySQL - {LEGACY_TABLE}\n")
                f.write(f"# Total Rows: {len(df):,}\n")
                f.write(f"# Columns: {len(df.columns)}\n")
                f.write(f"# Column Names: {', '.join(df.columns.tolist())}\n")

            print("\nâœ… Export completed successfully!")
            print(f"   ğŸ“„ Output file: {output_path}")
            print(f"   ğŸ“Š Total rows: {len(df):,}")
            print(f"   ğŸ“‹ Total columns: {len(df.columns)}")
            print(f"   ğŸ“„ Metadata: {metadata_path}")

            return output_path

    except Exception as e:
        print(f"âŒ Export failed: {e}")
        raise
    finally:
        engine.dispose()


# ========================================================================
# Step 2: Clean Data for New Pipeline
# ========================================================================


def clean_data(
    input_path: Path,
    output_path: Path,
) -> Path:
    """
    Clean exported annuity_income data for New Pipeline processing.

    Args:
        input_path: Path to input CSV file (exported from Legacy)
        output_path: Path to output CSV file (cleaned for New Pipeline)

    Returns:
        Path to the cleaned file
    """
    print(f"\n{'=' * 70}")
    print("STEP 2: CLEAN DATA FOR NEW PIPELINE")
    print(f"{'=' * 70}")
    print("ğŸ”§ Cleaning data for New Pipeline...")
    print(f"ğŸ“„ Input: {input_path}")
    print(f"ğŸ“„ Output: {output_path}")

    # Load data
    print(f"\nğŸ“¥ Loading data from {input_path}...")
    df = pd.read_csv(input_path)
    print(f"   âœ“ Loaded: {len(df):,} rows, {len(df.columns)} columns")

    # ========================================================================
    # Step 2.1: Handle Missing å¹´é‡‘è´¦æˆ·å
    # ========================================================================
    print("\nğŸ”„ Step 2.1: Handling missing å¹´é‡‘è´¦æˆ·å...")

    if "å¹´é‡‘è´¦æˆ·å" in df.columns:
        # Count empty å¹´é‡‘è´¦æˆ·å before filling
        empty_before = (
            df["å¹´é‡‘è´¦æˆ·å"].isna()
            | (df["å¹´é‡‘è´¦æˆ·å"] == "")
            | (df["å¹´é‡‘è´¦æˆ·å"] == "null")
        ).sum()
        print(
            f"   ğŸ“Š Empty å¹´é‡‘è´¦æˆ·å before filling: {empty_before:,} ({empty_before / len(df) * 100:.2f}%)"
        )

        # Fill missing å¹´é‡‘è´¦æˆ·å with å®¢æˆ·åç§°
        if "å®¢æˆ·åç§°" in df.columns:
            mask = (
                df["å¹´é‡‘è´¦æˆ·å"].isna()
                | (df["å¹´é‡‘è´¦æˆ·å"] == "")
                | (df["å¹´é‡‘è´¦æˆ·å"] == "null")
            )
            df.loc[mask, "å¹´é‡‘è´¦æˆ·å"] = df.loc[mask, "å®¢æˆ·åç§°"]

            filled_count = (
                (df["å¹´é‡‘è´¦æˆ·å"].notna())
                & (df["å¹´é‡‘è´¦æˆ·å"] != "")
                & (df["å¹´é‡‘è´¦æˆ·å"] != "null")
            ).sum()

            print("   âœ“ Filled missing å¹´é‡‘è´¦æˆ·å with å®¢æˆ·åç§°")
            print(
                f"   ğŸ“Š Non-empty å¹´é‡‘è´¦æˆ·å after filling: {filled_count:,} ({filled_count / len(df) * 100:.2f}%)"
            )
        else:
            print(
                "   âš ï¸ Warning: 'å®¢æˆ·åç§°' column not found, cannot fill missing å¹´é‡‘è´¦æˆ·å"
            )
    else:
        print("   âš ï¸ Warning: 'å¹´é‡‘è´¦æˆ·å' column not found")

    # ========================================================================
    # Step 2.2: Replace å®¢æˆ·åç§° with å¹´é‡‘è´¦æˆ·å
    # ========================================================================
    print("\nğŸ”„ Step 2.2: Replacing å®¢æˆ·åç§° with å¹´é‡‘è´¦æˆ·å (most original data)...")

    if "å¹´é‡‘è´¦æˆ·å" in df.columns:
        # Replace å®¢æˆ·åç§° with å¹´é‡‘è´¦æˆ·å
        df["å®¢æˆ·åç§°"] = df["å¹´é‡‘è´¦æˆ·å"]

        # Remove å¹´é‡‘è´¦æˆ·å column
        df = df.drop(columns=["å¹´é‡‘è´¦æˆ·å"])

        print("   âœ“ Replaced å®¢æˆ·åç§° with å¹´é‡‘è´¦æˆ·å")
        print("   âœ“ Removed å¹´é‡‘è´¦æˆ·å column (now duplicate)")
    else:
        print("   âš ï¸ Warning: 'å¹´é‡‘è´¦æˆ·å' column not found, skipping")

    # ========================================================================
    # Step 2.3: Remove company_id and äº§å“çº¿ä»£ç 
    # ========================================================================
    print("\nğŸ—‘ï¸  Step 2.3: Removing fields determined by Legacy...")

    for field in ["company_id", "äº§å“çº¿ä»£ç "]:
        if field in df.columns:
            df = df.drop(columns=[field])
            print(f"   âœ“ Removed: {field} (will be re-determined by New Pipeline)")

    # ========================================================================
    # Step 2.4: Reset G00 Institution Codes to NULL
    # ========================================================================
    print("\nğŸ”„ Step 2.4: Resetting 'G00' institution codes to NULL...")

    if "æœºæ„ä»£ç " in df.columns:
        g00_count = (df["æœºæ„ä»£ç "] == "G00").sum()
        df.loc[df["æœºæ„ä»£ç "] == "G00", "æœºæ„ä»£ç "] = None

        non_null_after = df["æœºæ„ä»£ç "].notna().sum()
        print(f"   âœ“ Reset: {g00_count:,} rows from 'G00' to NULL")
        print(
            f"   ğŸ“Š Non-null æœºæ„ä»£ç : {non_null_after:,} ({non_null_after / len(df) * 100:.1f}%)"
        )
    else:
        print("   âš ï¸ Warning: 'æœºæ„ä»£ç ' column not found")

    # ========================================================================
    # Final: Sort and Export
    # ========================================================================
    print("\nğŸ’¾ Final: Sorting and exporting cleaned data...")

    if "id" in df.columns:
        df = df.sort_values("id").reset_index(drop=True)
        print("   âœ“ Sorted by 'id' for consistent ordering")

    df.to_csv(output_path, index=False, encoding="utf-8-sig")
    print(f"   âœ“ Exported to: {output_path}")

    # ========================================================================
    # Summary
    # ========================================================================
    print(f"\n{'=' * 70}")
    print("ğŸ“Š CLEANING SUMMARY")
    print(f"{'=' * 70}")
    print(f"ğŸ“„ Output: {output_path}")
    print(f"ğŸ“Š Rows: {len(df):,}")
    print(f"ğŸ“‹ Columns: {len(df.columns)}")
    print("\nâœ… Cleaning completed!")
    print(f"{'=' * 70}")

    return output_path


# ========================================================================
# Main Pipeline
# ========================================================================


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Generate annuity_income seed data from Legacy MySQL",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Full pipeline (export + clean)
  %(prog)s

  # Export only
  %(prog)s --export-only

  # Clean only
  %(prog)s --clean-only --input data/seed_data/raw.csv

  # With limit for testing
  %(prog)s --limit 10000

  # Custom output path (raw file auto-generated with _raw suffix)
  %(prog)s -o data/my_output.csv
        """,
    )

    # Execution mode
    mode_group = parser.add_mutually_exclusive_group()
    mode_group.add_argument(
        "--export-only",
        action="store_true",
        help="Only export data from Legacy MySQL, skip cleaning",
    )
    mode_group.add_argument(
        "--clean-only",
        action="store_true",
        help="Only clean existing exported data, skip export",
    )

    # Input/Output
    parser.add_argument(
        "-i",
        "--input",
        type=str,
        default=None,
        help="Input CSV file (required for --clean-only mode)",
    )
    parser.add_argument(
        "-o",
        "--output",
        type=str,
        default=None,
        help="Output path for cleaned CSV (raw file auto-generated with _raw suffix)",
    )

    # Export options
    parser.add_argument(
        "-b",
        "--batch-size",
        type=int,
        default=DEFAULT_BATCH_SIZE,
        help=f"Number of rows to fetch per batch (default: {DEFAULT_BATCH_SIZE})",
    )
    parser.add_argument(
        "-l",
        "--limit",
        type=int,
        default=None,
        help="Maximum number of rows to export (default: all rows)",
    )

    args = parser.parse_args()

    # Validate arguments
    if args.clean_only and not args.input:
        parser.error("--input is required when using --clean-only")

    # Generate output paths
    # User specifies cleaned output path, raw path is auto-generated
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    default_filename = f"annuity_income_{timestamp}.csv"

    if args.output:
        output_path = Path(args.output)

        # Handle directory path: if path is a directory or ends with separator, use default filename
        if output_path.is_dir() or str(args.output).endswith(("/", "\\")):
            output_path.mkdir(parents=True, exist_ok=True)
            cleaned_output = output_path / default_filename
        else:
            # Ensure .csv extension
            if output_path.suffix.lower() != ".csv":
                output_path = output_path.with_suffix(".csv")
            cleaned_output = output_path

        # Generate raw path: add _raw suffix before extension
        raw_output = (
            cleaned_output.parent / f"{cleaned_output.stem}_raw{cleaned_output.suffix}"
        )
    else:
        cleaned_output = DEFAULT_OUTPUT_DIR / f"annuity_income_{timestamp}.csv"
        raw_output = DEFAULT_OUTPUT_DIR / f"annuity_income_{timestamp}_raw.csv"

    # ========================================================================
    # Execute Pipeline
    # ========================================================================

    print(f"\n{'=' * 70}")
    print("ANNUITY INCOME SEED DATA GENERATION")
    print(f"{'=' * 70}")
    print(f"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # Clean only mode
    if args.clean_only:
        clean_data(Path(args.input), cleaned_output)

    # Export only mode
    elif args.export_only:
        export_data(raw_output, batch_size=args.batch_size, limit=args.limit)

    # Full pipeline mode (default)
    else:
        # Step 1: Export
        export_data(raw_output, batch_size=args.batch_size, limit=args.limit)

        # Step 2: Clean
        clean_data(raw_output, cleaned_output)

        # Final summary
        print(f"\n{'=' * 70}")
        print("ğŸ‰ PIPELINE COMPLETED SUCCESSFULLY")
        print(f"{'=' * 70}")
        print(f"ğŸ“„ Raw data: {raw_output}")
        print(f"ğŸ“„ Cleaned data: {cleaned_output}")
        print("\nğŸ“ Next Steps:")
        print("   1. Move cleaned CSV to Bronze input directory")
        print("   2. Run New Pipeline ETL to generate Silver/Gold seed data")
        print("   3. Validate seed data quality")
        print(f"{'=' * 70}")

    print(f"\nEnd Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")


if __name__ == "__main__":
    main()
