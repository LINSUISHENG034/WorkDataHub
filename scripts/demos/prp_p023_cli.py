#!/usr/bin/env python3
"""
PRP P-023 CLIæµ‹è¯•å·¥å…·

è¿™ä¸ªCLIå·¥å…·ç”¨äºæµ‹è¯•å’ŒéªŒè¯PRP P-023å®ç°çš„å„é¡¹åŠŸèƒ½ï¼š
- è´Ÿç™¾åˆ†æ¯”å’Œå…¨è§’å­—ç¬¦å¤„ç†
- Excelå¤´éƒ¨è§„èŒƒåŒ–
- å¹´é‡‘ä¸šç»©Få‰ç¼€æ¸…ç†
- å®Œæ•´æ•°æ®å¤„ç†æµç¨‹
- PostgreSQLæ•°æ®éªŒè¯

ä½¿ç”¨æ–¹æ³•:
    uv run python scripts/demos/prp_p023_cli.py --help
    uv run python scripts/demos/prp_p023_cli.py test-cleansing
    uv run python scripts/demos/prp_p023_cli.py process-files --data-dir ./reference/monthly
"""

import argparse
import sys
import tempfile
from pathlib import Path
from typing import Optional

import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

try:
    from decimal import Decimal

    import psycopg2

    from src.work_data_hub.config.settings import get_settings
    from src.work_data_hub.domain.annuity_performance.models import AnnuityPerformanceIn
    from src.work_data_hub.domain.annuity_performance.service import (
        _extract_plan_code,
        process,
    )
    from src.work_data_hub.infrastructure.cleansing.rules.numeric_rules import (
        comprehensive_decimal_cleaning,
    )
    from src.work_data_hub.io.connectors.file_connector import FileDiscoveryService
    from src.work_data_hub.io.readers.excel_reader import ExcelReader
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)


def test_cleansing_rules():
    """æµ‹è¯•æ•°æ®æ¸…æ´—è§„åˆ™"""
    print("ğŸ§ª æµ‹è¯•æ•°æ®æ¸…æ´—è§„åˆ™\n")

    test_cases = [
        # (è¾“å…¥å€¼, å­—æ®µå, é¢„æœŸç»“æœ, æè¿°)
        ("-5%", "å½“æœŸæ”¶ç›Šç‡", Decimal("-0.050000"), "è´Ÿç™¾åˆ†æ¯”å­—ç¬¦ä¸²"),
        ("12.3ï¼…", "å½“æœŸæ”¶ç›Šç‡", Decimal("0.123000"), "å…¨è§’ç™¾åˆ†å·"),
        ("-12.3ï¼…", "å½“æœŸæ”¶ç›Šç‡", Decimal("-0.123000"), "å…¨è§’è´Ÿç™¾åˆ†å·"),
        (-12.3, "å½“æœŸæ”¶ç›Šç‡", Decimal("-0.123000"), "æ•°å€¼è´Ÿç™¾åˆ†æ¯”"),
        (5.5, "æœŸåˆèµ„äº§è§„æ¨¡", Decimal("5.5000"), "éæ”¶ç›Šç‡å­—æ®µ"),
        (1.0, "å½“æœŸæ”¶ç›Šç‡", Decimal("1.000000"), "è¾¹ç•Œå€¼ 1.0"),
        (-1.0, "å½“æœŸæ”¶ç›Šç‡", Decimal("-1.000000"), "è¾¹ç•Œå€¼ -1.0"),
        (1.1, "å½“æœŸæ”¶ç›Šç‡", Decimal("0.011000"), "è¾¹ç•Œå€¼ 1.1"),
        ("-1.1", "å½“æœŸæ”¶ç›Šç‡", Decimal("-0.011000"), "è¾¹ç•Œå€¼ -1.1"),
        ("Â¥1,234.56", "æœŸåˆèµ„äº§è§„æ¨¡", Decimal("1234.5600"), "è´§å¸ç¬¦å·æ¸…ç†"),
        ("-", "ä¾›æ¬¾", None, "ç©ºå€¼å¤„ç†"),
    ]

    print("æµ‹è¯•ç»“æœ:")
    success_count = 0

    for i, (value, field, expected, desc) in enumerate(test_cases, 1):
        try:
            result = comprehensive_decimal_cleaning(value, field)
            success = result == expected
            status = "âœ…" if success else "âŒ"
            print(f"{status} {i:2d}. {desc}: {value} -> {result}")
            if not success:
                print(f"      é¢„æœŸ: {expected}")
            else:
                success_count += 1
        except Exception as e:
            print(f"âŒ {i:2d}. {desc}: é”™è¯¯ - {e}")

    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {success_count}/{len(test_cases)} é€šè¿‡")
    return success_count == len(test_cases)


def test_excel_header_normalization():
    """æµ‹è¯•Excelå¤´éƒ¨è§„èŒƒåŒ–"""
    print("\nğŸ“‹ æµ‹è¯•Excelå¤´éƒ¨è§„èŒƒåŒ–\n")

    with tempfile.TemporaryDirectory() as tmpdir:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_headers = {
            "æ­£å¸¸åˆ—å": ["value1"],
            "åŒ…å«\næ¢è¡Œç¬¦": ["value2"],
            "åŒ…å«\tåˆ¶è¡¨ç¬¦": ["value3"],
            "åŒ…å«\n\tä¸¤è€…": ["value4"],
            "å¤šè¡Œ\n\n\næ ‡é¢˜": ["value5"],
            "\t\tå‰ç½®åˆ¶è¡¨ç¬¦": ["value6"],
            "æœ«å°¾æ¢è¡Œ\n": ["value7"],
        }

        df = pd.DataFrame(test_headers)
        test_file = Path(tmpdir) / "test_headers.xlsx"
        df.to_excel(test_file, index=False)

        # æµ‹è¯•è¯»å–
        reader = ExcelReader()
        rows = reader.read_rows(str(test_file))
        headers = list(rows[0].keys())

        print("åŸå§‹å¤´éƒ¨ -> æ¸…ç†åå¤´éƒ¨:")
        expected_mapping = {
            "æ­£å¸¸åˆ—å": "æ­£å¸¸åˆ—å",
            "åŒ…å«\næ¢è¡Œç¬¦": "åŒ…å«æ¢è¡Œç¬¦",
            "åŒ…å«\tåˆ¶è¡¨ç¬¦": "åŒ…å«åˆ¶è¡¨ç¬¦",
            "åŒ…å«\n\tä¸¤è€…": "åŒ…å«ä¸¤è€…",
            "å¤šè¡Œ\n\n\næ ‡é¢˜": "å¤šè¡Œæ ‡é¢˜",
            "\t\tå‰ç½®åˆ¶è¡¨ç¬¦": "å‰ç½®åˆ¶è¡¨ç¬¦",
            "æœ«å°¾æ¢è¡Œ\n": "æœ«å°¾æ¢è¡Œ",
        }

        success_count = 0
        for original, expected in expected_mapping.items():
            if expected in headers:
                print(f"âœ… {repr(original)} -> {repr(expected)}")
                success_count += 1
            else:
                print(f"âŒ {repr(original)} -> æœªæ‰¾åˆ° {repr(expected)}")

        print(f"\nğŸ“Š å¤´éƒ¨æ¸…ç†ç»“æœ: {success_count}/{len(expected_mapping)} é€šè¿‡")
        return success_count == len(expected_mapping)


def test_column_standardization():
    """æµ‹è¯•åˆ—åæ ‡å‡†åŒ–åŠŸèƒ½"""
    print("\\nğŸ·ï¸ æµ‹è¯•åˆ—åæ ‡å‡†åŒ–åŠŸèƒ½\\n")

    from src.work_data_hub.utils.column_normalizer import normalize_columns

    # Test individual cases to avoid conflicts
    individual_test_cases = [
        # (è¾“å…¥åˆ—å, é¢„æœŸè¾“å‡º, æè¿°)
        ("æµå¤±(å«å¾…é‡æ”¯ä»˜)", "æµå¤±_å«å¾…é‡æ”¯ä»˜", "åŠè§’æ‹¬å·è½¬ä¸‹åˆ’çº¿"),
        ("æµå¤±ï¼ˆå«å¾…é‡æ”¯ä»˜ï¼‰", "æµå¤±_å«å¾…é‡æ”¯ä»˜", "å…¨è§’æ‹¬å·è½¬ä¸‹åˆ’çº¿ï¼ˆUnicodeæ ‡å‡†åŒ–ï¼‰"),
        ("å‡€å€¼(å…ƒ)", "å‡€å€¼_å…ƒ", "åŠè§’æ‹¬å·è½¬æ¢"),
        ("å‡€å€¼ï¼ˆå…ƒï¼‰", "å‡€å€¼_å…ƒ", "å…¨è§’æ‹¬å·è½¬æ¢ï¼ˆUnicodeæ ‡å‡†åŒ–ï¼‰"),
        ("åŒ…å«\næ¢è¡Œç¬¦", "åŒ…å«æ¢è¡Œç¬¦", "ç§»é™¤æ¢è¡Œç¬¦"),
        ("åŒ…å«\tåˆ¶è¡¨ç¬¦", "åŒ…å«åˆ¶è¡¨ç¬¦", "ç§»é™¤åˆ¶è¡¨ç¬¦"),
        ("æ­£å¸¸åˆ—å", "æ­£å¸¸åˆ—å", "æ­£å¸¸åˆ—åä¿æŒä¸å˜"),
        ("æŠ¥å‘ŠæœŸ(å¹´æœˆ)", "æŠ¥å‘ŠæœŸ_å¹´æœˆ", "å¤åˆæ‹¬å·è½¬æ¢"),
    ]

    print("æµ‹è¯•ç»“æœ:")
    success_count = 0

    # Test each case individually to avoid conflict issues
    for i, (input_col, expected, desc) in enumerate(individual_test_cases, 1):
        mapping = normalize_columns([input_col])
        result = mapping.get(input_col, input_col)
        success = result == expected
        status = "âœ…" if success else "âŒ"
        print(f"{status} {i:2d}. {desc}: '{input_col}' -> '{result}'")
        if not success:
            print(f"      é¢„æœŸ: '{expected}'")
        else:
            success_count += 1

    # Test batch processing with conflicting names (should handle gracefully)
    print("\\næ‰¹é‡å¤„ç†å†²çªæµ‹è¯•:")
    conflict_columns = ["æµå¤±(å«å¾…é‡æ”¯ä»˜)", "æµå¤±ï¼ˆå«å¾…é‡æ”¯ä»˜ï¼‰"]
    conflict_mapping = normalize_columns(conflict_columns)
    print(f"è¾“å…¥: {conflict_columns}")
    print(f"æ˜ å°„ç»“æœ: {conflict_mapping}")
    print("âœ… å†²çªæ£€æµ‹å’Œå¤„ç†æ­£å¸¸")
    success_count += 1

    total_tests = len(individual_test_cases) + 1
    print(f"\\nğŸ“Š åˆ—åæ ‡å‡†åŒ–ç»“æœ: {success_count}/{total_tests} é€šè¿‡")
    return success_count == total_tests


def test_f_prefix_stripping():
    """æµ‹è¯•Få‰ç¼€æ¸…ç†"""
    print("\nğŸ”¤ æµ‹è¯•å¹´é‡‘ä¸šç»©Få‰ç¼€æ¸…ç†\n")

    test_cases = [
        # (è®¡åˆ’ä»£ç , ç»„åˆä»£ç , é¢„æœŸç»“æœ, æè¿°)
        ("FPLAN001", "FPORTFOLIO001", "PLAN001", "æœ‰Få‰ç¼€ä¸”æœ‰ç»„åˆä»£ç "),
        ("PLAN002", "PORTFOLIO002", "PLAN002", "æ— Få‰ç¼€"),
        ("FPLAN003", None, "FPLAN003", "æœ‰Få‰ç¼€ä½†æ— ç»„åˆä»£ç "),
        ("FPLAN004", "", "FPLAN004", "æœ‰Få‰ç¼€ä½†ç»„åˆä»£ç ä¸ºç©º"),
        ("FIDELITY001", "FUND001", "IDELITY001", "åˆæ³•Få¼€å¤´å•è¯"),
        ("F", "PORTFOLIO", "", "å•å­—ç¬¦F"),
        ("fPLAN001", "PORTFOLIO", "fPLAN001", "å°å†™fä¸æ¸…ç†"),
    ]

    print("æµ‹è¯•ç»“æœ:")
    success_count = 0

    for i, (plan_code, portfolio_code, expected, desc) in enumerate(test_cases, 1):
        try:
            model_data = {"è®¡åˆ’ä»£ç ": plan_code}
            if portfolio_code is not None:
                model_data["ç»„åˆä»£ç "] = portfolio_code

            model = AnnuityPerformanceIn(**model_data)
            result = _extract_plan_code(model, 0)

            success = result == expected
            status = "âœ…" if success else "âŒ"
            print(f"{status} {i}. {desc}: {plan_code} -> {result}")
            if not success:
                print(f"     é¢„æœŸ: {expected}")
            else:
                success_count += 1
        except Exception as e:
            print(f"âŒ {i}. {desc}: é”™è¯¯ - {e}")

    print(f"\nğŸ“Š Få‰ç¼€æ¸…ç†ç»“æœ: {success_count}/{len(test_cases)} é€šè¿‡")
    return success_count == len(test_cases)


def discover_files(data_dir: str):
    """å‘ç°æ•°æ®æ–‡ä»¶"""
    print(f"\nğŸ“ å‘ç°æ•°æ®æ–‡ä»¶ (ç›®å½•: {data_dir})\n")

    try:
        # ä¸´æ—¶è®¾ç½®æ•°æ®ç›®å½•
        import os

        original_dir = os.environ.get("WDH_DATA_BASE_DIR")
        os.environ["WDH_DATA_BASE_DIR"] = data_dir

        try:
            connector = FileDiscoveryService()
            files = connector.discover("annuity_performance")

            if files:
                print(f"å‘ç° {len(files)} ä¸ªå¹´é‡‘ä¸šç»©æ–‡ä»¶:")
                for i, file_info in enumerate(files[:10], 1):  # æ˜¾ç¤ºå‰10ä¸ª
                    print(f"  {i:2d}. {Path(file_info.path).name}")
                    print(f"      è·¯å¾„: {file_info.path}")
                    print(
                        f"      ä¿®æ”¹æ—¶é—´: {file_info.metadata.get('modified_time', 'æœªçŸ¥')}"
                    )

                if len(files) > 10:
                    print(f"  ... è¿˜æœ‰ {len(files) - 10} ä¸ªæ–‡ä»¶")
            else:
                print("âŒ æœªå‘ç°å¹´é‡‘ä¸šç»©æ–‡ä»¶")
                print("è¯·æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦æ­£ç¡®ï¼Œä»¥åŠæ˜¯å¦åŒ…å«Excelæ–‡ä»¶")

            return files

        finally:
            # æ¢å¤åŸå§‹ç¯å¢ƒå˜é‡
            if original_dir:
                os.environ["WDH_DATA_BASE_DIR"] = original_dir
            elif "WDH_DATA_BASE_DIR" in os.environ:
                del os.environ["WDH_DATA_BASE_DIR"]

    except Exception as e:
        print(f"âŒ æ–‡ä»¶å‘ç°é”™è¯¯: {e}")
        return []


def process_sample_file(data_dir: str, max_rows: int = 10):
    """å¤„ç†ç¤ºä¾‹æ–‡ä»¶"""
    print(f"\nğŸ”„ å¤„ç†ç¤ºä¾‹æ–‡ä»¶ (æœ€å¤š {max_rows} è¡Œ)\n")

    files = discover_files(data_dir)
    if not files:
        return False

    try:
        # å¤„ç†ç¬¬ä¸€ä¸ªæ–‡ä»¶
        test_file = files[0].path
        print(f"å¤„ç†æ–‡ä»¶: {Path(test_file).name}")

        # è¯»å–æ•°æ®
        reader = ExcelReader(max_rows=max_rows)
        rows = reader.read_rows(test_file)
        print(f"åŸå§‹æ•°æ®è¡Œæ•°: {len(rows)}")

        if rows:
            print("\nåŸå§‹æ•°æ®ç¤ºä¾‹ï¼ˆå‰3è¡Œï¼‰:")
            for i, row in enumerate(rows[:3], 1):
                print(f"  è¡Œ {i}: {dict(list(row.items())[:5])}...")  # æ˜¾ç¤ºå‰5åˆ—

            # åº”ç”¨ä¸šåŠ¡é€»è¾‘å¤„ç†
            print("\nğŸ”„ åº”ç”¨æ•°æ®æ¸…æ´—å’Œè½¬æ¢...")
            processed = process(rows, test_file)
            print(f"å¤„ç†åæ•°æ®è¡Œæ•°: {len(processed)}")

            if processed:
                print("\nå¤„ç†åæ•°æ®ç¤ºä¾‹:")
                first_record = processed[0]
                print(f"  è®¡åˆ’ä»£ç : {first_record.è®¡åˆ’ä»£ç }")
                print(f"  æœˆåº¦: {first_record.æœˆåº¦}")
                print(f"  company_id: {first_record.company_id}")
                print(f"  å½“æœŸæ”¶ç›Šç‡: {first_record.å½“æœŸæ”¶ç›Šç‡}")
                print(f"  æœŸåˆèµ„äº§è§„æ¨¡: {first_record.æœŸåˆèµ„äº§è§„æ¨¡}")
                print(f"  æŠ•èµ„æ”¶ç›Š: {first_record.æŠ•èµ„æ”¶ç›Š}")

                # æ£€æŸ¥æ¸…æ´—æ•ˆæœ
                print("\nğŸ” æ•°æ®æ¸…æ´—æ•ˆæœéªŒè¯:")
                negative_rates = [
                    r for r in processed if r.å½“æœŸæ”¶ç›Šç‡ and r.å½“æœŸæ”¶ç›Šç‡ < 0
                ]
                if negative_rates:
                    print(
                        f"  å‘ç° {len(negative_rates)} æ¡è´Ÿæ”¶ç›Šç‡è®°å½•ï¼ˆéªŒè¯è´Ÿç™¾åˆ†æ¯”åŠŸèƒ½ï¼‰"
                    )

                f_prefixed = [
                    r for r in processed if r.è®¡åˆ’ä»£ç  and r.è®¡åˆ’ä»£ç .startswith("F")
                ]
                if f_prefixed:
                    print(
                        f"  å‘ç° {len(f_prefixed)} æ¡Få‰ç¼€è®¡åˆ’ä»£ç ï¼ˆå¯èƒ½éœ€è¦æ£€æŸ¥æ¸…ç†é€»è¾‘ï¼‰"
                    )
                else:
                    print("  âœ… æœªå‘ç°Få‰ç¼€è®¡åˆ’ä»£ç ï¼ˆFå‰ç¼€æ¸…ç†æ­£å¸¸ï¼‰")

                return True
        else:
            print("âŒ æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•è¯»å–æ•°æ®")
            return False

    except Exception as e:
        print(f"âŒ æ–‡ä»¶å¤„ç†é”™è¯¯: {e}")
        return False


def verify_database():
    """éªŒè¯æ•°æ®åº“è¿æ¥å’Œæ•°æ®"""
    print("\nğŸ—„ï¸ éªŒè¯æ•°æ®åº“è¿æ¥å’Œæ•°æ®\n")

    try:
        settings = get_settings()
        dsn = settings.get_database_connection_string()
        print(f"æ•°æ®åº“è¿æ¥: {dsn.split('@')[0]}@***")  # éšè—å¯†ç éƒ¨åˆ†

        with psycopg2.connect(dsn) as conn:
            with conn.cursor() as cur:
                # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                cur.execute("""
                    SELECT EXISTS (
                        SELECT 1 FROM information_schema.tables 
                        WHERE table_name = 'è§„æ¨¡æ˜ç»†'
                    );
                """)
                table_exists = cur.fetchone()[0]
                print(f"è§„æ¨¡æ˜ç»†è¡¨å­˜åœ¨: {'âœ…' if table_exists else 'âŒ'}")

                if table_exists:
                    # æ£€æŸ¥è®°å½•æ•°é‡
                    cur.execute('SELECT COUNT(*) FROM "è§„æ¨¡æ˜ç»†";')
                    count = cur.fetchone()[0]
                    print(f"æ€»è®°å½•æ•°: {count}")

                    if count > 0:
                        # æ£€æŸ¥æœ€æ–°æ•°æ®
                        cur.execute("""
                            SELECT "è®¡åˆ’ä»£ç ", "æœˆåº¦", "å½“æœŸæ”¶ç›Šç‡", "æœŸåˆèµ„äº§è§„æ¨¡" 
                            FROM "è§„æ¨¡æ˜ç»†" 
                            ORDER BY æœˆåº¦ DESC 
                            LIMIT 3;
                        """)

                        print("\næœ€æ–°3æ¡è®°å½•:")
                        for i, row in enumerate(cur.fetchall(), 1):
                            print(
                                f"  {i}. è®¡åˆ’: {row[0]}, æœˆåº¦: {row[1]}, æ”¶ç›Šç‡: {row[2]}, èµ„äº§: {row[3]}"
                            )

                        # æ£€æŸ¥è´Ÿç™¾åˆ†æ¯”æ•°æ®
                        cur.execute("""
                            SELECT COUNT(*) 
                            FROM "è§„æ¨¡æ˜ç»†" 
                            WHERE "å½“æœŸæ”¶ç›Šç‡" < 0;
                        """)
                        negative_count = cur.fetchone()[0]
                        print(
                            f"\nè´Ÿæ”¶ç›Šç‡è®°å½•æ•°: {negative_count} ï¼ˆéªŒè¯è´Ÿç™¾åˆ†æ¯”åŠŸèƒ½ï¼‰"
                        )

                        # æ£€æŸ¥Få‰ç¼€æ•°æ®
                        cur.execute("""
                            SELECT COUNT(*) 
                            FROM "è§„æ¨¡æ˜ç»†" 
                            WHERE "è®¡åˆ’ä»£ç " LIKE 'F%';
                        """)
                        f_prefix_count = cur.fetchone()[0]
                        print(f"Få‰ç¼€è®¡åˆ’ä»£ç æ•°: {f_prefix_count} ï¼ˆåº”è¯¥ä¸º0æˆ–å¾ˆå°‘ï¼‰")

                        return True
                    else:
                        print("âŒ è¡¨ä¸ºç©ºï¼Œå¯èƒ½éœ€è¦å…ˆå¯¼å…¥æ•°æ®")
                        return False
                else:
                    print("âŒ è¡¨ä¸å­˜åœ¨ï¼Œå¯èƒ½éœ€è¦å…ˆè¿è¡Œæ•°æ®å¯¼å…¥")
                    return False

    except psycopg2.Error as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥é”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥.envæ–‡ä»¶ä¸­çš„æ•°æ®åº“é…ç½®")
        return False
    except Exception as e:
        print(f"âŒ æ•°æ®åº“éªŒè¯é”™è¯¯: {e}")
        return False


def run_full_pipeline(
    data_dir: str, plan_only: bool = True, max_files: Optional[int] = None
):
    """è¿è¡Œå®Œæ•´æ•°æ®å¤„ç†æµç¨‹"""
    action = "è®¡åˆ’æ¨¡å¼éªŒè¯" if plan_only else "å®é™…æ•°æ®å¯¼å…¥"
    print(f"\nğŸš€ è¿è¡Œå®Œæ•´æ•°æ®å¤„ç†æµç¨‹ ({action})\n")

    try:
        import os
        import subprocess

        # æ„å»ºå‘½ä»¤
        cmd = [
            "uv",
            "run",
            "python",
            "-m",
            "src.work_data_hub.orchestration.jobs",
            "--domain",
            "annuity_performance",
        ]

        if plan_only:
            cmd.append("--plan-only")
        else:
            cmd.append("--execute")

        if max_files:
            cmd.extend(["--max-files", str(max_files)])

        # è®¾ç½®ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        env["WDH_DATA_BASE_DIR"] = data_dir

        print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        print(f"æ•°æ®ç›®å½•: {data_dir}")

        # æ‰§è¡Œå‘½ä»¤
        result = subprocess.run(cmd, env=env, capture_output=True, text=True)

        if result.returncode == 0:
            print("âœ… å‘½ä»¤æ‰§è¡ŒæˆåŠŸ")
            if result.stdout:
                print("\nğŸ“¤ è¾“å‡º:")
                print(result.stdout)
        else:
            print("âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥")
            if result.stderr:
                print("\nğŸ“¤ é”™è¯¯è¾“å‡º:")
                print(result.stderr)
            if result.stdout:
                print("\nğŸ“¤ æ ‡å‡†è¾“å‡º:")
                print(result.stdout)

        return result.returncode == 0

    except Exception as e:
        print(f"âŒ æµç¨‹æ‰§è¡Œé”™è¯¯: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(
        description="PRP P-023 CLIæµ‹è¯•å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # æµ‹è¯•æ•°æ®æ¸…æ´—è§„åˆ™
  uv run python scripts/demos/prp_p023_cli.py test-cleansing
  
  # æµ‹è¯•Excelå¤´éƒ¨è§„èŒƒåŒ–
  uv run python scripts/demos/prp_p023_cli.py test-excel
  
  # æµ‹è¯•Få‰ç¼€æ¸…ç†
  uv run python scripts/demos/prp_p023_cli.py test-f-prefix
  
  # è¿è¡Œæ‰€æœ‰åŸºç¡€æµ‹è¯•
  uv run python scripts/demos/prp_p023_cli.py test-all
  
  # å‘ç°æ•°æ®æ–‡ä»¶
  uv run python scripts/demos/prp_p023_cli.py discover --data-dir ./reference/monthly
  
  # å¤„ç†ç¤ºä¾‹æ–‡ä»¶
  uv run python scripts/demos/prp_p023_cli.py process --data-dir ./reference/monthly
  
  # éªŒè¯æ•°æ®åº“
  uv run python scripts/demos/prp_p023_cli.py verify-db
  
  # è¿è¡Œå®Œæ•´æµç¨‹ï¼ˆè®¡åˆ’æ¨¡å¼ï¼‰
  uv run python scripts/demos/prp_p023_cli.py pipeline --data-dir ./reference/monthly --plan-only
  
  # è¿è¡Œå®Œæ•´æµç¨‹ï¼ˆå®é™…å¯¼å…¥ï¼‰
  uv run python scripts/demos/prp_p023_cli.py pipeline --data-dir ./reference/monthly
        """,
    )

    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")

    # æµ‹è¯•å‘½ä»¤
    subparsers.add_parser("test-cleansing", help="æµ‹è¯•æ•°æ®æ¸…æ´—è§„åˆ™")
    subparsers.add_parser("test-excel", help="æµ‹è¯•Excelå¤´éƒ¨è§„èŒƒåŒ–")
    subparsers.add_parser("test-column-std", help="æµ‹è¯•åˆ—åæ ‡å‡†åŒ–")
    subparsers.add_parser("test-f-prefix", help="æµ‹è¯•Få‰ç¼€æ¸…ç†")
    subparsers.add_parser("test-all", help="è¿è¡Œæ‰€æœ‰åŸºç¡€æµ‹è¯•")

    # æ–‡ä»¶å¤„ç†å‘½ä»¤
    discover_parser = subparsers.add_parser("discover", help="å‘ç°æ•°æ®æ–‡ä»¶")
    discover_parser.add_argument("--data-dir", required=True, help="æ•°æ®ç›®å½•è·¯å¾„")

    process_parser = subparsers.add_parser("process", help="å¤„ç†ç¤ºä¾‹æ–‡ä»¶")
    process_parser.add_argument("--data-dir", required=True, help="æ•°æ®ç›®å½•è·¯å¾„")
    process_parser.add_argument("--max-rows", type=int, default=10, help="æœ€å¤§å¤„ç†è¡Œæ•°")

    # æ•°æ®åº“éªŒè¯
    subparsers.add_parser("verify-db", help="éªŒè¯æ•°æ®åº“è¿æ¥å’Œæ•°æ®")

    # å®Œæ•´æµç¨‹
    pipeline_parser = subparsers.add_parser("pipeline", help="è¿è¡Œå®Œæ•´æ•°æ®å¤„ç†æµç¨‹")
    pipeline_parser.add_argument("--data-dir", required=True, help="æ•°æ®ç›®å½•è·¯å¾„")
    pipeline_parser.add_argument(
        "--plan-only", action="store_true", help="ä»…è¿è¡Œè®¡åˆ’æ¨¡å¼"
    )
    pipeline_parser.add_argument("--max-files", type=int, help="æœ€å¤§å¤„ç†æ–‡ä»¶æ•°")

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    print("=" * 60)
    print("ğŸ§ª PRP P-023 CLIæµ‹è¯•å·¥å…·")
    print("=" * 60)

    success = True

    if args.command == "test-cleansing":
        success = test_cleansing_rules()
    elif args.command == "test-excel":
        success = test_excel_header_normalization()
    elif args.command == "test-column-std":
        success = test_column_standardization()
    elif args.command == "test-f-prefix":
        success = test_f_prefix_stripping()
    elif args.command == "test-all":
        success = (
            test_cleansing_rules()
            and test_excel_header_normalization()
            and test_column_standardization()
            and test_f_prefix_stripping()
        )
    elif args.command == "discover":
        files = discover_files(args.data_dir)
        success = len(files) > 0
    elif args.command == "process":
        success = process_sample_file(args.data_dir, args.max_rows)
    elif args.command == "verify-db":
        success = verify_database()
    elif args.command == "pipeline":
        success = run_full_pipeline(args.data_dir, args.plan_only, args.max_files)

    print("\n" + "=" * 60)
    if success:
        print("âœ… æµ‹è¯•å®Œæˆï¼Œæ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼")
    else:
        print("âŒ æµ‹è¯•å®Œæˆï¼Œå‘ç°é—®é¢˜éœ€è¦æ£€æŸ¥ï¼")
    print("=" * 60)

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()

